{"cells":[{"cell_type":"markdown","metadata":{"id":"vRSJfpfXQkzn"},"source":["# Deep Learning Project - Part3\n","<div style=\"text-align: center\">\n","<h1 style = \"color: red\"> Sharif University Of Technology</h1>\n","<h2 style = \"color: green\"> DR. Fatemizadeh </h2>\n","<h3 style = \"color: cyan\"> Authors: Amirreza Velaee - Hessam Hosseini - Amirabbas Afzali - Mahshad Moradi<h3>\n","</div>"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-02-02T10:38:53.850562Z","iopub.status.busy":"2024-02-02T10:38:53.850290Z","iopub.status.idle":"2024-02-02T10:38:56.104113Z","shell.execute_reply":"2024-02-02T10:38:56.103054Z","shell.execute_reply.started":"2024-02-02T10:38:53.850536Z"},"id":"oeTnUDU-Op7F","outputId":"8c5f18ba-0507-4811-8895-77fce1145412","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Random Seed:  42\n"]}],"source":["import argparse\n","import os\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.nn.parallel\n","import torch.optim as optim\n","import torch.utils.data\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","from IPython.display import HTML\n","import json\n","from tqdm import tqdm, trange\n","from sklearn.metrics import precision_recall_fscore_support, matthews_corrcoef\n","import pickle\n","from torch.utils.data import random_split\n","from torch.utils.data import Dataset, Subset\n","from torch.utils.data import DataLoader,Dataset\n","from torch.nn.modules import ReLU,Linear,Dropout\n","import time\n","import math\n","import datetime\n","import torch.nn.functional as F\n","from collections import OrderedDict\n","\n","\n","# Set random seed for reproducibility\n","manualSeed = 42\n","print(\"Random Seed: \", manualSeed)\n","random.seed(manualSeed)\n","torch.manual_seed(manualSeed)\n","\n","ngpu = 1"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-02-02T10:38:56.106922Z","iopub.status.busy":"2024-02-02T10:38:56.106162Z","iopub.status.idle":"2024-02-02T10:38:56.187680Z","shell.execute_reply":"2024-02-02T10:38:56.186750Z","shell.execute_reply.started":"2024-02-02T10:38:56.106885Z"},"id":"ZAnJ2iOIQsyq","outputId":"7384417a-2a7b-419b-d81a-3cc3b66cefcd","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["There are 2 GPU(s) available.\n","We will use the GPU: Tesla T4\n"]}],"source":["# If there's a GPU available...\n","if torch.cuda.is_available():    \n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")\n","    \n","ngpu = torch.cuda.device_count()"]},{"cell_type":"markdown","metadata":{"id":"A2EynVlITXhz"},"source":["## load the dataset:"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-02-02T10:38:56.189602Z","iopub.status.busy":"2024-02-02T10:38:56.188945Z","iopub.status.idle":"2024-02-02T10:39:08.350768Z","shell.execute_reply":"2024-02-02T10:39:08.349915Z","shell.execute_reply.started":"2024-02-02T10:38:56.189562Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: gdown in /opt/conda/lib/python3.10/site-packages (5.0.1)\n","Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.2)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.13.1)\n","Requirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.31.0)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.66.1)\n","Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2023.11.17)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n"]}],"source":["!pip install gdown \n","import gdown "]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-02-02T10:39:08.353585Z","iopub.status.busy":"2024-02-02T10:39:08.352958Z","iopub.status.idle":"2024-02-02T10:39:10.545463Z","shell.execute_reply":"2024-02-02T10:39:10.544503Z","shell.execute_reply.started":"2024-02-02T10:39:08.353545Z"},"id":"IsNFXLLDTpJk","outputId":"3c00b8ab-3143-4460-ed79-a708447a8689","trusted":true},"outputs":[{"data":{"text/plain":["['/kaggle/working/SubtaskB/subtaskB_dev.jsonl',\n"," '/kaggle/working/SubtaskB/subtaskB_train.jsonl']"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')\n","\n","# id = \"11YeloR2eTXcTzdwI04Z-M2QVvIeQAU6-\"\n","id = \"1G-XttJCGvkAVkU9N_W_PxR0Cx099qbUa\"\n","gdown.download_folder(id=id, quiet=True, use_cookies=False)"]},{"cell_type":"markdown","metadata":{"id":"dncLXjy5VWUd"},"source":["**Subtask B:**\n","\n","An object of the JSON has the following format:"]},{"cell_type":"markdown","metadata":{"id":"WV5LP7CRoOqY"},"source":["\n","-  **id** -> identifier of the example,\n","- **label** -> label (human: 0, chatGPT: 1, cohere: 2, davinci: 3, bloomz: 4, dolly: 5),\n","- **text** -> text generated by machine or written by human,\n","- **model** -> model name that generated data,\n","- **source** -> source (Wikipedia, Wikihow, Peerread, Reddit, Arxiv) on English\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-02-02T10:39:13.355228Z","iopub.status.busy":"2024-02-02T10:39:13.354843Z","iopub.status.idle":"2024-02-02T10:39:14.482951Z","shell.execute_reply":"2024-02-02T10:39:14.482136Z","shell.execute_reply.started":"2024-02-02T10:39:13.355201Z"},"id":"jtqmb0FPTeYc","trusted":true},"outputs":[],"source":["# content/drive/My Drive/Project\n","with open('/kaggle/working/SubtaskB/subtaskB_train.jsonl', 'r') as file:\n","    \n","    lines = file.readlines()\n","\n","# Parse each line as a JSON object\n","train_objects = [json.loads(line) for line in lines]"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-02-02T10:39:14.485148Z","iopub.status.busy":"2024-02-02T10:39:14.484795Z","iopub.status.idle":"2024-02-02T10:39:14.539183Z","shell.execute_reply":"2024-02-02T10:39:14.538267Z","shell.execute_reply.started":"2024-02-02T10:39:14.485121Z"},"id":"ujtd9MmMUUFL","trusted":true},"outputs":[],"source":["with open('/kaggle/working/SubtaskB/subtaskB_dev.jsonl', 'r') as file:\n","    lines = file.readlines()\n","\n","dev_objects = [json.loads(line) for line in lines]"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-02-02T10:39:14.540426Z","iopub.status.busy":"2024-02-02T10:39:14.540157Z","iopub.status.idle":"2024-02-02T10:39:15.029157Z","shell.execute_reply":"2024-02-02T10:39:15.028141Z","shell.execute_reply.started":"2024-02-02T10:39:14.540403Z"},"id":"rBcEi416T8iQ","outputId":"5994de5e-5f23-4711-e326-cc29da869334","trusted":true},"outputs":[{"data":{"text/plain":["(71027, 3000)"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["len(train_objects), len(dev_objects) "]},{"cell_type":"markdown","metadata":{"id":"VYTe6PmLV0LW"},"source":["an example:"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-02-02T10:39:15.031374Z","iopub.status.busy":"2024-02-02T10:39:15.030968Z","iopub.status.idle":"2024-02-02T10:39:15.039847Z","shell.execute_reply":"2024-02-02T10:39:15.039046Z","shell.execute_reply.started":"2024-02-02T10:39:15.031348Z"},"id":"NWJNu5ETVuzX","outputId":"7d10092c-6bbb-468f-e6f1-7f0eb76f74f8","trusted":true},"outputs":[{"data":{"text/plain":["dict_keys(['text', 'model', 'source', 'label', 'id'])"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["train_objects[100].keys()"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-02-02T10:39:15.041282Z","iopub.status.busy":"2024-02-02T10:39:15.040959Z","iopub.status.idle":"2024-02-02T10:39:15.050102Z","shell.execute_reply":"2024-02-02T10:39:15.049108Z","shell.execute_reply.started":"2024-02-02T10:39:15.041250Z"},"id":"LxK-CV8fV-46","outputId":"68c4f9ce-1c9b-4201-939a-34cfbbc0fd2d","trusted":true},"outputs":[{"data":{"text/plain":["('chatGPT', 'wikihow', 1, 100)"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["train_objects[100]['model'], train_objects[100]['source'],train_objects[100]['label'],train_objects[100]['id']"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-02-02T10:39:15.865757Z","iopub.status.busy":"2024-02-02T10:39:15.864938Z","iopub.status.idle":"2024-02-02T10:39:15.870235Z","shell.execute_reply":"2024-02-02T10:39:15.869353Z","shell.execute_reply.started":"2024-02-02T10:39:15.865719Z"},"id":"OgxeDd4pV5fn","outputId":"cb6fed31-5408-482f-f129-2a4e69ddb634","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["If you're looking to set up a home PC with multiple modems and phone lines, you're in luck! With a little bit of know-how, you can have a faster, more reliable internet connection than ever before. Here's how to get started.\n","\n","Part 1: Setting Up Your Modems\n","\n","Step 1: Find out if your local ISP supports Multi-Link accounts. Some ISPs offer plans that allow you to connect multiple modems to your computer to boost your internet speeds. Check with your provider to see if this is an option for you.\n","\n","Step 2: Get a second modem. Most computers come with one modem installed, but if you need a second one, you can easily purchase one online or at a tech store. Make sure it's compatible with your operating system before you buy.\n","\n","Step 3: Use the dial-up creation dialog if you're using Windows XP. Navigate to the control panel and click \"network and internet connections.\" From there, select \"create a new connection\" and then choose \"connect to the internet.\"\n","\n","Step 4: Click next, and it will ask for what network type you want to use. Select \"dial-up.\"\n","\n","Step 5: Click next, and choose the type of service you would like to use. This will depend on your ISP. Click next and select \"connect using dial-up modem.\"\n","\n","Step 6: Click next; give the dial-up connection a name (this can be anything you want). Click next, and enter the telephone number needed to access the net.\n","\n","Step 7: Click next and enter the user name and password, which will be used for logging into the server. Click next and make sure that the check boxes are set up the way you want them to stay.\n","\n","Part 2: Connecting Your Phone Lines\n","\n","Step 1: Create the dialog. To set up a multi-link connection, you'll need to connect two phone lines to your computer. Make sure both phones are clear in tone with no noticeable static or wavering noises.\n","\n","Step 2: Connect the phone lines to each phone line junction box. Make sure to follow the instructions that came with your modem.\n","\n","Step 3: Double click on the name of the icon created for your connection. This will open up your connection dialog.\n","\n","Step 4: Hook up two computers using a CAT5 cable through a 5 port Switch. This will allow you to share your internet connection between multiple computers.\n","\n","Step 5: Click on the properties button on the connection dialog. From there, enable software compression, and negotiating multi-link for single dial-up.\n","\n","Step 6: Click the ok button, and if needed now you can set up whatever IP address is used for your connection.\n","\n","Step 7: Setup one computer for the gateway dial out (use the advanced tab for this). This will allow you to share your internet connection between multiple computers.\n","\n","And that's it! With these steps, you'll be up and running with a high-speed, reliable internet connection that's perfect for all your home computing needs. Bear in mind that setting up multiple modems and phone lines can be a complex process, especially if you're not particularly tech-savvy. If you're having difficulty, don't hesitate to reach out for help from a qualified tech support professional.\n"]}],"source":["print(train_objects[104]['text'])"]},{"cell_type":"markdown","metadata":{},"source":["More details about the dataset and the Exploratory Data Analysis have been reported in `EDA.ipynb`."]},{"cell_type":"markdown","metadata":{"id":"ubyocB_epmnh"},"source":["## load the pretrain `RoBERTa` from $huggingface$:"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-02-02T10:39:16.509274Z","iopub.status.busy":"2024-02-02T10:39:16.508983Z","iopub.status.idle":"2024-02-02T10:39:40.379589Z","shell.execute_reply":"2024-02-02T10:39:40.378411Z","shell.execute_reply.started":"2024-02-02T10:39:16.509252Z"},"id":"dKyrJEtPQSzH","outputId":"02800931-b2b5-448a-dd7f-f13ff732d00c","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.37.0)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.4)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.1)\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n","Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.1.99)\n"]}],"source":["!pip install transformers\n","!pip install sentencepiece"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-02-02T10:39:40.382149Z","iopub.status.busy":"2024-02-02T10:39:40.381803Z","iopub.status.idle":"2024-02-02T10:39:42.332554Z","shell.execute_reply":"2024-02-02T10:39:42.331581Z","shell.execute_reply.started":"2024-02-02T10:39:40.382114Z"},"id":"sOOunD55QVXY","trusted":true},"outputs":[],"source":["from transformers import RobertaTokenizer, RobertaModel\n","from transformers import DistilBertTokenizer, DistilBertModel\n","\n","import sentencepiece\n","from transformers import get_constant_schedule_with_warmup"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-02-02T10:39:42.334292Z","iopub.status.busy":"2024-02-02T10:39:42.333812Z","iopub.status.idle":"2024-02-02T10:39:42.938011Z","shell.execute_reply":"2024-02-02T10:39:42.937127Z","shell.execute_reply.started":"2024-02-02T10:39:42.334255Z"},"id":"zZp3vmPPQiqs","outputId":"47183acf-b17b-4120-d696-481f1657c79b","trusted":true},"outputs":[],"source":["# Load pre-trained BERT model and tokenizer\n","# model_name = 'roberta-large'\n","# tokenizer = RobertaTokenizer.from_pretrained(model_name)\n","# bert_model = RobertaModel.from_pretrained(model_name)\n","\n","model_name = 'distilbert-base-uncased'\n","tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n","bert_model = DistilBertModel.from_pretrained(model_name)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-02-02T10:39:42.941259Z","iopub.status.busy":"2024-02-02T10:39:42.940607Z","iopub.status.idle":"2024-02-02T10:39:42.946595Z","shell.execute_reply":"2024-02-02T10:39:42.945678Z","shell.execute_reply.started":"2024-02-02T10:39:42.941222Z"},"id":"6DyPAryTRPeF","trusted":true},"outputs":[],"source":["class BERT_Embedder(nn.Module):\n","    def __init__(self, bert_modele):\n","        super(BERT_Embedder, self).__init__()\n","        self.bert = bert_modele\n","\n","    def forward(self, encoded_ids,attention_mask):\n","        outputs = self.bert(encoded_ids,attention_mask)\n","        last_hidden_states = outputs.last_hidden_state[:,0]  # return embedding of 'CLS' token for classification.\n","\n","        return last_hidden_states"]},{"cell_type":"markdown","metadata":{"id":"ah-BDYpjTDNR"},"source":["Now we can define **Discriminator** and **Generator** completely:"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-02-02T10:39:42.948224Z","iopub.status.busy":"2024-02-02T10:39:42.947871Z","iopub.status.idle":"2024-02-02T10:39:42.957995Z","shell.execute_reply":"2024-02-02T10:39:42.957126Z","shell.execute_reply.started":"2024-02-02T10:39:42.948177Z"},"id":"b00Mx2wuRUmF","trusted":true},"outputs":[],"source":["# custom weights initialization\n","# def weights_init(m):\n","#     classname = m.__class__.__name__\n","#     if classname.find('Conv') != -1:\n","#         nn.init.normal_(m.weight.data, 0.0, 0.02)\n","#     elif classname.find('BatchNorm') != -1:\n","#         nn.init.normal_(m.weight.data, 1.0, 0.02)\n","#         nn.init.constant_(m.bias.data, 0)\n","\n","\n","import torch.nn.init as init\n","\n","def custom_weights_init(m):\n","    if isinstance(m, nn.Linear):\n","        init.xavier_normal_(m.weight.data)\n","        if m.bias is not None:\n","            init.constant_(m.bias.data, 0)\n"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-02-02T10:39:42.959772Z","iopub.status.busy":"2024-02-02T10:39:42.958959Z","iopub.status.idle":"2024-02-02T10:39:42.970113Z","shell.execute_reply":"2024-02-02T10:39:42.969221Z","shell.execute_reply.started":"2024-02-02T10:39:42.959747Z"},"id":"Ml-JM9SsTMM5","trusted":true},"outputs":[],"source":["class Discriminator(nn.Module):\n","    def __init__(self, input_size, num_classes,dropout_rate=0.2,relu_slop=0.2):\n","        super(Discriminator, self).__init__()\n","        self.input_size = input_size\n","        self.num_classes = num_classes\n","\n","        self.main = torch.nn.Sequential(\n","            Dropout(p=dropout_rate),\n","            \n","            Linear(in_features=self.input_size, out_features=512, bias=True),\n","            nn.LeakyReLU(relu_slop, inplace=True),\n","            Dropout(p=dropout_rate),\n","            \n","            Linear(in_features=512, out_features=256, bias=True),\n","            nn.LeakyReLU(relu_slop, inplace=True),\n","            Dropout(p=dropout_rate),\n","            \n","            Linear(in_features=256, out_features=256, bias=True),\n","            nn.LeakyReLU(relu_slop, inplace=True),\n","            Dropout(p=dropout_rate),\n","        )\n","\n","        self.logit = nn.Linear(256,self.num_classes+1)\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, input):\n","        last_rep = self.main(input)  # for do 'feature matching'\n","        logits = self.logit(last_rep)\n","        probs = self.softmax(logits)\n","        return last_rep, logits, probs\n"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-02-02T10:39:42.971532Z","iopub.status.busy":"2024-02-02T10:39:42.971209Z","iopub.status.idle":"2024-02-02T10:39:42.984420Z","shell.execute_reply":"2024-02-02T10:39:42.983722Z","shell.execute_reply.started":"2024-02-02T10:39:42.971502Z"},"id":"BYUu-z4NTCp5","trusted":true},"outputs":[],"source":["class Generator1(nn.Module):\n","    def __init__(self, input_size, output_size,dropout_rate=0.2,relu_slop=0.2):\n","        super(Generator1, self).__init__()\n","\n","        self.input_size = input_size\n","        self.output_size = output_size\n","\n","        self.main = torch.nn.Sequential(\n","            Linear(in_features=self.input_size, out_features=256, bias=True),\n","            nn.LeakyReLU(relu_slop, inplace=True),\n","            Dropout(p=dropout_rate, inplace=False),\n","            Linear(in_features=256, out_features=self.output_size, bias=True),\n","        )\n","\n","    def forward(self, input):\n","        return self.main(input)\n"]},{"cell_type":"markdown","metadata":{"id":"TPbVNzM3vnWH"},"source":["Set Hyperparameter :"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-02-02T10:39:42.985771Z","iopub.status.busy":"2024-02-02T10:39:42.985451Z","iopub.status.idle":"2024-02-02T10:39:42.995508Z","shell.execute_reply":"2024-02-02T10:39:42.994705Z","shell.execute_reply.started":"2024-02-02T10:39:42.985740Z"},"id":"24sxiObdh5z7","trusted":true},"outputs":[],"source":["num_classes = 6\n","input_size = 768\n","noise_size = 100\n","label_list = list(range(6))"]},{"cell_type":"markdown","metadata":{"id":"YyyGFvUSW1ml"},"source":["### Define the Dataset class:"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-02-02T10:39:42.996837Z","iopub.status.busy":"2024-02-02T10:39:42.996570Z","iopub.status.idle":"2024-02-02T10:39:43.007660Z","shell.execute_reply":"2024-02-02T10:39:43.006965Z","shell.execute_reply.started":"2024-02-02T10:39:42.996815Z"},"id":"hTsPHA58npxK","trusted":true},"outputs":[],"source":["\n","class SemEval_Dataset(Dataset):\n","    def __init__(self, json_file,label_list,label_masks,\n","                 max_seq_length, tokenizer,dtype=torch.long):\n","\n","        self.json_file = json_file\n","        self.dtype = dtype\n","        self.label_list = label_list # [0, 1, 2, 3, 4, 5]\n","        self.max_seq_length = max_seq_length\n","        self.tokenizer = tokenizer\n","        self.label_masks = label_masks\n","\n","    def __len__(self):\n","        return len(self.json_file)\n","\n","    def feature_extractor(self, text, label=None):\n","        features = []\n","        tokenized_text = tokenizer(text,padding='max_length', truncation=True,\n","                                   max_length=self.max_seq_length,\n","                                   return_tensors=\"pt\")\n","\n","        input_ids = tokenized_text['input_ids']\n","        input_mask = tokenized_text['attention_mask']\n","\n","        if len(input_ids) > self.max_seq_length:\n","            input_ids = input_ids[0:(self.max_seq_length)]   # crop long sentences\n","            input_mask = input_mask[0:(self.max_seq_length)]\n","\n","        assert len(input_ids[0]) == self.max_seq_length\n","        assert len(input_mask[0]) == self.max_seq_length\n","\n","        if label != None:\n","            return input_ids, input_mask, label\n","        else:\n","            return input_ids, input_mask\n","\n","    def __getitem__(self, idx):\n","        data = self.json_file[idx]\n","        input_ids, input_mask, label_id = self.feature_extractor(data['text'], label=data['label'])\n","\n","        return input_ids.squeeze(0), input_mask.squeeze(0), data['label'], self.label_masks[idx]"]},{"cell_type":"markdown","metadata":{"id":"5-6cooKmuy6d"},"source":["Create Dataset and Dataloader :"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-02-02T10:39:43.011128Z","iopub.status.busy":"2024-02-02T10:39:43.010859Z","iopub.status.idle":"2024-02-02T10:39:43.023371Z","shell.execute_reply":"2024-02-02T10:39:43.022655Z","shell.execute_reply.started":"2024-02-02T10:39:43.011080Z"},"id":"XbCXkjW2Ib9p","trusted":true},"outputs":[],"source":["max_seq_length = 256\n","batch_size = 128"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-02-02T10:39:43.024637Z","iopub.status.busy":"2024-02-02T10:39:43.024380Z","iopub.status.idle":"2024-02-02T10:39:43.037615Z","shell.execute_reply":"2024-02-02T10:39:43.036715Z","shell.execute_reply.started":"2024-02-02T10:39:43.024616Z"},"id":"vBsI2GALOlIS","trusted":true},"outputs":[],"source":["unlabeled_examples = True\n","labeled_ratio = 0.5                  # 0.01, 0.1 ,0.05 ,0.5 \n","train_dataset_size_labeled = int(labeled_ratio* len(train_objects))\n","\n","#The labeled (train) dataset is assigned with a mask set to True\n","train_label_masks = torch.ones(train_dataset_size_labeled, dtype=bool)\n","#If unlabel examples are available\n","if unlabeled_examples:\n","  #The unlabeled (train) dataset is assigned with a mask set to False\n","    tmp_masks = torch.zeros(len(train_objects)- train_dataset_size_labeled , dtype=bool)\n","    train_label_masks = torch.concatenate([train_label_masks,tmp_masks])\n","    idx = torch.randperm(train_label_masks.shape[0])\n","    train_label_masks = train_label_masks[idx].view(train_label_masks.size())\n","\n","assert train_label_masks.shape[0] == len(train_objects)\n","train_dataset = SemEval_Dataset(train_objects, label_list, train_label_masks,max_seq_length, tokenizer)\n","# train_dataset = torch.utils.data.Subset(train_dataset, [i for i in range(train_dataset_size)])"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-02-02T10:39:43.039262Z","iopub.status.busy":"2024-02-02T10:39:43.038807Z","iopub.status.idle":"2024-02-02T10:39:43.055191Z","shell.execute_reply":"2024-02-02T10:39:43.054301Z","shell.execute_reply.started":"2024-02-02T10:39:43.039220Z"},"id":"qQdi1pyGtCE6","trusted":true},"outputs":[],"source":["train_size = int(0.8 * len(train_objects))  # 80% for training\n","val_size = len(train_objects) - train_size  # Remaining 20% for validation\n","\n","train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n","test_label_masks = torch.ones(len(dev_objects), dtype=bool)\n","test_dataset = SemEval_Dataset(dev_objects, label_list, test_label_masks,max_seq_length, tokenizer)\n","\n","train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, num_workers=os.cpu_count(),shuffle=True, drop_last=False)\n","val_dataloader = DataLoader(dataset=val_dataset, batch_size=batch_size, num_workers=os.cpu_count(),shuffle=True, drop_last=False)\n","test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, num_workers=os.cpu_count(),shuffle=True, drop_last=False)"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-02-02T10:39:43.056611Z","iopub.status.busy":"2024-02-02T10:39:43.056307Z","iopub.status.idle":"2024-02-02T10:39:43.061756Z","shell.execute_reply":"2024-02-02T10:39:43.060807Z","shell.execute_reply.started":"2024-02-02T10:39:43.056576Z"},"id":"DGwYFnbRPPsP","outputId":"3ef791d9-52a9-43ed-b3cd-13fa96d3bf6b","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of train samples:  56821\n","Number of validation samples:  14206\n","Number of test samples:  3000\n"]}],"source":["print('Number of train samples: ', len(train_dataset))\n","print('Number of validation samples: ', len(val_dataset))\n","print('Number of test samples: ', len(test_dataset))"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-02-02T10:39:43.063363Z","iopub.status.busy":"2024-02-02T10:39:43.062941Z","iopub.status.idle":"2024-02-02T10:39:48.073687Z","shell.execute_reply":"2024-02-02T10:39:48.072664Z","shell.execute_reply.started":"2024-02-02T10:39:43.063336Z"},"id":"UUXhfRCQxR2G","outputId":"90b8fef2-6212-461a-dd23-f21adbf11a0d","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["input_ids shape: torch.Size([128, 256]), \n","input_mask shape: torch.Size([128, 256]),         \n","label_ids shape: torch.Size([128]),\n","label_mask shape: torch.Size([128])\n"]}],"source":["# test train_dataset\n","betch = next(iter(train_dataloader))\n","print(f'input_ids shape: {betch[0].shape}, \\ninput_mask shape: {betch[1].shape}, \\\n","        \\nlabel_ids shape: {betch[2].shape},\\nlabel_mask shape: {betch[3].shape}')"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-02-02T10:39:48.076170Z","iopub.status.busy":"2024-02-02T10:39:48.075280Z","iopub.status.idle":"2024-02-02T10:39:53.471287Z","shell.execute_reply":"2024-02-02T10:39:53.470289Z","shell.execute_reply.started":"2024-02-02T10:39:48.076131Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([1, 1, 5, 5, 5, 2, 4, 2, 4, 0, 2, 3, 1, 2, 2, 2, 5, 2, 5, 3, 0, 1, 3, 2,\n","        0, 1, 0, 3, 3, 3, 2, 2, 5, 4, 2, 0, 3, 4, 0, 5, 3, 2, 2, 3, 1, 4, 2, 2,\n","        5, 3, 0, 4, 5, 3, 1, 4, 5, 2, 1, 1, 5, 3, 1, 0, 3, 5, 5])\n","\n"]}],"source":["for batch in train_dataloader:\n","    print(batch[2][batch[3]])\n","    print()\n","#     print(batch[3])\n","    break"]},{"cell_type":"markdown","metadata":{"id":"AwpJPzUqbgzI"},"source":["## GAN-BERT"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"execution":{"iopub.execute_input":"2024-02-02T10:40:02.070650Z","iopub.status.busy":"2024-02-02T10:40:02.069805Z","iopub.status.idle":"2024-02-02T10:40:02.074137Z","shell.execute_reply":"2024-02-02T10:40:02.073341Z","shell.execute_reply.started":"2024-02-02T10:40:02.070616Z"},"id":"it3v2R_IdPwB","outputId":"99890cc6-bd00-4aae-d742-32817ee7b8c1","trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","img = plt.imread('/content/drive/My Drive/Project/GAN-BERT.png')\n","plt.imshow(img);\n","plt.axis('off');"]},{"cell_type":"markdown","metadata":{"id":"ywMFAgkjjQeG"},"source":["Hyperparameters:"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-02-02T10:40:38.036505Z","iopub.status.busy":"2024-02-02T10:40:38.035873Z","iopub.status.idle":"2024-02-02T10:40:38.041041Z","shell.execute_reply":"2024-02-02T10:40:38.040121Z","shell.execute_reply.started":"2024-02-02T10:40:38.036476Z"},"id":"GaylbkGFOBX5","trusted":true},"outputs":[],"source":["epoch_num = 8\n","\n","learning_rate = 5e-4\n","noise_size = 100\n","epsilon = 1e-8\n","warmup_proportion = 0.1  #TODO"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-02-02T10:40:38.239274Z","iopub.status.busy":"2024-02-02T10:40:38.238995Z","iopub.status.idle":"2024-02-02T10:40:38.441257Z","shell.execute_reply":"2024-02-02T10:40:38.440358Z","shell.execute_reply.started":"2024-02-02T10:40:38.239251Z"},"id":"-COnKU7NhU2x","outputId":"28480ff1-cc2c-405c-9903-2182abe3723f","trusted":true},"outputs":[{"data":{"text/plain":["Generator1(\n","  (main): Sequential(\n","    (0): Linear(in_features=100, out_features=256, bias=True)\n","    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (2): Dropout(p=0.2, inplace=False)\n","    (3): Linear(in_features=256, out_features=768, bias=True)\n","  )\n",")"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["# Create the Discriminator and Generator\n","discriminator = Discriminator(input_size,num_classes).to(device)\n","generator1 = Generator1(noise_size,input_size).to(device)\n","bert = BERT_Embedder(bert_model).to(device)\n","\n","\n","# Handle multi-GPU if desired\n","if (device.type == 'cuda') and (ngpu > 1):\n","    discriminator = nn.DataParallel(discriminator, list(range(ngpu)))\n","    generator = nn.DataParallel(generator1, list(range(ngpu)))    \n","    bert = nn.DataParallel(bert, list(range(ngpu)))\n","\n","# weights initialization   # TODO : Xavier weight initialization\n","discriminator.apply(custom_weights_init)\n","generator1.apply(custom_weights_init)\n","\n","# print(discriminator)\n","# print()\n","# print(generator1)\n","# print()\n","# print(bert)"]},{"cell_type":"markdown","metadata":{"id":"ja2TR4WPi3fY"},"source":["### Define the Optimizers and Scheduler:"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-02-02T10:40:40.774990Z","iopub.status.busy":"2024-02-02T10:40:40.774144Z","iopub.status.idle":"2024-02-02T10:40:40.782386Z","shell.execute_reply":"2024-02-02T10:40:40.781586Z","shell.execute_reply.started":"2024-02-02T10:40:40.774959Z"},"id":"2zuKJdvdgjoO","trusted":true},"outputs":[],"source":["gen_optimizer = torch.optim.AdamW(generator1.parameters(), lr=learning_rate)\n","dis_optimizer = torch.optim.AdamW(list(bert.parameters()) + list(discriminator.parameters()), lr=learning_rate)\n","\n","#scheduler\n","num_train_examples = len(train_dataset)\n","num_train_steps = int(num_train_examples / batch_size * epoch_num) \n","num_warmup_steps = int(num_train_steps * warmup_proportion)\n","\n","scheduler_d = get_constant_schedule_with_warmup(dis_optimizer, \n","                                       num_warmup_steps = num_warmup_steps)\n","scheduler_g = get_constant_schedule_with_warmup(gen_optimizer, \n","                                       num_warmup_steps = num_warmup_steps) "]},{"cell_type":"markdown","metadata":{},"source":[" The loss function of $Discriminator$ is defined as:  $$\\quad L_{\\mathcal{D}}=L_{\\mathcal{D}_{\\text {sup. }}}+L_{\\mathcal{D}_{\\text {unsup. }}}$$\n"," where:\n","$$\n","\\begin{aligned}\n","L_{\\mathcal{D}_{\\text {sup. }}} & =-\\mathbb{E}_{x, y \\sim p_d} \\log \\left[p_{\\mathrm{m}}(\\hat{y}=y \\mid x, y \\in(1, \\ldots, k))\\right] \\\\\n","L_{\\mathcal{D}_{\\text {unsup. }}} & =-\\mathbb{E}_{x \\sim p_d} \\log \\left[1-p_{\\mathrm{m}}(\\hat{y}=y \\mid x, y=k+1)\\right] -\\mathbb{E}_{x \\sim \\mathcal{G}} \\log \\left[p_{\\mathrm{m}}(\\hat{y}=y \\mid x, y=k+1)\\right] \\\\\n","\\rightarrow  L_{\\mathcal{D}_{\\text {unsup. }}} & =-\\mathbb{E}_{x \\sim p_d} [\\log (\\mathcal{D}(x))] -\\mathbb{E}_{x \\sim \\mathcal{G}} \n","[\\log (1-\\mathcal{D}(x))]\n","\\end{aligned}\n","$$"]},{"cell_type":"markdown","metadata":{},"source":["And loss function of $Generator$ is defined as: $$\\quad L_{\\mathcal{G}}=L_{\\mathcal{G}_{\\text {feature matching }}}+L_{\\mathcal{G}_{\\text {unsup. }}}$$ \n","where:\n","\n","$$L_{\\mathcal{G}_{\\text {unsup. }}}=-\\mathbb{E}_{x \\sim \\mathcal{G}} \n","\\log \\left[1-p_m(\\hat{y}=y \\mid x, y=k+1)\\right]$$\n","\n","$$ L_{\\mathcal{G}_{\\text {feature matching }}} = ||\\mathbb{E}_{x \\sim p_d} f(x) \n","- \\mathbb{E}_{x \\sim \\mathcal{G}} f(x) ||_2^2$$\n"]},{"cell_type":"code","execution_count":178,"metadata":{"execution":{"iopub.execute_input":"2024-02-02T14:20:29.702326Z","iopub.status.busy":"2024-02-02T14:20:29.701957Z","iopub.status.idle":"2024-02-02T14:20:29.749573Z","shell.execute_reply":"2024-02-02T14:20:29.748786Z","shell.execute_reply.started":"2024-02-02T14:20:29.702299Z"},"trusted":true},"outputs":[],"source":["class GANBERT():\n","    def __init__(self, discriminator, generator, bert,gen_optimizer, dis_optimizer,\n","                scheduler_d,scheduler_g, path): \n","\n","        self.discriminator = discriminator\n","        self.generator = generator\n","        self.bert = bert\n","        self.gen_optimizer = gen_optimizer\n","        self.dis_optimizer = dis_optimizer\n","        self.scheduler_g = scheduler_g\n","        self.scheduler_d = scheduler_d\n","        self.nll_loss = torch.nn.CrossEntropyLoss(ignore_index=-1 , label_smoothing=0.005) # which one\n","        self.path = path\n"," \n","    def trainer(self, epoch_num, label_list,labeled_ratio,\n","               train_dataloader, val_dataloader=None,report=True):\n","        \n","        best_score = 1e-5\n","    \n","        def format_time(elapsed):\n","            '''\n","            Takes a time in seconds and returns a string hh:mm:ss\n","            '''\n","            # Round to the nearest second.\n","            elapsed_rounded = int(round((elapsed)))\n","            # Format as hh:mm:ss\n","            return str(datetime.timedelta(seconds=elapsed_rounded))\n","        \n","        results = []\n","        print(f'With labeled_ratio : {labeled_ratio}\\n')\n","        for epoch in range(epoch_num):\n","            # Measure how long the each epoch takes.\n","            t0 = time.time()\n","            \n","            self.bert.train()\n","            self.generator.train()\n","            self.discriminator.train()\n","\n","            tr_g_loss = 0\n","            tr_d_loss = 0\n","            nb_tr_examples, nb_tr_steps = 0, 0\n","\n","            print(f'Epoch {epoch+1}/{epoch_num} :')\n","            for step, batch in enumerate(train_dataloader):\n","\n","                src_input_ids, src_input_mask, label_ids, b_label_mask = batch # unpacking\n","                src_input_ids = src_input_ids.to(device)\n","                src_input_mask = src_input_mask.to(device)\n","                label_ids = label_ids.to(device)\n","                b_label_mask = b_label_mask.to(device)\n","\n","                self.bert.zero_grad()\n","                self.discriminator.zero_grad()\n","\n","                # Real representations\n","                embedding = self.bert(src_input_ids, attention_mask=src_input_mask)\n","                D_real_features, D_real_logits, D_real_probs = self.discriminator(embedding)\n","\n","                # Random noise\n","                noise = torch.zeros(src_input_ids.shape[0],noise_size, device=device).uniform_(0, 1)#.requires_grad_(True)\n","                gen_rep = self.generator(noise)\n","                \n","                ############################\n","                # Update Generator network: minimize -E[log(D(G(z)))] + feature_matching LOSS\n","                ###########################\n","                D_fake_features, D_fake_logits, D_fake_probs = self.discriminator(gen_rep) # .detach()\n","\n","                g_loss_d = -1 * torch.mean(torch.log(1 - D_fake_probs[:,-1] + epsilon))\n","                g_feat_reg = torch.mean(torch.pow(torch.mean(D_real_features, dim=0) - torch.mean(D_fake_features, dim=0), 2))\n","                g_loss = g_loss_d + g_feat_reg\n","\n","                ############################\n","                #  Update Discriminator network: minimize -E[log(D(x)) + log(1 - D(G(z)))]\n","                ###########################\n","                logits = D_real_logits[:,0:-1]\n","                log_probs = F.log_softmax(logits, dim=-1)\n","                # The discriminator provides an output for labeled and unlabeled real data\n","                # so the loss evaluated for unlabeled data is ignored (masked)\n","                label2one_hot = torch.nn.functional.one_hot(label_ids, len(label_list))\n","                per_example_loss = -torch.sum(label2one_hot * log_probs, dim=-1)\n","                per_example_loss = torch.masked_select(per_example_loss, b_label_mask.to(device))\n","                labeled_example_count = per_example_loss.type(torch.float32).numel()\n","\n","                # It may be the case that a batch does not contain labeled examples,\n","                # so the \"supervised loss\" in this case is not evaluated\n","                if labeled_example_count == 0:\n","                    D_L_Supervised = 0\n","                else:\n","                    D_L_Supervised = torch.div(torch.sum(per_example_loss.to(device)), labeled_example_count)\n","\n","                D_L_unsupervised1U = -1 * torch.mean(torch.log(1 - D_real_probs[:, -1] + epsilon))\n","                D_L_unsupervised2U = -1 * torch.mean(torch.log(D_fake_probs[:, -1] + epsilon))\n","                d_loss = D_L_Supervised + D_L_unsupervised1U + D_L_unsupervised2U\n","\n","                #---------------------------------\n","                #  OPTIMIZATION\n","                #---------------------------------\n","                self.gen_optimizer.zero_grad()\n","                self.dis_optimizer.zero_grad()\n","\n","                # Calculate weigth updates\n","                # retain_graph=True is required since the underlying graph will be deleted after backward\n","                g_loss.backward(retain_graph=True)\n","                d_loss.backward()\n","\n","                # Apply modifications\n","                self.gen_optimizer.step()\n","                self.dis_optimizer.step()\n","\n","                # Save the losses to print them later\n","                tr_g_loss += g_loss.item()\n","                tr_d_loss += d_loss.item()\n","\n","            # Output training stats\n","                if report:\n","                    if step % 100 == 0:\n","                        print('''\\n[Epoch %d/%d][iter %d/%d]\\ttotal Loss_D: %.4f\\ttotal Loss_G: %.4f,\\n\n","                        details of Loss_D:  Loss_D_sup: %.4f,\\t-E[log(D(x))]: %.4f,\\t-E[log(1-D(G(z)))]: %.4f,\\n\n","                        details of Loss_G:  -E[log(D(G(z)))]: %.4f,\\tLoss_G_feat: %.4f\\n\n","                        D(x): %.4f\\tD(G(z)): %.4f'''\n","                          %(epoch+1, epoch_num, step, len(train_dataloader),\n","                            d_loss.mean().item(), g_loss.mean().item(), \n","                            D_L_Supervised, D_L_unsupervised1U, D_L_unsupervised2U,\n","                            g_loss_d, g_feat_reg,\n","                            torch.mean(D_real_probs[:, -1]).item(), \n","                              torch.mean(D_fake_probs[:, -1]).item() ))\n","                        \n","                        # save checkpoints\n","                        self.save_checkpoint(epoch)\n","\n","            # Update the learning rate with the scheduler\n","            self.scheduler_d.step()\n","            self.scheduler_g.step()\n","\n","            # Calculate the average loss over all of the batches.\n","            avg_train_loss_g = tr_g_loss / len(train_dataloader)\n","            avg_train_loss_d = tr_d_loss / len(train_dataloader)\n","\n","            # Measure how long this epoch took.\n","            epoch_time = format_time(time.time() - t0)\n","\n","            print(\"\")\n","            print(f' Training stats at epoch {epoch+1}: ')\n","            print(f' G_loss = {tr_g_loss}, D_loss = {tr_d_loss} \\n')\n","            print(\" Training epcoh took: {:}\".format(epoch_time))\n","            \n","            if val_dataloader != None:\n","                self.bert.eval()\n","                self.discriminator.eval() \n","\n","                all_preds = np.array([])\n","                all_label_ids = np.array([])\n","                eval_loss = 0\n","                nb_eval_steps = 0\n","                for val_step, batch in enumerate(val_dataloader):\n","                    src_input_ids, src_input_mask, label_ids, _ = batch # unpacking\n","                    src_input_ids = src_input_ids.to(device)\n","                    src_input_mask = src_input_mask.to(device)\n","                    label_ids = label_ids.to(device)\n","\n","\n","                    with torch.no_grad():\n","                        doc_rep = self.bert(src_input_ids, attention_mask=src_input_mask)\n","                        _, logits, _ = self.discriminator(doc_rep)\n","#                         probs = torch.nn.functional.softmax(logits[:,0:-1], dim=-1)\n","                        probs = logits[:,0:-1]\n","                        tmp_eval_loss = self.nll_loss(probs, label_ids.view(-1))\n","\n","                    eval_loss += tmp_eval_loss.mean().item()\n","\n","                    probs = probs.detach().cpu().numpy()\n","                    label_ids = label_ids.to('cpu').numpy()\n","                    all_preds = np.append(all_preds, np.argmax(probs, axis=1))\n","                    all_label_ids = np.append(all_label_ids, label_ids)\n","\n","                    nb_eval_steps += 1\n","\n","                eval_loss = eval_loss / nb_eval_steps\n","#                 precision, recall, f1, _ = precision_recall_fscore_support(all_label_ids, all_preds, average=\"micro\",\n","#                                                                          labels=list(range(0,len(label_list))))\n","                mcc = matthews_corrcoef(all_preds, all_label_ids)\n","                acc = (all_preds == all_label_ids).sum().item() / all_label_ids.shape[0]\n","\n","\n","                # Output validation stats\n","                print(f'Validation stats: ')\n","                print('Loss: %.4f,\\tAccuracy: %.4f,\\tmcc: %.4f,'\n","                  %(eval_loss,acc,mcc))\n","                \n","            result = {\n","                'epoch': epoch_time,\n","                \"gen_loss\": tr_g_loss,\n","                \"dis_loss\": tr_d_loss,\n","                \"eval_loss\": eval_loss,\n","                \"mcc\": mcc,\n","                \"acc\": acc,\n","                'epoch_time': epoch_time}\n","#                 \"precision_micro\": precision,\n","#                 \"recall_micro\": recall,\n","#                 \"f1_micro\": f1,\n","                \n","\n","            results.append(result)\n","            # save checkpoints\n","            self.save_checkpoint(epoch,results)\n","            \n","            # seva best model\n","            if acc > best_score:\n","                best_score = acc \n","                self.save_checkpoint(epoch ,result,best=True)\n","            \n","    def save_checkpoint(self,epoch,results=None,best=False):\n","        checkpoint = {\n","            'epoch': epoch + 1,\n","            'bert_state_dict': self.bert.state_dict(),\n","            'disc_state_dict': self.discriminator.state_dict(),\n","            'gen_state_dict': self.generator.state_dict(),\n","            'disc_optimizer_state_dict': self.dis_optimizer.state_dict(),\n","            'gen_optimizer_state_dict': self.gen_optimizer.state_dict(),\n","            }\n","        # for colab : /content/drive/My Drive/Project/checkpoints\n","        if best:\n","            torch.save(checkpoint, f'{self.path}/GAN_BERT_checkpoint_BEST.pth')\n","            if results!= None:\n","                with open(f'{self.path}/results_BEST.pickle', 'wb') as file:\n","                    pickle.dump(results, file)\n","            \n","        else:\n","            torch.save(checkpoint, f'{self.path}/GAN_BERT_checkpoint{epoch+1}.pth')\n","            if results!= None:\n","                with open(f'{self.path}/results.pickle', 'wb') as file:\n","                    pickle.dump(results, file)\n","    \n","    def test(self, test_dataloader):\n","        self.bert.eval()\n","        self.discriminator.eval()\n","\n","        all_preds = np.array([])\n","        all_label_ids = np.array([])\n","        eval_loss = 0\n","        nb_eval_steps = 0\n","        nll_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\n","        \n","        for val_step, batch in enumerate(test_dataloader):\n","            src_input_ids, src_input_mask, label_ids, _ = batch # unpacking\n","            src_input_ids = src_input_ids.to(device)\n","            src_input_mask = src_input_mask.to(device)\n","            label_ids = label_ids.to(device)\n","\n","\n","            with torch.no_grad():\n","                doc_rep = self.bert(src_input_ids, attention_mask=src_input_mask)\n","                _, logits, _ = self.discriminator(doc_rep)\n","            # probs = torch.nn.functional.softmax(logits[:,0:-1], dim=-1)\n","            probs = logits[:,0:-1]    \n","            tmp_eval_loss = nll_loss(probs, label_ids.view(-1))\n","\n","            eval_loss += tmp_eval_loss.mean().item()\n","\n","            probs = probs.detach().cpu().numpy()\n","            label_ids = label_ids.to('cpu').numpy()\n","            \n","            all_preds = np.append(all_preds, np.argmax(probs, axis=1))\n","            all_label_ids = np.append(all_label_ids, label_ids)\n","\n","            nb_eval_steps += 1\n","\n","        eval_loss = eval_loss / nb_eval_steps\n","\n","\n","        mcc = matthews_corrcoef(all_preds, all_label_ids)\n","        acc = (all_preds == all_label_ids).sum().item() / all_label_ids.shape[0]\n","        \n","        # Output validation stats\n","        print(f'Test stats: ')\n","        print('Total loss: %.4f,\\tAccuracy: %.4f,\\tmcc: %.4f,'\n","          %(eval_loss,acc,mcc) ) \n","        return all_preds\n","\n","    @staticmethod\n","    def rename_keys(original_ordered_dict):\n","        new_keys_mapping = dict()\n","        for a in list(original_ordered_dict.keys()):\n","            new_keys_mapping[a] = a.split('module.')[-1] \n","\n","        return OrderedDict((new_keys_mapping.get(k, k), v) for k, v in original_ordered_dict.items())\n","\n","    \n","    def load_checkpoint(self,checkpoint_path):\n","        state_dict = torch.load(checkpoint_path)\n","        \n","        if (device.type == 'cuda') and (ngpu > 1):\n","            # Load the state dictionary into the model\n","            self.bert.load_state_dict(state_dict['bert_state_dict'])\n","            self.discriminator.load_state_dict(state_dict['disc_state_dict'])\n","            self.generator.load_state_dict(state_dict['gen_state_dict'])\n","            self.dis_optimizer.load_state_dict(state_dict['disc_optimizer_state_dict'])\n","            self.gen_optimizer.load_state_dict(state_dict['gen_optimizer_state_dict'])\n","            \n","        else: \n","            self.bert.load_state_dict(self.rename_keys(state_dict['bert_state_dict']))\n","            self.discriminator.load_state_dict(self.rename_keys(state_dict['disc_state_dict']))\n","            self.generator.load_state_dict(self.rename_keys(state_dict['gen_state_dict']))\n","            self.dis_optimizer.load_state_dict(state_dict['disc_optimizer_state_dict'])\n","            self.gen_optimizer.load_state_dict(state_dict['gen_optimizer_state_dict'])\n","\n","        print('Loaded !')\n","            \n","    def plot_results():\n","        pass\n","            \n","    def show_tensorboard():\n","        pass"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# !pip install numba\n","\n","# from numba import cuda\n","# device = cuda.get_current_device()\n","# device.reset()\n","# torch.cuda.empty_cache()  "]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-02-02T10:40:48.007730Z","iopub.status.busy":"2024-02-02T10:40:48.006745Z","iopub.status.idle":"2024-02-02T10:40:48.980310Z","shell.execute_reply":"2024-02-02T10:40:48.979339Z","shell.execute_reply.started":"2024-02-02T10:40:48.007692Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["SubtaskB  part3\n"]}],"source":["# !ls\n","!mkdir part3\n","!ls"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-02-02T10:40:55.173102Z","iopub.status.busy":"2024-02-02T10:40:55.172608Z","iopub.status.idle":"2024-02-02T10:40:55.179673Z","shell.execute_reply":"2024-02-02T10:40:55.178355Z","shell.execute_reply.started":"2024-02-02T10:40:55.173049Z"},"trusted":true},"outputs":[],"source":["ganbert = GANBERT(discriminator, generator1, bert,gen_optimizer, dis_optimizer,\n","                scheduler_d,scheduler_g, path='/kaggle/working/part3') "]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-02-02T10:40:59.968905Z","iopub.status.busy":"2024-02-02T10:40:59.968052Z","iopub.status.idle":"2024-02-02T13:14:34.061148Z","shell.execute_reply":"2024-02-02T13:14:34.060119Z","shell.execute_reply.started":"2024-02-02T10:40:59.968877Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["With labeled_ratio : 0.5\n","\n","Epoch 1/8 :\n","\n","[Epoch 1/8][iter 0/444]\ttotal Loss_D: 4.0050\ttotal Loss_G: 0.1567,\n","\n","                        details of Loss_D:  Loss_D_sup: 1.7802,\t-E[log(D(x))]: 0.1595,\t-E[log(1-D(G(z)))]: 2.0653,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.1381,\tLoss_G_feat: 0.0186\n","\n","                        D(x): 0.1464\tD(G(z)): 0.1287\n","\n","[Epoch 1/8][iter 100/444]\ttotal Loss_D: 4.1242\ttotal Loss_G: 0.1571,\n","\n","                        details of Loss_D:  Loss_D_sup: 1.8994,\t-E[log(D(x))]: 0.1696,\t-E[log(1-D(G(z)))]: 2.0551,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.1400,\tLoss_G_feat: 0.0171\n","\n","                        D(x): 0.1549\tD(G(z)): 0.1303\n","\n","[Epoch 1/8][iter 200/444]\ttotal Loss_D: 4.1256\ttotal Loss_G: 0.1593,\n","\n","                        details of Loss_D:  Loss_D_sup: 1.9195,\t-E[log(D(x))]: 0.1614,\t-E[log(1-D(G(z)))]: 2.0447,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.1412,\tLoss_G_feat: 0.0181\n","\n","                        D(x): 0.1479\tD(G(z)): 0.1314\n","\n","[Epoch 1/8][iter 300/444]\ttotal Loss_D: 3.9689\ttotal Loss_G: 0.1571,\n","\n","                        details of Loss_D:  Loss_D_sup: 1.7533,\t-E[log(D(x))]: 0.1573,\t-E[log(1-D(G(z)))]: 2.0583,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.1392,\tLoss_G_feat: 0.0179\n","\n","                        D(x): 0.1445\tD(G(z)): 0.1297\n","\n","[Epoch 1/8][iter 400/444]\ttotal Loss_D: 3.9899\ttotal Loss_G: 0.1616,\n","\n","                        details of Loss_D:  Loss_D_sup: 1.8126,\t-E[log(D(x))]: 0.1485,\t-E[log(1-D(G(z)))]: 2.0288,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.1435,\tLoss_G_feat: 0.0181\n","\n","                        D(x): 0.1371\tD(G(z)): 0.1334\n","\n"," Training stats at epoch 1: \n"," G_loss = 69.90765023231506, D_loss = 1808.82297706604 \n","\n"," Training epcoh took: 0:17:52\n","Validation stats: \n","Loss: 1.8191,\tAccuracy: 0.1668,\tmcc: 0.0029,\n","Epoch 2/8 :\n","\n","[Epoch 2/8][iter 0/444]\ttotal Loss_D: 4.1227\ttotal Loss_G: 0.1571,\n","\n","                        details of Loss_D:  Loss_D_sup: 1.9015,\t-E[log(D(x))]: 0.1621,\t-E[log(1-D(G(z)))]: 2.0592,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.1386,\tLoss_G_feat: 0.0185\n","\n","                        D(x): 0.1486\tD(G(z)): 0.1292\n","\n","[Epoch 2/8][iter 100/444]\ttotal Loss_D: 3.7302\ttotal Loss_G: 0.1905,\n","\n","                        details of Loss_D:  Loss_D_sup: 1.7978,\t-E[log(D(x))]: 0.1134,\t-E[log(1-D(G(z)))]: 1.8191,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.1811,\tLoss_G_feat: 0.0094\n","\n","                        D(x): 0.1069\tD(G(z)): 0.1651\n","\n","[Epoch 2/8][iter 200/444]\ttotal Loss_D: 3.4715\ttotal Loss_G: 0.2413,\n","\n","                        details of Loss_D:  Loss_D_sup: 1.7698,\t-E[log(D(x))]: 0.0870,\t-E[log(1-D(G(z)))]: 1.6146,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.2280,\tLoss_G_feat: 0.0133\n","\n","                        D(x): 0.0831\tD(G(z)): 0.2029\n","\n","[Epoch 2/8][iter 300/444]\ttotal Loss_D: 3.2155\ttotal Loss_G: 0.3146,\n","\n","                        details of Loss_D:  Loss_D_sup: 1.7468,\t-E[log(D(x))]: 0.0710,\t-E[log(1-D(G(z)))]: 1.3977,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.2941,\tLoss_G_feat: 0.0205\n","\n","                        D(x): 0.0683\tD(G(z)): 0.2528\n","\n","[Epoch 2/8][iter 400/444]\ttotal Loss_D: 2.9404\ttotal Loss_G: 0.4273,\n","\n","                        details of Loss_D:  Loss_D_sup: 1.7407,\t-E[log(D(x))]: 0.0557,\t-E[log(1-D(G(z)))]: 1.1440,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.3964,\tLoss_G_feat: 0.0309\n","\n","                        D(x): 0.0540\tD(G(z)): 0.3243\n","\n"," Training stats at epoch 2: \n"," G_loss = 122.50499753654003, D_loss = 1525.010344028473 \n","\n"," Training epcoh took: 0:17:43\n","Validation stats: \n","Loss: 1.6563,\tAccuracy: 0.4093,\tmcc: 0.3202,\n","Epoch 3/8 :\n","\n","[Epoch 3/8][iter 0/444]\ttotal Loss_D: 2.8396\ttotal Loss_G: 0.4529,\n","\n","                        details of Loss_D:  Loss_D_sup: 1.6800,\t-E[log(D(x))]: 0.0516,\t-E[log(1-D(G(z)))]: 1.1080,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.4189,\tLoss_G_feat: 0.0340\n","\n","                        D(x): 0.0501\tD(G(z)): 0.3380\n","\n","[Epoch 3/8][iter 100/444]\ttotal Loss_D: 2.2580\ttotal Loss_G: 0.6198,\n","\n","                        details of Loss_D:  Loss_D_sup: 1.3386,\t-E[log(D(x))]: 0.0374,\t-E[log(1-D(G(z)))]: 0.8820,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.5615,\tLoss_G_feat: 0.0583\n","\n","                        D(x): 0.0365\tD(G(z)): 0.4226\n","\n","[Epoch 3/8][iter 200/444]\ttotal Loss_D: 2.0316\ttotal Loss_G: 0.6844,\n","\n","                        details of Loss_D:  Loss_D_sup: 1.1909,\t-E[log(D(x))]: 0.0305,\t-E[log(1-D(G(z)))]: 0.8102,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.6227,\tLoss_G_feat: 0.0616\n","\n","                        D(x): 0.0299\tD(G(z)): 0.4550\n","\n","[Epoch 3/8][iter 300/444]\ttotal Loss_D: 1.8170\ttotal Loss_G: 0.7257,\n","\n","                        details of Loss_D:  Loss_D_sup: 1.0283,\t-E[log(D(x))]: 0.0296,\t-E[log(1-D(G(z)))]: 0.7591,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.6686,\tLoss_G_feat: 0.0571\n","\n","                        D(x): 0.0290\tD(G(z)): 0.4778\n","\n","[Epoch 3/8][iter 400/444]\ttotal Loss_D: 1.7742\ttotal Loss_G: 0.7089,\n","\n","                        details of Loss_D:  Loss_D_sup: 0.9705,\t-E[log(D(x))]: 0.0268,\t-E[log(1-D(G(z)))]: 0.7769,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.6535,\tLoss_G_feat: 0.0553\n","\n","                        D(x): 0.0262\tD(G(z)): 0.4696\n","\n"," Training stats at epoch 3: \n"," G_loss = 298.70788714289665, D_loss = 883.539324760437 \n","\n"," Training epcoh took: 0:17:42\n","Validation stats: \n","Loss: 0.8967,\tAccuracy: 0.6545,\tmcc: 0.5960,\n","Epoch 4/8 :\n","\n","[Epoch 4/8][iter 0/444]\ttotal Loss_D: 1.7075\ttotal Loss_G: 0.6891,\n","\n","                        details of Loss_D:  Loss_D_sup: 0.8962,\t-E[log(D(x))]: 0.0260,\t-E[log(1-D(G(z)))]: 0.7853,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.6378,\tLoss_G_feat: 0.0512\n","\n","                        D(x): 0.0254\tD(G(z)): 0.4640\n","\n","[Epoch 4/8][iter 100/444]\ttotal Loss_D: 1.5516\ttotal Loss_G: 0.7542,\n","\n","                        details of Loss_D:  Loss_D_sup: 0.8099,\t-E[log(D(x))]: 0.0212,\t-E[log(1-D(G(z)))]: 0.7204,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.7024,\tLoss_G_feat: 0.0518\n","\n","                        D(x): 0.0209\tD(G(z)): 0.4954\n","\n","[Epoch 4/8][iter 200/444]\ttotal Loss_D: 1.4285\ttotal Loss_G: 0.7349,\n","\n","                        details of Loss_D:  Loss_D_sup: 0.6733,\t-E[log(D(x))]: 0.0218,\t-E[log(1-D(G(z)))]: 0.7334,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.6869,\tLoss_G_feat: 0.0480\n","\n","                        D(x): 0.0214\tD(G(z)): 0.4888\n","\n","[Epoch 4/8][iter 300/444]\ttotal Loss_D: 1.2843\ttotal Loss_G: 0.7468,\n","\n","                        details of Loss_D:  Loss_D_sup: 0.5490,\t-E[log(D(x))]: 0.0194,\t-E[log(1-D(G(z)))]: 0.7158,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.7007,\tLoss_G_feat: 0.0462\n","\n","                        D(x): 0.0190\tD(G(z)): 0.4964\n","\n","[Epoch 4/8][iter 400/444]\ttotal Loss_D: 1.5267\ttotal Loss_G: 0.7037,\n","\n","                        details of Loss_D:  Loss_D_sup: 0.7441,\t-E[log(D(x))]: 0.0184,\t-E[log(1-D(G(z)))]: 0.7642,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.6556,\tLoss_G_feat: 0.0481\n","\n","                        D(x): 0.0181\tD(G(z)): 0.4734\n","\n"," Training stats at epoch 4: \n"," G_loss = 320.6014759540558, D_loss = 649.2512899637222 \n","\n"," Training epcoh took: 0:17:43\n","Validation stats: \n","Loss: 0.7442,\tAccuracy: 0.7296,\tmcc: 0.6824,\n","Epoch 5/8 :\n","\n","[Epoch 5/8][iter 0/444]\ttotal Loss_D: 1.4326\ttotal Loss_G: 0.7368,\n","\n","                        details of Loss_D:  Loss_D_sup: 0.6899,\t-E[log(D(x))]: 0.0182,\t-E[log(1-D(G(z)))]: 0.7246,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.6927,\tLoss_G_feat: 0.0441\n","\n","                        D(x): 0.0178\tD(G(z)): 0.4924\n","\n","[Epoch 5/8][iter 100/444]\ttotal Loss_D: 1.3056\ttotal Loss_G: 0.7000,\n","\n","                        details of Loss_D:  Loss_D_sup: 0.5285,\t-E[log(D(x))]: 0.0170,\t-E[log(1-D(G(z)))]: 0.7601,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.6580,\tLoss_G_feat: 0.0420\n","\n","                        D(x): 0.0167\tD(G(z)): 0.4750\n","\n","[Epoch 5/8][iter 200/444]\ttotal Loss_D: 1.1290\ttotal Loss_G: 0.7192,\n","\n","                        details of Loss_D:  Loss_D_sup: 0.3731,\t-E[log(D(x))]: 0.0161,\t-E[log(1-D(G(z)))]: 0.7398,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.6807,\tLoss_G_feat: 0.0385\n","\n","                        D(x): 0.0158\tD(G(z)): 0.4855\n","\n","[Epoch 5/8][iter 300/444]\ttotal Loss_D: 1.2096\ttotal Loss_G: 0.7211,\n","\n","                        details of Loss_D:  Loss_D_sup: 0.4645,\t-E[log(D(x))]: 0.0132,\t-E[log(1-D(G(z)))]: 0.7318,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.6829,\tLoss_G_feat: 0.0382\n","\n","                        D(x): 0.0130\tD(G(z)): 0.4880\n","\n","[Epoch 5/8][iter 400/444]\ttotal Loss_D: 1.2274\ttotal Loss_G: 0.7251,\n","\n","                        details of Loss_D:  Loss_D_sup: 0.4803,\t-E[log(D(x))]: 0.0142,\t-E[log(1-D(G(z)))]: 0.7329,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.6827,\tLoss_G_feat: 0.0424\n","\n","                        D(x): 0.0139\tD(G(z)): 0.4878\n","\n"," Training stats at epoch 5: \n"," G_loss = 318.51246160268784, D_loss = 571.7051740884781 \n","\n"," Training epcoh took: 0:17:44\n","Validation stats: \n","Loss: 0.6777,\tAccuracy: 0.7629,\tmcc: 0.7211,\n","Epoch 6/8 :\n","\n","[Epoch 6/8][iter 0/444]\ttotal Loss_D: 1.0831\ttotal Loss_G: 0.6953,\n","\n","                        details of Loss_D:  Loss_D_sup: 0.3027,\t-E[log(D(x))]: 0.0132,\t-E[log(1-D(G(z)))]: 0.7673,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.6536,\tLoss_G_feat: 0.0417\n","\n","                        D(x): 0.0130\tD(G(z)): 0.4726\n","\n","[Epoch 6/8][iter 100/444]\ttotal Loss_D: 1.0776\ttotal Loss_G: 0.7503,\n","\n","                        details of Loss_D:  Loss_D_sup: 0.3611,\t-E[log(D(x))]: 0.0119,\t-E[log(1-D(G(z)))]: 0.7045,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.7140,\tLoss_G_feat: 0.0362\n","\n","                        D(x): 0.0118\tD(G(z)): 0.5021\n","\n","[Epoch 6/8][iter 200/444]\ttotal Loss_D: 1.0943\ttotal Loss_G: 0.7269,\n","\n","                        details of Loss_D:  Loss_D_sup: 0.3578,\t-E[log(D(x))]: 0.0120,\t-E[log(1-D(G(z)))]: 0.7245,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.6912,\tLoss_G_feat: 0.0356\n","\n","                        D(x): 0.0118\tD(G(z)): 0.4918\n","\n","[Epoch 6/8][iter 300/444]\ttotal Loss_D: 1.0852\ttotal Loss_G: 0.7323,\n","\n","                        details of Loss_D:  Loss_D_sup: 0.3525,\t-E[log(D(x))]: 0.0125,\t-E[log(1-D(G(z)))]: 0.7201,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.6989,\tLoss_G_feat: 0.0335\n","\n","                        D(x): 0.0123\tD(G(z)): 0.4946\n","\n","[Epoch 6/8][iter 400/444]\ttotal Loss_D: 1.0838\ttotal Loss_G: 0.6960,\n","\n","                        details of Loss_D:  Loss_D_sup: 0.3124,\t-E[log(D(x))]: 0.0103,\t-E[log(1-D(G(z)))]: 0.7611,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.6605,\tLoss_G_feat: 0.0355\n","\n","                        D(x): 0.0102\tD(G(z)): 0.4755\n","\n"," Training stats at epoch 6: \n"," G_loss = 317.42171889543533, D_loss = 515.0222570896149 \n","\n"," Training epcoh took: 0:17:44\n","Validation stats: \n","Loss: 0.8303,\tAccuracy: 0.7378,\tmcc: 0.6947,\n","Epoch 7/8 :\n","\n","[Epoch 7/8][iter 0/444]\ttotal Loss_D: 1.0038\ttotal Loss_G: 0.7156,\n","\n","                        details of Loss_D:  Loss_D_sup: 0.2638,\t-E[log(D(x))]: 0.0131,\t-E[log(1-D(G(z)))]: 0.7269,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.6840,\tLoss_G_feat: 0.0317\n","\n","                        D(x): 0.0127\tD(G(z)): 0.4896\n","\n","[Epoch 7/8][iter 100/444]\ttotal Loss_D: 1.0496\ttotal Loss_G: 0.7126,\n","\n","                        details of Loss_D:  Loss_D_sup: 0.3050,\t-E[log(D(x))]: 0.0115,\t-E[log(1-D(G(z)))]: 0.7332,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.6836,\tLoss_G_feat: 0.0291\n","\n","                        D(x): 0.0113\tD(G(z)): 0.4878\n","\n","[Epoch 7/8][iter 200/444]\ttotal Loss_D: 0.9056\ttotal Loss_G: 0.7289,\n","\n","                        details of Loss_D:  Loss_D_sup: 0.1739,\t-E[log(D(x))]: 0.0086,\t-E[log(1-D(G(z)))]: 0.7232,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.6956,\tLoss_G_feat: 0.0333\n","\n","                        D(x): 0.0085\tD(G(z)): 0.4931\n","\n","[Epoch 7/8][iter 300/444]\ttotal Loss_D: 0.9518\ttotal Loss_G: 0.7022,\n","\n","                        details of Loss_D:  Loss_D_sup: 0.2018,\t-E[log(D(x))]: 0.0113,\t-E[log(1-D(G(z)))]: 0.7387,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.6725,\tLoss_G_feat: 0.0297\n","\n","                        D(x): 0.0112\tD(G(z)): 0.4837\n","\n","[Epoch 7/8][iter 400/444]\ttotal Loss_D: 1.0326\ttotal Loss_G: 0.7030,\n","\n","                        details of Loss_D:  Loss_D_sup: 0.2849,\t-E[log(D(x))]: 0.0096,\t-E[log(1-D(G(z)))]: 0.7380,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.6751,\tLoss_G_feat: 0.0279\n","\n","                        D(x): 0.0095\tD(G(z)): 0.4845\n","\n"," Training stats at epoch 7: \n"," G_loss = 316.2200320959091, D_loss = 476.7541116476059 \n","\n"," Training epcoh took: 0:17:42\n","Validation stats: \n","Loss: 0.7198,\tAccuracy: 0.7744,\tmcc: 0.7362,\n","Epoch 8/8 :\n","\n","[Epoch 8/8][iter 0/444]\ttotal Loss_D: 0.9450\ttotal Loss_G: 0.7284,\n","\n","                        details of Loss_D:  Loss_D_sup: 0.2247,\t-E[log(D(x))]: 0.0103,\t-E[log(1-D(G(z)))]: 0.7100,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.6983,\tLoss_G_feat: 0.0301\n","\n","                        D(x): 0.0101\tD(G(z)): 0.4970\n","\n","[Epoch 8/8][iter 100/444]\ttotal Loss_D: 1.0973\ttotal Loss_G: 0.6731,\n","\n","                        details of Loss_D:  Loss_D_sup: 0.3140,\t-E[log(D(x))]: 0.0088,\t-E[log(1-D(G(z)))]: 0.7745,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.6455,\tLoss_G_feat: 0.0275\n","\n","                        D(x): 0.0087\tD(G(z)): 0.4683\n","\n","[Epoch 8/8][iter 200/444]\ttotal Loss_D: 1.0424\ttotal Loss_G: 0.7157,\n","\n","                        details of Loss_D:  Loss_D_sup: 0.3091,\t-E[log(D(x))]: 0.0081,\t-E[log(1-D(G(z)))]: 0.7252,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.6869,\tLoss_G_feat: 0.0288\n","\n","                        D(x): 0.0080\tD(G(z)): 0.4907\n","\n","[Epoch 8/8][iter 300/444]\ttotal Loss_D: 1.0018\ttotal Loss_G: 0.7084,\n","\n","                        details of Loss_D:  Loss_D_sup: 0.2623,\t-E[log(D(x))]: 0.0079,\t-E[log(1-D(G(z)))]: 0.7315,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.6835,\tLoss_G_feat: 0.0249\n","\n","                        D(x): 0.0078\tD(G(z)): 0.4883\n","\n","[Epoch 8/8][iter 400/444]\ttotal Loss_D: 0.9734\ttotal Loss_G: 0.6861,\n","\n","                        details of Loss_D:  Loss_D_sup: 0.2140,\t-E[log(D(x))]: 0.0075,\t-E[log(1-D(G(z)))]: 0.7519,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.6592,\tLoss_G_feat: 0.0270\n","\n","                        D(x): 0.0074\tD(G(z)): 0.4774\n","\n"," Training stats at epoch 8: \n"," G_loss = 315.25574862957, D_loss = 440.4342610836029 \n","\n"," Training epcoh took: 0:17:43\n","Validation stats: \n","Loss: 0.7017,\tAccuracy: 0.7964,\tmcc: 0.7615,\n"]}],"source":["ganbert.trainer(epoch_num,label_list,labeled_ratio,train_dataloader, val_dataloader,report=True)"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-02-02T13:15:34.451691Z","iopub.status.busy":"2024-02-02T13:15:34.451000Z","iopub.status.idle":"2024-02-02T13:15:50.937425Z","shell.execute_reply":"2024-02-02T13:15:50.936295Z","shell.execute_reply.started":"2024-02-02T13:15:34.451652Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Test stats: \n","Total loss: 2.1642,\tAccuracy: 0.5450,\tmcc: 0.4876,\n"]}],"source":["test_res = ganbert.test(test_dataloader) "]},{"cell_type":"markdown","metadata":{"id":"x1zcuLgibO3X"},"source":["The `Matthews correlation coefficient` , is a measure of the quality of classifications in machine learning. It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. It's defined in the range from -1 to 1, with 1 being a perfect prediction, 0 being the result of a random prediction, and -1 indicating total disagreement between prediction and observation."]},{"cell_type":"markdown","metadata":{"id":"Hm86KFjT1d9I"},"source":["---\n","### Load the best model"]},{"cell_type":"code","execution_count":179,"metadata":{"execution":{"iopub.execute_input":"2024-02-02T14:20:37.212522Z","iopub.status.busy":"2024-02-02T14:20:37.212060Z","iopub.status.idle":"2024-02-02T14:20:37.236952Z","shell.execute_reply":"2024-02-02T14:20:37.236003Z","shell.execute_reply.started":"2024-02-02T14:20:37.212492Z"},"trusted":true},"outputs":[],"source":["discriminator = Discriminator(input_size,num_classes).to(device)\n","generator1 = Generator1(noise_size,input_size).to(device)\n","bert = BERT_Embedder(bert_model).to(device)\n","\n","if (device.type == 'cuda') and (ngpu > 1):\n","    discriminator = nn.DataParallel(discriminator, list(range(ngpu)))\n","    generator = nn.DataParallel(generator1, list(range(ngpu)))    \n","    bert = nn.DataParallel(bert, list(range(ngpu)))\n","    \n","gen_optimizer = torch.optim.AdamW(generator1.parameters(), lr=learning_rate)\n","dis_optimizer = torch.optim.AdamW(list(bert.parameters()) + list(discriminator.parameters()), lr=learning_rate)\n","\n","#scheduler\n","num_train_examples = len(train_dataset)\n","num_train_steps = int(num_train_examples / batch_size * epoch_num) \n","num_warmup_steps = int(num_train_steps * warmup_proportion)\n","\n","scheduler_d = get_constant_schedule_with_warmup(dis_optimizer, \n","                                       num_warmup_steps = num_warmup_steps)\n","scheduler_g = get_constant_schedule_with_warmup(gen_optimizer, \n","                                       num_warmup_steps = num_warmup_steps) \n","\n","ganbert_best = GANBERT(discriminator, generator1, bert,gen_optimizer, dis_optimizer,\n","                scheduler_d,scheduler_g, path='/kaggle/working/part3') "]},{"cell_type":"code","execution_count":180,"metadata":{"execution":{"iopub.execute_input":"2024-02-02T14:20:42.693854Z","iopub.status.busy":"2024-02-02T14:20:42.693505Z","iopub.status.idle":"2024-02-02T14:20:43.176735Z","shell.execute_reply":"2024-02-02T14:20:43.175794Z","shell.execute_reply.started":"2024-02-02T14:20:42.693827Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded !\n"]}],"source":["path = '/kaggle/working/part3/GAN_BERT_checkpoint_BEST.pth'\n","ganbert_best.load_checkpoint(path)"]},{"cell_type":"code","execution_count":182,"metadata":{"execution":{"iopub.execute_input":"2024-02-02T14:21:11.705667Z","iopub.status.busy":"2024-02-02T14:21:11.704759Z","iopub.status.idle":"2024-02-02T14:21:28.205914Z","shell.execute_reply":"2024-02-02T14:21:28.204866Z","shell.execute_reply.started":"2024-02-02T14:21:11.705635Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Test stats: \n","Total loss: 2.1513,\tAccuracy: 0.5450,\tmcc: 0.4876,\n"]}],"source":["test_res = ganbert_best.test(test_dataloader) "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-02-02T14:17:43.589081Z","iopub.status.idle":"2024-02-02T14:17:43.589549Z","shell.execute_reply":"2024-02-02T14:17:43.589349Z","shell.execute_reply.started":"2024-02-02T14:17:43.589329Z"},"id":"aDZl0qCwSDUH","trusted":true},"outputs":[],"source":["plt.figure(figsize=(10,5))\n","plt.title(\"Generator and Discriminator Loss During Training\")\n","plt.plot(G_losses,label=\"G\")\n","plt.plot(D_losses,label=\"D\")\n","plt.xlabel(\"iterations\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30648,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}

{"cells":[{"cell_type":"markdown","metadata":{"id":"vRSJfpfXQkzn"},"source":["# Deep Learning Project - Part3\n","<div style=\"text-align: center\">\n","<h1 style = \"color: red\"> Sharif University Of Technology</h1>\n","<h2 style = \"color: green\"> DR. Fatemizadeh </h2>\n","<h3 style = \"color: cyan\"> Authors: Amirreza Velaee - Hessam Hosseini - Amirabbas Afzali - Mahshad Moradi<h3>\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T20:49:15.914411Z","iopub.status.busy":"2024-02-03T20:49:15.914048Z","iopub.status.idle":"2024-02-03T20:49:15.923091Z","shell.execute_reply":"2024-02-03T20:49:15.922285Z","shell.execute_reply.started":"2024-02-03T20:49:15.914385Z"},"id":"oeTnUDU-Op7F","outputId":"8c5f18ba-0507-4811-8895-77fce1145412","trusted":true},"outputs":[],"source":["import argparse\n","import os\n","import random\n","import torch\n","import torch.nn as nn \n","import torch.nn.parallel\n","import torch.optim as optim\n","import torch.utils.data\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","from IPython.display import HTML\n","import json\n","from tqdm import tqdm, trange\n","from sklearn.metrics import precision_recall_fscore_support, matthews_corrcoef\n","import pickle\n","from torch.utils.data import random_split\n","from torch.utils.data import Dataset, Subset\n","from torch.utils.data import DataLoader,Dataset\n","from torch.nn.modules import ReLU,Linear,Dropout\n","import time\n","import math\n","import datetime\n","import torch.nn.functional as F\n","from collections import OrderedDict\n","\n","\n","# Set random seed for reproducibility\n","manualSeed = 42\n","print(\"Random Seed: \", manualSeed)\n","random.seed(manualSeed)\n","torch.manual_seed(manualSeed)\n","\n","ngpu = 1"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T20:49:15.924394Z","iopub.status.busy":"2024-02-03T20:49:15.924106Z","iopub.status.idle":"2024-02-03T20:49:15.945134Z","shell.execute_reply":"2024-02-03T20:49:15.944423Z","shell.execute_reply.started":"2024-02-03T20:49:15.924371Z"},"id":"ZAnJ2iOIQsyq","outputId":"7384417a-2a7b-419b-d81a-3cc3b66cefcd","trusted":true},"outputs":[],"source":["# If there's a GPU available...\n","if torch.cuda.is_available():    \n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")\n","    \n","ngpu = torch.cuda.device_count()"]},{"cell_type":"markdown","metadata":{"id":"A2EynVlITXhz"},"source":["## load the dataset:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T20:49:15.946399Z","iopub.status.busy":"2024-02-03T20:49:15.946113Z","iopub.status.idle":"2024-02-03T20:49:28.151720Z","shell.execute_reply":"2024-02-03T20:49:28.150769Z","shell.execute_reply.started":"2024-02-03T20:49:15.946377Z"},"trusted":true},"outputs":[],"source":["!pip install gdown \n","import gdown "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T20:49:28.153571Z","iopub.status.busy":"2024-02-03T20:49:28.153219Z","iopub.status.idle":"2024-02-03T20:49:30.725922Z","shell.execute_reply":"2024-02-03T20:49:30.724928Z","shell.execute_reply.started":"2024-02-03T20:49:28.153544Z"},"id":"IsNFXLLDTpJk","outputId":"3c00b8ab-3143-4460-ed79-a708447a8689","trusted":true},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')\n","\n","# id = \"11YeloR2eTXcTzdwI04Z-M2QVvIeQAU6-\"\n","id = \"1G-XttJCGvkAVkU9N_W_PxR0Cx099qbUa\"\n","gdown.download_folder(id=id, quiet=True, use_cookies=False)"]},{"cell_type":"markdown","metadata":{"id":"dncLXjy5VWUd"},"source":["**Subtask B:**\n","\n","An object of the JSON has the following format:"]},{"cell_type":"markdown","metadata":{"id":"WV5LP7CRoOqY"},"source":["\n","-  **id** -> identifier of the example,\n","- **label** -> label (human: 0, chatGPT: 1, cohere: 2, davinci: 3, bloomz: 4, dolly: 5),\n","- **text** -> text generated by machine or written by human,\n","- **model** -> model name that generated data,\n","- **source** -> source (Wikipedia, Wikihow, Peerread, Reddit, Arxiv) on English\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T20:49:34.490901Z","iopub.status.busy":"2024-02-03T20:49:34.490128Z","iopub.status.idle":"2024-02-03T20:49:35.668106Z","shell.execute_reply":"2024-02-03T20:49:35.667264Z","shell.execute_reply.started":"2024-02-03T20:49:34.490870Z"},"id":"jtqmb0FPTeYc","trusted":true},"outputs":[],"source":["# content/drive/My Drive/Project\n","with open('/kaggle/working/SubtaskB/subtaskB_train.jsonl', 'r') as file:    \n","    lines = file.readlines()\n","\n","# Parse each line as a JSON object\n","train_objects = [json.loads(line) for line in lines] "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T20:49:37.534213Z","iopub.status.busy":"2024-02-03T20:49:37.533512Z","iopub.status.idle":"2024-02-03T20:49:37.587748Z","shell.execute_reply":"2024-02-03T20:49:37.586978Z","shell.execute_reply.started":"2024-02-03T20:49:37.534184Z"},"id":"ujtd9MmMUUFL","trusted":true},"outputs":[],"source":["with open('/kaggle/working/SubtaskB/subtaskB_dev.jsonl', 'r') as file:\n","    lines = file.readlines()\n","\n","dev_objects = [json.loads(line) for line in lines]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T20:49:37.824908Z","iopub.status.busy":"2024-02-03T20:49:37.824543Z","iopub.status.idle":"2024-02-03T20:49:37.831350Z","shell.execute_reply":"2024-02-03T20:49:37.830426Z","shell.execute_reply.started":"2024-02-03T20:49:37.824879Z"},"id":"rBcEi416T8iQ","outputId":"5994de5e-5f23-4711-e326-cc29da869334","trusted":true},"outputs":[],"source":["len(train_objects), len(dev_objects) "]},{"cell_type":"markdown","metadata":{"id":"VYTe6PmLV0LW"},"source":["an example:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T20:49:39.902391Z","iopub.status.busy":"2024-02-03T20:49:39.901996Z","iopub.status.idle":"2024-02-03T20:49:39.909602Z","shell.execute_reply":"2024-02-03T20:49:39.908622Z","shell.execute_reply.started":"2024-02-03T20:49:39.902361Z"},"id":"NWJNu5ETVuzX","outputId":"7d10092c-6bbb-468f-e6f1-7f0eb76f74f8","trusted":true},"outputs":[],"source":["train_objects[100].keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T20:49:40.222156Z","iopub.status.busy":"2024-02-03T20:49:40.221772Z","iopub.status.idle":"2024-02-03T20:49:40.229656Z","shell.execute_reply":"2024-02-03T20:49:40.228710Z","shell.execute_reply.started":"2024-02-03T20:49:40.222125Z"},"id":"LxK-CV8fV-46","outputId":"68c4f9ce-1c9b-4201-939a-34cfbbc0fd2d","trusted":true},"outputs":[],"source":["train_objects[100]['model'], train_objects[100]['source'],train_objects[100]['label'],train_objects[100]['id']"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T20:49:40.781630Z","iopub.status.busy":"2024-02-03T20:49:40.781254Z","iopub.status.idle":"2024-02-03T20:49:40.787188Z","shell.execute_reply":"2024-02-03T20:49:40.786130Z","shell.execute_reply.started":"2024-02-03T20:49:40.781593Z"},"id":"OgxeDd4pV5fn","outputId":"cb6fed31-5408-482f-f129-2a4e69ddb634","trusted":true},"outputs":[],"source":["print(train_objects[104]['text'])"]},{"cell_type":"markdown","metadata":{},"source":["More details about the dataset and the Exploratory Data Analysis have been reported in `EDA.ipynb`."]},{"cell_type":"markdown","metadata":{"id":"ubyocB_epmnh"},"source":["## load the pretrained `RoBERTa`/`DidstilBert` from $huggingface$:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T20:49:44.413341Z","iopub.status.busy":"2024-02-03T20:49:44.412953Z","iopub.status.idle":"2024-02-03T20:50:08.653589Z","shell.execute_reply":"2024-02-03T20:50:08.652462Z","shell.execute_reply.started":"2024-02-03T20:49:44.413311Z"},"id":"dKyrJEtPQSzH","outputId":"02800931-b2b5-448a-dd7f-f13ff732d00c","trusted":true},"outputs":[],"source":["!pip install transformers\n","!pip install sentencepiece"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T20:50:08.655990Z","iopub.status.busy":"2024-02-03T20:50:08.655682Z","iopub.status.idle":"2024-02-03T20:50:12.287101Z","shell.execute_reply":"2024-02-03T20:50:12.286279Z","shell.execute_reply.started":"2024-02-03T20:50:08.655963Z"},"id":"sOOunD55QVXY","trusted":true},"outputs":[],"source":["from transformers import RobertaTokenizer, RobertaModel\n","from transformers import DistilBertTokenizer, DistilBertModel\n","\n","import sentencepiece\n","from transformers import get_constant_schedule_with_warmup"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T20:50:12.288694Z","iopub.status.busy":"2024-02-03T20:50:12.288222Z","iopub.status.idle":"2024-02-03T20:50:15.209684Z","shell.execute_reply":"2024-02-03T20:50:15.208727Z","shell.execute_reply.started":"2024-02-03T20:50:12.288668Z"},"id":"zZp3vmPPQiqs","outputId":"47183acf-b17b-4120-d696-481f1657c79b","trusted":true},"outputs":[],"source":["# Load pre-trained BERT model and tokenizer\n","# model_name = 'roberta-large'\n","# tokenizer = RobertaTokenizer.from_pretrained(model_name)\n","# bert_model = RobertaModel.from_pretrained(model_name)\n","\n","model_name = 'distilbert-base-uncased'\n","tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n","bert_model = DistilBertModel.from_pretrained(model_name)\n","\n","bert_generator = DistilBertModel.from_pretrained(model_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T20:50:15.211819Z","iopub.status.busy":"2024-02-03T20:50:15.211528Z","iopub.status.idle":"2024-02-03T20:50:15.217778Z","shell.execute_reply":"2024-02-03T20:50:15.216763Z","shell.execute_reply.started":"2024-02-03T20:50:15.211794Z"},"id":"6DyPAryTRPeF","trusted":true},"outputs":[],"source":["class BERT_Embedder(nn.Module):\n","    def __init__(self, bert_modele):\n","        super(BERT_Embedder, self).__init__()\n","        self.bert = bert_modele\n","\n","    def forward(self, encoded_ids,attention_mask):\n","        outputs = self.bert(encoded_ids,attention_mask)\n","        last_hidden_states = outputs.last_hidden_state[:,0]  # return embedding of 'CLS' token for classification.\n","\n","        return last_hidden_states"]},{"cell_type":"markdown","metadata":{},"source":["# **Bag of Words**"]},{"cell_type":"markdown","metadata":{},"source":["## Data cleaning"]},{"cell_type":"markdown","metadata":{},"source":["- Expand Contractions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T20:50:25.078603Z","iopub.status.busy":"2024-02-03T20:50:25.078253Z","iopub.status.idle":"2024-02-03T20:50:25.096101Z","shell.execute_reply":"2024-02-03T20:50:25.095130Z","shell.execute_reply.started":"2024-02-03T20:50:25.078576Z"},"trusted":true},"outputs":[],"source":["# For regular expressions\n","import re\n","\n","# Dictionary of English Contractions\n","contractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\n","                     \"can't\": \"cannot\",\"can't've\": \"cannot have\",\n","                     \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n","                     \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n","                     \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n","                     \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n","                     \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n","                     \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n","                     \"I'd\": \"I would\", \"I'd've\": \"I would have\",\"I'll\": \"I will\",\n","                     \"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\", \"isn't\": \"is not\",\n","                     \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n","                     \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n","                     \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n","                     \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n","                     \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n","                     \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n","                     \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n","                     \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n","                     \"she'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n","                     \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n","                     \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n","                     \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n","                     \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n","                     \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\n","                     \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n","                     \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n","                     \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n","                     \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n","                     \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n","                     \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n","                     \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n","                     \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n","                     \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n","                     \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n","                     \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n","                     \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n","                     \"you've\": \"you have\"}\n","\n","# Regular expression for finding contractions\n","contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n","\n","# Function for expanding contractions\n","def expand_contractions(text,contractions_dict=contractions_dict):\n","    def replace(match):\n","        return contractions_dict[match.group(0)]\n","    return contractions_re.sub(replace, text)\n"]},{"cell_type":"markdown","metadata":{},"source":["- Remove digits and words containing digits"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T20:50:25.851335Z","iopub.status.busy":"2024-02-03T20:50:25.850685Z","iopub.status.idle":"2024-02-03T20:50:25.855795Z","shell.execute_reply":"2024-02-03T20:50:25.854727Z","shell.execute_reply.started":"2024-02-03T20:50:25.851303Z"},"trusted":true},"outputs":[],"source":["def demove_digts(x):\n","    return re.sub('\\w*\\d\\w*','', x)"]},{"cell_type":"markdown","metadata":{},"source":["- Remove Punctuations"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T20:50:26.923562Z","iopub.status.busy":"2024-02-03T20:50:26.922492Z","iopub.status.idle":"2024-02-03T20:50:26.928554Z","shell.execute_reply":"2024-02-03T20:50:26.927543Z","shell.execute_reply.started":"2024-02-03T20:50:26.923526Z"},"trusted":true},"outputs":[],"source":["import string\n","\n","def demove_punctuations(x):\n","    out = re.sub('[%s]' % re.escape(string.punctuation), '', x)\n","    return re.sub(' +',' ',out) # Removing extra spaces"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T20:50:27.426289Z","iopub.status.busy":"2024-02-03T20:50:27.425377Z","iopub.status.idle":"2024-02-03T20:50:27.437419Z","shell.execute_reply":"2024-02-03T20:50:27.436292Z","shell.execute_reply.started":"2024-02-03T20:50:27.426231Z"},"trusted":true},"outputs":[],"source":["from collections import Counter\n","\n","class Fake_Dataset(Dataset):\n","    \"\"\"\n","    Generate the fake sentences as inputs of G2 (BERT Generator) \n","    \"\"\"\n","    def __init__(self, json_file,seq_length, tokenizer):\n","\n","        self.json_file = json_file\n","        self.seq_length = seq_length\n","        self.tokenizer = tokenizer\n","        self.weighted_keys = self.create_distribiution()\n","\n","    def __len__(self):\n","        return len(self.json_file)\n","    \n","    def text_cleaner(self,text):\n","         return demove_punctuations(demove_digts(expand_contractions(text)))\n","    \n","    \n","    def create_distribiution(self):\n","        token_counts = Counter([]) \n","        for sample in tqdm(self.json_file):\n","            text = self.text_cleaner(sample['text'])\n","            tokens_a = self.tokenizer.tokenize(text)\n","            input_ids = self.tokenizer.convert_tokens_to_ids(tokens_a)\n","            token_counts += Counter(input_ids) \n","\n","        token_counts_dict = dict(token_counts)\n","        weighted_keys = [key for key, count in token_counts_dict.items() for _ in range(count)]\n","        return weighted_keys\n","        \n","\n","    def sampler(self,seq_length):\n","        input_ids = random.choices(self.weighted_keys, k=seq_length)\n","        input_ids = torch.tensor(input_ids, dtype=torch.long)\n","        input_mask = torch.ones(input_ids.shape[0],dtype=torch.int32)\n","        return input_ids, input_mask \n","\n","    def __getitem__(self,idx):\n","        input_ids, input_mask = self.sampler(self.seq_length)\n","        return input_ids, input_mask #input_ids.squeeze(0)"]},{"cell_type":"markdown","metadata":{"id":"ah-BDYpjTDNR"},"source":["Now we can define **Discriminator** and **Generator** completely:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T20:50:28.461527Z","iopub.status.busy":"2024-02-03T20:50:28.460443Z","iopub.status.idle":"2024-02-03T20:50:28.466850Z","shell.execute_reply":"2024-02-03T20:50:28.465865Z","shell.execute_reply.started":"2024-02-03T20:50:28.461484Z"},"id":"b00Mx2wuRUmF","trusted":true},"outputs":[],"source":["# custom weights initialization\n","import torch.nn.init as init\n","\n","def custom_weights_init(m):\n","    if isinstance(m, nn.Linear):\n","        init.xavier_normal_(m.weight.data)\n","        if m.bias is not None:\n","            init.constant_(m.bias.data, 0)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T20:50:29.891446Z","iopub.status.busy":"2024-02-03T20:50:29.890848Z","iopub.status.idle":"2024-02-03T20:50:29.901148Z","shell.execute_reply":"2024-02-03T20:50:29.900090Z","shell.execute_reply.started":"2024-02-03T20:50:29.891416Z"},"id":"Ml-JM9SsTMM5","trusted":true},"outputs":[],"source":["class Discriminator(nn.Module):\n","    def __init__(self, input_size, num_classes,dropout_rate=0.2,relu_slop=0.2):\n","        super(Discriminator, self).__init__()\n","        self.input_size = input_size\n","        self.num_classes = num_classes\n","\n","        self.main = torch.nn.Sequential(\n","            Dropout(p=dropout_rate),\n","            \n","            Linear(in_features=self.input_size, out_features=512, bias=True),\n","            nn.LeakyReLU(relu_slop, inplace=True),\n","            Dropout(p=dropout_rate),\n","            \n","            Linear(in_features=512, out_features=256, bias=True),\n","            nn.LeakyReLU(relu_slop, inplace=True),\n","            Dropout(p=dropout_rate),\n","            \n","            Linear(in_features=256, out_features=256, bias=True),\n","            nn.LeakyReLU(relu_slop, inplace=True),\n","            Dropout(p=dropout_rate),\n","        )\n","\n","        self.logit = nn.Linear(256,self.num_classes+1)\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, input):\n","        last_rep = self.main(input)  # for do 'feature matching'\n","        logits = self.logit(last_rep)\n","        probs = self.softmax(logits)\n","        return last_rep, logits, probs\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T20:50:30.428568Z","iopub.status.busy":"2024-02-03T20:50:30.428173Z","iopub.status.idle":"2024-02-03T20:50:30.435524Z","shell.execute_reply":"2024-02-03T20:50:30.434599Z","shell.execute_reply.started":"2024-02-03T20:50:30.428540Z"},"id":"BYUu-z4NTCp5","trusted":true},"outputs":[],"source":["class Generator1(nn.Module):\n","    def __init__(self, input_size, output_size,dropout_rate=0.2,relu_slop=0.2):\n","        super(Generator1, self).__init__()\n","\n","        self.input_size = input_size\n","        self.output_size = output_size\n","\n","        self.main = torch.nn.Sequential(\n","            Linear(in_features=self.input_size, out_features=256, bias=True),\n","            nn.LeakyReLU(relu_slop, inplace=True),\n","            Dropout(p=dropout_rate, inplace=False),\n","            Linear(in_features=256, out_features=self.output_size, bias=True),\n","        )\n","\n","    def forward(self, input):\n","        return self.main(input)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T20:50:30.956269Z","iopub.status.busy":"2024-02-03T20:50:30.955631Z","iopub.status.idle":"2024-02-03T20:50:30.961889Z","shell.execute_reply":"2024-02-03T20:50:30.960863Z","shell.execute_reply.started":"2024-02-03T20:50:30.956227Z"},"trusted":true},"outputs":[],"source":["class Generator2(nn.Module):\n","    def __init__(self, bert_modele):\n","        super(Generator2, self).__init__()\n","        self.bert = bert_modele\n","\n","    def forward(self, encoded_ids,attention_mask):\n","        outputs = self.bert(encoded_ids,attention_mask)\n","        last_hidden_states = outputs.last_hidden_state[:,0]  # return embedding of 'CLS' token for classification.\n","\n","        return last_hidden_states"]},{"cell_type":"markdown","metadata":{"id":"TPbVNzM3vnWH"},"source":["Set Hyperparameter :"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T20:50:31.983992Z","iopub.status.busy":"2024-02-03T20:50:31.983372Z","iopub.status.idle":"2024-02-03T20:50:31.988478Z","shell.execute_reply":"2024-02-03T20:50:31.987540Z","shell.execute_reply.started":"2024-02-03T20:50:31.983961Z"},"id":"24sxiObdh5z7","trusted":true},"outputs":[],"source":["num_classes = 6\n","input_size = 768\n","noise_size = 100\n","label_list = list(range(6))"]},{"cell_type":"markdown","metadata":{"id":"YyyGFvUSW1ml"},"source":["### Define the Dataset class:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T20:50:34.382405Z","iopub.status.busy":"2024-02-03T20:50:34.381781Z","iopub.status.idle":"2024-02-03T20:50:34.392448Z","shell.execute_reply":"2024-02-03T20:50:34.391509Z","shell.execute_reply.started":"2024-02-03T20:50:34.382375Z"},"id":"hTsPHA58npxK","trusted":true},"outputs":[],"source":["\n","class SemEval_Dataset(Dataset):\n","    def __init__(self, json_file,label_list,label_masks,\n","                 max_seq_length, tokenizer,dtype=torch.long):\n","\n","        self.json_file = json_file\n","        self.dtype = dtype\n","        self.label_list = label_list # [0, 1, 2, 3, 4, 5]\n","        self.max_seq_length = max_seq_length\n","        self.tokenizer = tokenizer\n","        self.label_masks = label_masks\n","\n","    def __len__(self):\n","        return len(self.json_file)\n","\n","    def feature_extractor(self, text, label=None):\n","        features = []\n","        tokenized_text = tokenizer(text,padding='max_length', truncation=True,\n","                                   max_length=self.max_seq_length,\n","                                   return_tensors=\"pt\")\n","\n","        input_ids = tokenized_text['input_ids']\n","        input_mask = tokenized_text['attention_mask']\n","\n","        if len(input_ids) > self.max_seq_length:\n","            input_ids = input_ids[0:(self.max_seq_length)]   # crop long sentences\n","            input_mask = input_mask[0:(self.max_seq_length)]\n","\n","        assert len(input_ids[0]) == self.max_seq_length\n","        assert len(input_mask[0]) == self.max_seq_length\n","\n","        if label != None:\n","            return input_ids, input_mask, label\n","        else:\n","            return input_ids, input_mask\n","\n","    def __getitem__(self, idx):\n","        data = self.json_file[idx]\n","        input_ids, input_mask, label_id = self.feature_extractor(data['text'], label=data['label'])\n","\n","        return input_ids.squeeze(0), input_mask.squeeze(0), data['label'], self.label_masks[idx]"]},{"cell_type":"markdown","metadata":{"id":"5-6cooKmuy6d"},"source":["Create Dataset and Dataloader :"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T20:50:36.172359Z","iopub.status.busy":"2024-02-03T20:50:36.171970Z","iopub.status.idle":"2024-02-03T20:50:36.177036Z","shell.execute_reply":"2024-02-03T20:50:36.175848Z","shell.execute_reply.started":"2024-02-03T20:50:36.172331Z"},"id":"XbCXkjW2Ib9p","trusted":true},"outputs":[],"source":["max_seq_length = 256\n","batch_size = 64"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T20:50:39.383805Z","iopub.status.busy":"2024-02-03T20:50:39.382756Z","iopub.status.idle":"2024-02-03T20:50:39.410229Z","shell.execute_reply":"2024-02-03T20:50:39.409462Z","shell.execute_reply.started":"2024-02-03T20:50:39.383768Z"},"id":"vBsI2GALOlIS","trusted":true},"outputs":[],"source":["unlabeled_examples = True\n","labeled_ratio = 0.5               # 0.01, 0.1 ,0.05 ,0.5 \n","train_dataset_size_labeled = int(labeled_ratio* len(train_objects))\n","\n","#The labeled (train) dataset is assigned with a mask set to True\n","train_label_masks = torch.ones(train_dataset_size_labeled, dtype=bool)\n","#If unlabel examples are available\n","if unlabeled_examples:\n","  #The unlabeled (train) dataset is assigned with a mask set to False\n","    tmp_masks = torch.zeros(len(train_objects)- train_dataset_size_labeled , dtype=bool)\n","    train_label_masks = torch.concatenate([train_label_masks,tmp_masks])\n","    idx = torch.randperm(train_label_masks.shape[0])\n","    train_label_masks = train_label_masks[idx].view(train_label_masks.size())\n","\n","assert train_label_masks.shape[0] == len(train_objects)\n","train_dataset = SemEval_Dataset(train_objects, label_list, train_label_masks,max_seq_length, tokenizer)\n","# train_dataset = torch.utils.data.Subset(train_dataset, [i for i in range(train_dataset_size)])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T20:50:39.812387Z","iopub.status.busy":"2024-02-03T20:50:39.811666Z","iopub.status.idle":"2024-02-03T20:50:39.824223Z","shell.execute_reply":"2024-02-03T20:50:39.823445Z","shell.execute_reply.started":"2024-02-03T20:50:39.812356Z"},"id":"qQdi1pyGtCE6","trusted":true},"outputs":[],"source":["train_size = int(0.8 * len(train_objects))  # 80% for training\n","val_size = len(train_objects) - train_size  # Remaining 20% for validation\n","\n","train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n","test_label_masks = torch.ones(len(dev_objects), dtype=bool)\n","test_dataset = SemEval_Dataset(dev_objects, label_list, test_label_masks,max_seq_length, tokenizer)\n","\n","train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, num_workers=os.cpu_count(),shuffle=True, drop_last=False)\n","val_dataloader = DataLoader(dataset=val_dataset, batch_size=batch_size, num_workers=os.cpu_count(),shuffle=True, drop_last=False)\n","test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, num_workers=os.cpu_count(),shuffle=True, drop_last=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T20:50:40.290002Z","iopub.status.busy":"2024-02-03T20:50:40.289427Z","iopub.status.idle":"2024-02-03T20:50:40.294908Z","shell.execute_reply":"2024-02-03T20:50:40.294056Z","shell.execute_reply.started":"2024-02-03T20:50:40.289972Z"},"id":"DGwYFnbRPPsP","outputId":"3ef791d9-52a9-43ed-b3cd-13fa96d3bf6b","trusted":true},"outputs":[],"source":["print('Number of train samples: ', len(train_dataset))\n","print('Number of validation samples: ', len(val_dataset))\n","print('Number of test samples: ', len(test_dataset))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T20:50:40.679103Z","iopub.status.busy":"2024-02-03T20:50:40.678744Z","iopub.status.idle":"2024-02-03T20:50:42.900744Z","shell.execute_reply":"2024-02-03T20:50:42.899629Z","shell.execute_reply.started":"2024-02-03T20:50:40.679066Z"},"id":"UUXhfRCQxR2G","outputId":"90b8fef2-6212-461a-dd23-f21adbf11a0d","trusted":true},"outputs":[],"source":["# test train_dataset\n","betch = next(iter(train_dataloader))\n","print(f'input_ids shape: {betch[0].shape}, \\ninput_mask shape: {betch[1].shape}, \\\n","        \\nlabel_ids shape: {betch[2].shape},\\nlabel_mask shape: {betch[3].shape}')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T20:50:42.902986Z","iopub.status.busy":"2024-02-03T20:50:42.902678Z","iopub.status.idle":"2024-02-03T20:50:42.907648Z","shell.execute_reply":"2024-02-03T20:50:42.906682Z","shell.execute_reply.started":"2024-02-03T20:50:42.902955Z"},"trusted":true},"outputs":[],"source":["# for batch in train_dataloader:\n","# #     print(batch[2][batch[3]])\n","#     print()\n","#     print(batch[1])\n","#     break"]},{"cell_type":"markdown","metadata":{},"source":["Create a **Fake_Dataset** :"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T20:50:42.909148Z","iopub.status.busy":"2024-02-03T20:50:42.908833Z","iopub.status.idle":"2024-02-03T21:09:42.464264Z","shell.execute_reply":"2024-02-03T21:09:42.463210Z","shell.execute_reply.started":"2024-02-03T20:50:42.909118Z"},"trusted":true},"outputs":[],"source":["fake_dataset =  Fake_Dataset(train_objects,256, tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T21:09:47.088536Z","iopub.status.busy":"2024-02-03T21:09:47.087776Z","iopub.status.idle":"2024-02-03T21:09:47.093209Z","shell.execute_reply":"2024-02-03T21:09:47.092117Z","shell.execute_reply.started":"2024-02-03T21:09:47.088492Z"},"trusted":true},"outputs":[],"source":["noisy_dataloader = DataLoader(dataset=fake_dataset, batch_size=batch_size, num_workers=os.cpu_count(),shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T21:09:48.064894Z","iopub.status.busy":"2024-02-03T21:09:48.064526Z","iopub.status.idle":"2024-02-03T21:09:48.392948Z","shell.execute_reply":"2024-02-03T21:09:48.391771Z","shell.execute_reply.started":"2024-02-03T21:09:48.064863Z"},"trusted":true},"outputs":[],"source":["next(iter(noisy_dataloader))[0]"]},{"cell_type":"markdown","metadata":{"id":"AwpJPzUqbgzI"},"source":["## GAN-BERT"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T12:43:11.705761Z","iopub.status.busy":"2024-02-03T12:43:11.705000Z","iopub.status.idle":"2024-02-03T12:43:12.814783Z","shell.execute_reply":"2024-02-03T12:43:12.813274Z","shell.execute_reply.started":"2024-02-03T12:43:11.705722Z"},"id":"it3v2R_IdPwB","outputId":"99890cc6-bd00-4aae-d742-32817ee7b8c1","trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","img = plt.imread('/content/drive/My Drive/Project/GAN-BERT.png')\n","plt.imshow(img);\n","plt.axis('off');"]},{"cell_type":"markdown","metadata":{"id":"ywMFAgkjjQeG"},"source":["Hyperparameters:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T21:09:56.388596Z","iopub.status.busy":"2024-02-03T21:09:56.388221Z","iopub.status.idle":"2024-02-03T21:09:56.393388Z","shell.execute_reply":"2024-02-03T21:09:56.392372Z","shell.execute_reply.started":"2024-02-03T21:09:56.388563Z"},"id":"GaylbkGFOBX5","trusted":true},"outputs":[],"source":["epoch_num = 4\n","\n","learning_rate = 5e-4\n","noise_size = 100\n","epsilon = 1e-8\n","warmup_proportion = 0.1  #TODO"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T21:10:00.227573Z","iopub.status.busy":"2024-02-03T21:10:00.227149Z","iopub.status.idle":"2024-02-03T21:10:00.694469Z","shell.execute_reply":"2024-02-03T21:10:00.693457Z","shell.execute_reply.started":"2024-02-03T21:10:00.227543Z"},"id":"-COnKU7NhU2x","outputId":"28480ff1-cc2c-405c-9903-2182abe3723f","trusted":true},"outputs":[],"source":["# Create the Discriminator and Generator\n","discriminator = Discriminator(input_size,num_classes).to(device)\n","generator2 = Generator2(bert_generator).to(device)\n","bert = BERT_Embedder(bert_model).to(device)\n","\n","\n","# Handle multi-GPU if desired\n","if (device.type == 'cuda') and (ngpu > 1):\n","    discriminator = nn.DataParallel(discriminator, list(range(ngpu)))\n","    generator = nn.DataParallel(generator2, list(range(ngpu)))    \n","    bert = nn.DataParallel(bert, list(range(ngpu)))\n","\n","# weights initialization   # TODO : Xavier weight initialization\n","discriminator.apply(custom_weights_init)\n","# generator1.apply(custom_weights_init)\n","\n","# print(discriminator)\n","# print()\n","# print(generator1)\n","# print()\n","# print(bert)"]},{"cell_type":"markdown","metadata":{"id":"ja2TR4WPi3fY"},"source":["### Define the Optimizers and Scheduler:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T21:10:02.628944Z","iopub.status.busy":"2024-02-03T21:10:02.628568Z","iopub.status.idle":"2024-02-03T21:10:02.637845Z","shell.execute_reply":"2024-02-03T21:10:02.636729Z","shell.execute_reply.started":"2024-02-03T21:10:02.628912Z"},"id":"2zuKJdvdgjoO","trusted":true},"outputs":[],"source":["gen_optimizer = torch.optim.AdamW(generator2.parameters(), lr=learning_rate)\n","dis_optimizer = torch.optim.AdamW(list(bert.parameters()) + list(discriminator.parameters()), lr=learning_rate)\n","\n","#scheduler\n","num_train_examples = len(train_dataset)\n","num_train_steps = int(num_train_examples / batch_size * epoch_num) \n","num_warmup_steps = int(num_train_steps * warmup_proportion)\n","\n","scheduler_d = get_constant_schedule_with_warmup(dis_optimizer, \n","                                       num_warmup_steps = num_warmup_steps)\n","scheduler_g = get_constant_schedule_with_warmup(gen_optimizer, \n","                                       num_warmup_steps = num_warmup_steps) "]},{"cell_type":"markdown","metadata":{},"source":[" The loss function of $Discriminator$ is defined as:  $$\\quad L_{\\mathcal{D}}=L_{\\mathcal{D}_{\\text {sup. }}}+L_{\\mathcal{D}_{\\text {unsup. }}}$$\n"," where:\n","$$\n","\\begin{aligned}\n","L_{\\mathcal{D}_{\\text {sup. }}} & =-\\mathbb{E}_{x, y \\sim p_d} \\log \\left[p_{\\mathrm{m}}(\\hat{y}=y \\mid x, y \\in(1, \\ldots, k))\\right] \\\\\n","L_{\\mathcal{D}_{\\text {unsup. }}} & =-\\mathbb{E}_{x \\sim p_d} \\log \\left[1-p_{\\mathrm{m}}(\\hat{y}=y \\mid x, y=k+1)\\right] -\\mathbb{E}_{x \\sim \\mathcal{G}} \\log \\left[p_{\\mathrm{m}}(\\hat{y}=y \\mid x, y=k+1)\\right] \\\\\n","\\rightarrow  L_{\\mathcal{D}_{\\text {unsup. }}} & =-\\mathbb{E}_{x \\sim p_d} [\\log (\\mathcal{D}(x))] -\\mathbb{E}_{x \\sim \\mathcal{G}} \n","[\\log (1-\\mathcal{D}(x))]\n","\\end{aligned}\n","$$"]},{"cell_type":"markdown","metadata":{},"source":["And loss function of $Generator$ is defined as: $$\\quad L_{\\mathcal{G}}=L_{\\mathcal{G}_{\\text {feature matching }}}+L_{\\mathcal{G}_{\\text {unsup. }}}$$ \n","where:\n","\n","$$L_{\\mathcal{G}_{\\text {unsup. }}}=-\\mathbb{E}_{x \\sim \\mathcal{G}} \n","\\log \\left[1-p_m(\\hat{y}=y \\mid x, y=k+1)\\right]$$\n","\n","$$ L_{\\mathcal{G}_{\\text {feature matching }}} = ||\\mathbb{E}_{x \\sim p_d} f(x) \n","- \\mathbb{E}_{x \\sim \\mathcal{G}} f(x) ||_2^2$$\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T21:10:04.238189Z","iopub.status.busy":"2024-02-03T21:10:04.237818Z","iopub.status.idle":"2024-02-03T21:10:04.387230Z","shell.execute_reply":"2024-02-03T21:10:04.386369Z","shell.execute_reply.started":"2024-02-03T21:10:04.238157Z"},"trusted":true},"outputs":[],"source":["class GANBERT():\n","    def __init__(self, discriminator, generator, bert,gen_optimizer, dis_optimizer,\n","                scheduler_d,scheduler_g, path,G2=False): \n","\n","        self.discriminator = discriminator\n","        self.generator = generator\n","        self.bert = bert\n","        self.gen_optimizer = gen_optimizer\n","        self.dis_optimizer = dis_optimizer\n","        self.scheduler_g = scheduler_g\n","        self.scheduler_d = scheduler_d\n","        self.nll_loss = torch.nn.CrossEntropyLoss(ignore_index=-1 , label_smoothing=0.005) # which one\n","        self.path = path\n","        self.G2 = G2\n"," \n","    def trainer(self, epoch_num, label_list,labeled_ratio,\n","               train_dataloader, val_dataloader=None, noisy_dataloader=None\n","                ,report=True):\n","        \n","        best_score = 1e-5\n","    \n","        def format_time(elapsed):\n","            '''\n","            Takes a time in seconds and returns a string hh:mm:ss\n","            '''\n","            # Round to the nearest second.\n","            elapsed_rounded = int(round((elapsed)))\n","            # Format as hh:mm:ss\n","            return str(datetime.timedelta(seconds=elapsed_rounded))\n","        \n","        results = []\n","        print(f'With labeled_ratio : {labeled_ratio}\\n')\n","        for epoch in range(epoch_num):\n","            # Measure how long the each epoch takes.\n","            t0 = time.time()\n","            \n","            self.bert.train()\n","            self.generator.train()\n","            self.discriminator.train()\n","\n","            tr_g_loss = 0\n","            tr_d_loss = 0\n","            nb_tr_examples, nb_tr_steps = 0, 0\n","\n","            print(f'Epoch {epoch+1}/{epoch_num} :')\n","            for step, batch in enumerate(train_dataloader):\n","\n","                src_input_ids, src_input_mask, label_ids, b_label_mask = batch # unpacking\n","                src_input_ids = src_input_ids.to(device)\n","                src_input_mask = src_input_mask.to(device)\n","                label_ids = label_ids.to(device)\n","                b_label_mask = b_label_mask.to(device)\n","\n","                self.bert.zero_grad()\n","                self.discriminator.zero_grad()\n","\n","                # Real representations\n","                embedding = self.bert(src_input_ids, attention_mask=src_input_mask)\n","                D_real_features, D_real_logits, D_real_probs = self.discriminator(embedding)\n","\n","                # Random noise\n","                if self.G2:\n","                    noisy_input_ids, noisy_input_mask = next(iter(noisy_dataloader))\n","                    noisy_input_ids = noisy_input_ids.to(device)\n","                    noisy_input_mask = noisy_input_mask.to(device)\n","                    gen_rep = self.generator(noisy_input_ids, attention_mask=noisy_input_mask)\n","                \n","                else:\n","                    noise = torch.zeros(src_input_ids.shape[0],noise_size, device=device).uniform_(0, 1)\n","                    gen_rep = self.generator(noise)\n","                \n","                \n","                ############################\n","                # Update Generator network: minimize -E[log(D(G(z)))] + feature_matching LOSS\n","                ###########################\n","                D_fake_features, D_fake_logits, D_fake_probs = self.discriminator(gen_rep) # .detach()\n","\n","                g_loss_d = -1 * torch.mean(torch.log(1 - D_fake_probs[:,-1] + epsilon))\n","                g_feat_reg = torch.mean(torch.pow(torch.mean(D_real_features, dim=0) - torch.mean(D_fake_features, dim=0), 2))\n","                g_loss = g_loss_d + g_feat_reg\n","\n","                ############################\n","                #  Update Discriminator network: minimize -E[log(D(x)) + log(1 - D(G(z)))]\n","                ###########################\n","                logits = D_real_logits[:,0:-1]\n","                log_probs = F.log_softmax(logits, dim=-1)\n","                # The discriminator provides an output for labeled and unlabeled real data\n","                # so the loss evaluated for unlabeled data is ignored (masked)\n","                label2one_hot = torch.nn.functional.one_hot(label_ids, len(label_list))\n","                per_example_loss = -torch.sum(label2one_hot * log_probs, dim=-1)\n","                per_example_loss = torch.masked_select(per_example_loss, b_label_mask.to(device))\n","                labeled_example_count = per_example_loss.type(torch.float32).numel()\n","\n","                # It may be the case that a batch does not contain labeled examples,\n","                # so the \"supervised loss\" in this case is not evaluated\n","                if labeled_example_count == 0:\n","                    D_L_Supervised = 0\n","                else:\n","                    D_L_Supervised = torch.div(torch.sum(per_example_loss.to(device)), labeled_example_count)\n","\n","                D_L_unsupervised1U = -1 * torch.mean(torch.log(1 - D_real_probs[:, -1] + epsilon))\n","                D_L_unsupervised2U = -1 * torch.mean(torch.log(D_fake_probs[:, -1] + epsilon))\n","                d_loss = D_L_Supervised + D_L_unsupervised1U + D_L_unsupervised2U\n","\n","                #---------------------------------\n","                #  OPTIMIZATION\n","                #---------------------------------\n","                self.gen_optimizer.zero_grad()\n","                self.dis_optimizer.zero_grad()\n","\n","                # Calculate weigth updates\n","                # retain_graph=True is required since the underlying graph will be deleted after backward\n","                g_loss.backward(retain_graph=True)\n","                d_loss.backward() \n","\n","                # Apply modifications\n","                self.gen_optimizer.step()\n","                self.dis_optimizer.step()\n","\n","                # Save the losses to print them later\n","                tr_g_loss += g_loss.item()\n","                tr_d_loss += d_loss.item()\n","\n","            # Output training stats\n","                if report:\n","                    if step % 100 == 0:\n","                        print('''\\n[Epoch %d/%d][iter %d/%d]\\ttotal Loss_D: %.4f\\ttotal Loss_G: %.4f,\\n\n","                        details of Loss_D:  Loss_D_sup: %.4f,\\t-E[log(D(x))]: %.4f,\\t-E[log(1-D(G(z)))]: %.4f,\\n\n","                        details of Loss_G:  -E[log(D(G(z)))]: %.4f,\\tLoss_G_feat: %.4f\\n\n","                        D(x): %.4f\\tD(G(z)): %.4f'''\n","                          %(epoch+1, epoch_num, step, len(train_dataloader),\n","                            d_loss.mean().item(), g_loss.mean().item(), \n","                            D_L_Supervised, D_L_unsupervised1U, D_L_unsupervised2U,\n","                            g_loss_d, g_feat_reg,\n","                            torch.mean(D_real_probs[:, -1]).item(), \n","                              torch.mean(D_fake_probs[:, -1]).item() ))\n","                        \n","                        # save checkpoints\n","                        self.save_checkpoint(epoch)\n","\n","            # Update the learning rate with the scheduler\n","            self.scheduler_d.step()\n","            self.scheduler_g.step()\n","\n","            # Calculate the average loss over all of the batches.\n","            avg_train_loss_g = tr_g_loss / len(train_dataloader)\n","            avg_train_loss_d = tr_d_loss / len(train_dataloader)\n","\n","            # Measure how long this epoch took.\n","            epoch_time = format_time(time.time() - t0)\n","\n","            print(\"\")\n","            print(f' Training stats at epoch {epoch+1}: ')\n","            print(f' G_loss = {tr_g_loss}, D_loss = {tr_d_loss} \\n')\n","            print(\" Training epcoh took: {:}\".format(epoch_time))\n","            \n","            if val_dataloader != None:\n","                self.bert.eval()\n","                self.discriminator.eval() \n","\n","                all_preds = np.array([])\n","                all_label_ids = np.array([])\n","                eval_loss = 0\n","                nb_eval_steps = 0\n","                for val_step, batch in enumerate(val_dataloader):\n","                    src_input_ids, src_input_mask, label_ids, _ = batch # unpacking\n","                    src_input_ids = src_input_ids.to(device)\n","                    src_input_mask = src_input_mask.to(device)\n","                    label_ids = label_ids.to(device)\n","\n","\n","                    with torch.no_grad():\n","                        doc_rep = self.bert(src_input_ids, attention_mask=src_input_mask)\n","                        _, logits, _ = self.discriminator(doc_rep)\n","#                         probs = torch.nn.functional.softmax(logits[:,0:-1], dim=-1)\n","                        probs = logits[:,0:-1]\n","                        tmp_eval_loss = self.nll_loss(probs, label_ids.view(-1))\n","\n","                    eval_loss += tmp_eval_loss.mean().item()\n","\n","                    probs = probs.detach().cpu().numpy()\n","                    label_ids = label_ids.to('cpu').numpy()\n","                    all_preds = np.append(all_preds, np.argmax(probs, axis=1))\n","                    all_label_ids = np.append(all_label_ids, label_ids)\n","\n","                    nb_eval_steps += 1\n","\n","                eval_loss = eval_loss / nb_eval_steps\n","#                 precision, recall, f1, _ = precision_recall_fscore_support(all_label_ids, all_preds, average=\"micro\",\n","#                                                                          labels=list(range(0,len(label_list))))\n","                mcc = matthews_corrcoef(all_preds, all_label_ids)\n","                acc = (all_preds == all_label_ids).sum().item() / all_label_ids.shape[0]\n","\n","\n","                # Output validation stats\n","                print(f'Validation stats: ')\n","                print('Loss: %.4f,\\tAccuracy: %.4f,\\tmcc: %.4f,'\n","                  %(eval_loss,acc,mcc))\n","                \n","            result = {\n","                'epoch': epoch_time,\n","                \"gen_loss\": tr_g_loss,\n","                \"dis_loss\": tr_d_loss,\n","                \"eval_loss\": eval_loss,\n","                \"mcc\": mcc,\n","                \"acc\": acc,\n","                'epoch_time': epoch_time}\n","#                 \"precision_micro\": precision,\n","#                 \"recall_micro\": recall,\n","#                 \"f1_micro\": f1,\n","                \n","\n","            results.append(result)\n","            # save checkpoints\n","            self.save_checkpoint(epoch,results)\n","            \n","            # seva best model\n","            if acc > best_score:\n","                best_score = acc \n","                self.save_checkpoint(epoch ,result,best=True)\n","            \n","    def save_checkpoint(self,epoch,results=None,best=False):\n","        checkpoint = {\n","            'epoch': epoch + 1,\n","            'bert_state_dict': self.bert.state_dict(),\n","            'disc_state_dict': self.discriminator.state_dict(),\n","            'gen_state_dict': self.generator.state_dict(),\n","            'disc_optimizer_state_dict': self.dis_optimizer.state_dict(),\n","            'gen_optimizer_state_dict': self.gen_optimizer.state_dict(),\n","            }\n","        # for colab : /content/drive/My Drive/Project/checkpoints\n","        if best:\n","            torch.save(checkpoint, f'{self.path}/GAN_BERT_checkpoint_BEST.pth')\n","            if results!= None:\n","                with open(f'{self.path}/results_BEST.pickle', 'wb') as file:\n","                    pickle.dump(results, file)\n","            \n","        else:\n","            torch.save(checkpoint, f'{self.path}/GAN_BERT_checkpoint{epoch+1}.pth')\n","            if results!= None:\n","                with open(f'{self.path}/results.pickle', 'wb') as file:\n","                    pickle.dump(results, file)\n","    \n","    def test(self, test_dataloader):\n","        self.bert.eval()\n","        self.discriminator.eval()\n","\n","        all_preds = np.array([])\n","        all_label_ids = np.array([])\n","        eval_loss = 0\n","        nb_eval_steps = 0\n","        nll_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\n","        \n","        for val_step, batch in enumerate(test_dataloader):\n","            src_input_ids, src_input_mask, label_ids, _ = batch # unpacking\n","            src_input_ids = src_input_ids.to(device)\n","            src_input_mask = src_input_mask.to(device)\n","            label_ids = label_ids.to(device)\n","\n","\n","            with torch.no_grad():\n","                doc_rep = self.bert(src_input_ids, attention_mask=src_input_mask)\n","                _, logits, _ = self.discriminator(doc_rep)\n","            # probs = torch.nn.functional.softmax(logits[:,0:-1], dim=-1)\n","            probs = logits[:,0:-1]    \n","            tmp_eval_loss = nll_loss(probs, label_ids.view(-1))\n","\n","            eval_loss += tmp_eval_loss.mean().item()\n","\n","            probs = probs.detach().cpu().numpy()\n","            label_ids = label_ids.to('cpu').numpy()\n","            \n","            all_preds = np.append(all_preds, np.argmax(probs, axis=1))\n","            all_label_ids = np.append(all_label_ids, label_ids)\n","\n","            nb_eval_steps += 1\n","\n","        eval_loss = eval_loss / nb_eval_steps\n","\n","\n","        mcc = matthews_corrcoef(all_preds, all_label_ids)\n","        acc = (all_preds == all_label_ids).sum().item() / all_label_ids.shape[0]\n","        \n","        # Output validation stats\n","        print(f'Test stats: ')\n","        print('Total loss: %.4f,\\tAccuracy: %.4f,\\tmcc: %.4f,'\n","          %(eval_loss,acc,mcc) ) \n","        return all_preds\n","\n","    @staticmethod\n","    def rename_keys(original_ordered_dict):\n","        new_keys_mapping = dict()\n","        for a in list(original_ordered_dict.keys()):\n","            new_keys_mapping[a] = a.split('module.')[-1] \n","\n","        return OrderedDict((new_keys_mapping.get(k, k), v) for k, v in original_ordered_dict.items())\n","\n","    \n","    def load_checkpoint(self,checkpoint_path):\n","        state_dict = torch.load(checkpoint_path)\n","        \n","        if (device.type == 'cuda') and (ngpu > 1):\n","            # Load the state dictionary into the model\n","            self.bert.load_state_dict(state_dict['bert_state_dict'])\n","            self.discriminator.load_state_dict(state_dict['disc_state_dict'])\n","            self.generator.load_state_dict(state_dict['gen_state_dict'])\n","            self.dis_optimizer.load_state_dict(state_dict['disc_optimizer_state_dict'])\n","            self.gen_optimizer.load_state_dict(state_dict['gen_optimizer_state_dict'])\n","            \n","        else: \n","            self.bert.load_state_dict(self.rename_keys(state_dict['bert_state_dict']))\n","            self.discriminator.load_state_dict(self.rename_keys(state_dict['disc_state_dict']))\n","            self.generator.load_state_dict(self.rename_keys(state_dict['gen_state_dict']))\n","            self.dis_optimizer.load_state_dict(state_dict['disc_optimizer_state_dict'])\n","            self.gen_optimizer.load_state_dict(state_dict['gen_optimizer_state_dict'])\n","\n","        print('Loaded !')\n","            \n","    def plot_results():\n","        pass\n","            \n","    def show_tensorboard():\n","        pass"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T21:10:04.601086Z","iopub.status.busy":"2024-02-03T21:10:04.600772Z","iopub.status.idle":"2024-02-03T21:10:04.605087Z","shell.execute_reply":"2024-02-03T21:10:04.604019Z","shell.execute_reply.started":"2024-02-03T21:10:04.601060Z"},"trusted":true},"outputs":[],"source":["# !pip install numba\n","\n","# from numba import cuda\n","# device = cuda.get_current_device()\n","# device.reset()\n","# torch.cuda.empty_cache()  "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T21:10:05.049063Z","iopub.status.busy":"2024-02-03T21:10:05.048269Z","iopub.status.idle":"2024-02-03T21:10:07.036186Z","shell.execute_reply":"2024-02-03T21:10:07.035220Z","shell.execute_reply.started":"2024-02-03T21:10:05.049030Z"},"trusted":true},"outputs":[],"source":["# !ls\n","!mkdir part3\n","!ls"]},{"cell_type":"markdown","metadata":{},"source":["----\n","## With **G1** :"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-02T22:40:33.572567Z","iopub.status.busy":"2024-02-02T22:40:33.572246Z","iopub.status.idle":"2024-02-02T22:40:33.623467Z","shell.execute_reply":"2024-02-02T22:40:33.622169Z","shell.execute_reply.started":"2024-02-02T22:40:33.572534Z"},"trusted":true},"outputs":[],"source":["ganbert = GANBERT(discriminator, generator1, bert,gen_optimizer, dis_optimizer,\n","                scheduler_d,scheduler_g, path='/kaggle/working/part3') "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-02T10:40:59.968905Z","iopub.status.busy":"2024-02-02T10:40:59.968052Z","iopub.status.idle":"2024-02-02T13:14:34.061148Z","shell.execute_reply":"2024-02-02T13:14:34.060119Z","shell.execute_reply.started":"2024-02-02T10:40:59.968877Z"},"trusted":true},"outputs":[],"source":["ganbert.trainer(epoch_num,label_list,labeled_ratio,train_dataloader, val_dataloader,report=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T16:17:00.492497Z","iopub.status.busy":"2024-02-03T16:17:00.492129Z","iopub.status.idle":"2024-02-03T16:17:00.526838Z","shell.execute_reply":"2024-02-03T16:17:00.525609Z","shell.execute_reply.started":"2024-02-03T16:17:00.492465Z"},"trusted":true},"outputs":[],"source":["test_res = ganbert.test(test_dataloader) "]},{"cell_type":"markdown","metadata":{"id":"x1zcuLgibO3X"},"source":["The `Matthews correlation coefficient` , is a measure of the quality of classifications in machine learning. It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. It's defined in the range from -1 to 1, with 1 being a perfect prediction, 0 being the result of a random prediction, and -1 indicating total disagreement between prediction and observation."]},{"cell_type":"markdown","metadata":{"id":"Hm86KFjT1d9I"},"source":["---\n","### Load the best model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-02T14:20:37.212522Z","iopub.status.busy":"2024-02-02T14:20:37.212060Z","iopub.status.idle":"2024-02-02T14:20:37.236952Z","shell.execute_reply":"2024-02-02T14:20:37.236003Z","shell.execute_reply.started":"2024-02-02T14:20:37.212492Z"},"trusted":true},"outputs":[],"source":["discriminator = Discriminator(input_size,num_classes).to(device)\n","generator1 = Generator1(noise_size,input_size).to(device)\n","bert = BERT_Embedder(bert_model).to(device)\n","\n","if (device.type == 'cuda') and (ngpu > 1):\n","    discriminator = nn.DataParallel(discriminator, list(range(ngpu)))\n","    generator = nn.DataParallel(generator1, list(range(ngpu)))    \n","    bert = nn.DataParallel(bert, list(range(ngpu)))\n","    \n","gen_optimizer = torch.optim.AdamW(generator1.parameters(), lr=learning_rate)\n","dis_optimizer = torch.optim.AdamW(list(bert.parameters()) + list(discriminator.parameters()), lr=learning_rate)\n","\n","#scheduler\n","num_train_examples = len(train_dataset)\n","num_train_steps = int(num_train_examples / batch_size * epoch_num) \n","num_warmup_steps = int(num_train_steps * warmup_proportion)\n","\n","scheduler_d = get_constant_schedule_with_warmup(dis_optimizer, \n","                                       num_warmup_steps = num_warmup_steps)\n","scheduler_g = get_constant_schedule_with_warmup(gen_optimizer, \n","                                       num_warmup_steps = num_warmup_steps) \n","\n","ganbert_best = GANBERT(discriminator, generator1, bert,gen_optimizer, dis_optimizer,\n","                scheduler_d,scheduler_g, path='/kaggle/working/part3') "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T00:48:48.958348Z","iopub.status.busy":"2024-02-04T00:48:48.957949Z","iopub.status.idle":"2024-02-04T00:48:49.393339Z","shell.execute_reply":"2024-02-04T00:48:49.392140Z","shell.execute_reply.started":"2024-02-04T00:48:48.958311Z"},"trusted":true},"outputs":[],"source":["path = '/kaggle/working/part3/GAN_BERT_checkpoint_BEST.pth'\n","ganbert_best.load_checkpoint(path)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-02T14:21:11.705667Z","iopub.status.busy":"2024-02-02T14:21:11.704759Z","iopub.status.idle":"2024-02-02T14:21:28.205914Z","shell.execute_reply":"2024-02-02T14:21:28.204866Z","shell.execute_reply.started":"2024-02-02T14:21:11.705635Z"},"trusted":true},"outputs":[],"source":["test_res = ganbert_best.test(test_dataloader) "]},{"cell_type":"markdown","metadata":{},"source":["----\n","## With **G2** :"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T21:10:22.401770Z","iopub.status.busy":"2024-02-03T21:10:22.401352Z"},"trusted":true},"outputs":[],"source":["ganbert2 = GANBERT(discriminator, generator2, bert,gen_optimizer, dis_optimizer,\n","                scheduler_d,scheduler_g, path='/kaggle/working/part3', G2=True) \n","\n","\n","ganbert2.trainer(epoch_num,label_list,labeled_ratio,train_dataloader, val_dataloader,noisy_dataloader,report=True)\n","# test_res = ganbert.test(test_dataloader) "]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T00:49:08.751984Z","iopub.status.busy":"2024-02-04T00:49:08.751618Z","iopub.status.idle":"2024-02-04T00:49:09.983629Z","shell.execute_reply":"2024-02-04T00:49:09.982589Z","shell.execute_reply.started":"2024-02-04T00:49:08.751953Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded !\n"]}],"source":["ganbert2.load_checkpoint('/kaggle/working/part3/GAN_BERT_checkpoint_BEST.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T00:52:48.105880Z","iopub.status.busy":"2024-02-04T00:52:48.105471Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["With labeled_ratio : 0.5\n","\n","Epoch 1/3 :\n","\n","[Epoch 1/3][iter 0/888]\ttotal Loss_D: 1.3547\ttotal Loss_G: 0.7173,\n","\n","                        details of Loss_D:  Loss_D_sup: 0.6033,\t-E[log(D(x))]: 0.0178,\t-E[log(1-D(G(z)))]: 0.7336,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.6742,\tLoss_G_feat: 0.0432\n","\n","                        D(x): 0.0175\tD(G(z)): 0.4857\n","\n","[Epoch 1/3][iter 100/888]\ttotal Loss_D: 1.7101\ttotal Loss_G: 0.7100,\n","\n","                        details of Loss_D:  Loss_D_sup: 0.9509,\t-E[log(D(x))]: 0.0151,\t-E[log(1-D(G(z)))]: 0.7441,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.6662,\tLoss_G_feat: 0.0438\n","\n","                        D(x): 0.0150\tD(G(z)): 0.4811\n","\n","[Epoch 1/3][iter 200/888]\ttotal Loss_D: 1.2740\ttotal Loss_G: 0.7294,\n","\n","                        details of Loss_D:  Loss_D_sup: 0.5329,\t-E[log(D(x))]: 0.0169,\t-E[log(1-D(G(z)))]: 0.7242,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.6867,\tLoss_G_feat: 0.0427\n","\n","                        D(x): 0.0166\tD(G(z)): 0.4907\n","\n","[Epoch 1/3][iter 300/888]\ttotal Loss_D: 1.5432\ttotal Loss_G: 0.7073,\n","\n","                        details of Loss_D:  Loss_D_sup: 0.7863,\t-E[log(D(x))]: 0.0186,\t-E[log(1-D(G(z)))]: 0.7382,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.6723,\tLoss_G_feat: 0.0350\n","\n","                        D(x): 0.0183\tD(G(z)): 0.4840\n","\n","[Epoch 1/3][iter 400/888]\ttotal Loss_D: 1.1878\ttotal Loss_G: 0.7077,\n","\n","                        details of Loss_D:  Loss_D_sup: 0.4342,\t-E[log(D(x))]: 0.0149,\t-E[log(1-D(G(z)))]: 0.7387,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.6703,\tLoss_G_feat: 0.0374\n","\n","                        D(x): 0.0147\tD(G(z)): 0.4832\n","\n","[Epoch 1/3][iter 500/888]\ttotal Loss_D: 1.4896\ttotal Loss_G: 0.7353,\n","\n","                        details of Loss_D:  Loss_D_sup: 0.7595,\t-E[log(D(x))]: 0.0157,\t-E[log(1-D(G(z)))]: 0.7145,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.7007,\tLoss_G_feat: 0.0346\n","\n","                        D(x): 0.0154\tD(G(z)): 0.4966\n","\n","[Epoch 1/3][iter 600/888]\ttotal Loss_D: 1.3821\ttotal Loss_G: 0.7241,\n","\n","                        details of Loss_D:  Loss_D_sup: 0.6441,\t-E[log(D(x))]: 0.0157,\t-E[log(1-D(G(z)))]: 0.7223,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.6884,\tLoss_G_feat: 0.0357\n","\n","                        D(x): 0.0154\tD(G(z)): 0.4920\n","\n","[Epoch 1/3][iter 700/888]\ttotal Loss_D: 1.4245\ttotal Loss_G: 0.7037,\n","\n","                        details of Loss_D:  Loss_D_sup: 0.6693,\t-E[log(D(x))]: 0.0116,\t-E[log(1-D(G(z)))]: 0.7436,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.6655,\tLoss_G_feat: 0.0382\n","\n","                        D(x): 0.0114\tD(G(z)): 0.4809\n","\n","[Epoch 1/3][iter 800/888]\ttotal Loss_D: 0.9341\ttotal Loss_G: 0.7514,\n","\n","                        details of Loss_D:  Loss_D_sup: 0.2241,\t-E[log(D(x))]: 0.0102,\t-E[log(1-D(G(z)))]: 0.6997,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.7143,\tLoss_G_feat: 0.0371\n","\n","                        D(x): 0.0101\tD(G(z)): 0.5036\n","\n"," Training stats at epoch 1: \n"," G_loss = 637.215004503727, D_loss = 1188.1344618797302 \n","\n"," Training epcoh took: 0:53:03\n","Validation stats: \n","Loss: 0.7000,\tAccuracy: 0.7521,\tmcc: 0.7092,\n","Epoch 2/3 :\n","\n","[Epoch 2/3][iter 0/888]\ttotal Loss_D: 1.1728\ttotal Loss_G: 0.7169,\n","\n","                        details of Loss_D:  Loss_D_sup: 0.4249,\t-E[log(D(x))]: 0.0147,\t-E[log(1-D(G(z)))]: 0.7333,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.6799,\tLoss_G_feat: 0.0370\n","\n","                        D(x): 0.0144\tD(G(z)): 0.4869\n","\n","[Epoch 2/3][iter 100/888]\ttotal Loss_D: 1.2320\ttotal Loss_G: 0.6761,\n","\n","                        details of Loss_D:  Loss_D_sup: 0.4507,\t-E[log(D(x))]: 0.0120,\t-E[log(1-D(G(z)))]: 0.7693,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.6401,\tLoss_G_feat: 0.0360\n","\n","                        D(x): 0.0118\tD(G(z)): 0.4686\n","\n","[Epoch 2/3][iter 200/888]\ttotal Loss_D: 1.3484\ttotal Loss_G: 0.7134,\n","\n","                        details of Loss_D:  Loss_D_sup: 0.6097,\t-E[log(D(x))]: 0.0099,\t-E[log(1-D(G(z)))]: 0.7288,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.6780,\tLoss_G_feat: 0.0354\n","\n","                        D(x): 0.0098\tD(G(z)): 0.4873\n","\n","[Epoch 2/3][iter 300/888]\ttotal Loss_D: 1.2498\ttotal Loss_G: 0.7414,\n","\n","                        details of Loss_D:  Loss_D_sup: 0.5294,\t-E[log(D(x))]: 0.0158,\t-E[log(1-D(G(z)))]: 0.7047,\n","\n","                        details of Loss_G:  -E[log(D(G(z)))]: 0.7050,\tLoss_G_feat: 0.0364\n","\n","                        D(x): 0.0154\tD(G(z)): 0.5000\n"]}],"source":["ganbert2.trainer(3,label_list,labeled_ratio,train_dataloader, val_dataloader,noisy_dataloader,report=True)"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2024-02-04T00:49:13.595206Z","iopub.status.busy":"2024-02-04T00:49:13.594257Z","iopub.status.idle":"2024-02-04T00:49:29.335662Z","shell.execute_reply":"2024-02-04T00:49:29.334418Z","shell.execute_reply.started":"2024-02-04T00:49:13.595158Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Test stats: \n","Total loss: 1.4534,\tAccuracy: 0.5413,\tmcc: 0.4610,\n"]}],"source":["test_res = ganbert2.test(test_dataloader) "]},{"cell_type":"markdown","metadata":{},"source":["---"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-02-02T14:17:43.589081Z","iopub.status.idle":"2024-02-02T14:17:43.589549Z","shell.execute_reply":"2024-02-02T14:17:43.589349Z","shell.execute_reply.started":"2024-02-02T14:17:43.589329Z"},"id":"aDZl0qCwSDUH","trusted":true},"outputs":[],"source":["plt.figure(figsize=(10,5))\n","plt.title(\"Generator and Discriminator Loss During Training\")\n","plt.plot(G_losses,label=\"G\")\n","plt.plot(D_losses,label=\"D\")\n","plt.xlabel(\"iterations\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30648,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}

{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Deep Learning Project - Part3\n<div style=\"text-align: center\">\n<h1 style = \"color: red\"> Sharif University Of Technology</h1>\n<h2 style = \"color: green\"> DR. Fatemizadeh </h2>\n<h3 style = \"color: cyan\"> Authors: Amirreza Velaee - Hessam Hosseini - Amirabbas Afzali - Mahshad Moradi<h3>\n</div>","metadata":{"id":"vRSJfpfXQkzn"}},{"cell_type":"code","source":"import argparse \nimport os\nimport random\nimport torch\nimport torch.nn as nn \nimport torch.nn.parallel\nimport torch.optim as optim\nimport torch.utils.data\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\nimport json\nfrom tqdm import tqdm, trange\nfrom sklearn.metrics import precision_recall_fscore_support, matthews_corrcoef\nimport pickle\nfrom torch.utils.data import random_split\nfrom torch.utils.data import Dataset, Subset\nfrom torch.utils.data import DataLoader,Dataset\nfrom torch.nn.modules import ReLU,Linear,Dropout\nimport time\nimport math\nimport datetime\nimport torch.nn.functional as F\nfrom collections import OrderedDict\nfrom collections import Counter\n\n# Set random seed for reproducibility\nmanualSeed = 42\nprint(\"Random Seed: \", manualSeed)\nrandom.seed(manualSeed)\ntorch.manual_seed(manualSeed)\n\nngpu = 1","metadata":{"id":"oeTnUDU-Op7F","outputId":"8c5f18ba-0507-4811-8895-77fce1145412","execution":{"iopub.status.busy":"2024-02-03T22:58:05.411744Z","iopub.execute_input":"2024-02-03T22:58:05.412131Z","iopub.status.idle":"2024-02-03T22:58:07.648148Z","shell.execute_reply.started":"2024-02-03T22:58:05.412099Z","shell.execute_reply":"2024-02-03T22:58:07.647142Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Random Seed:  42\n","output_type":"stream"}]},{"cell_type":"code","source":"# If there's a GPU available...\nif torch.cuda.is_available():    \n    # Tell PyTorch to use the GPU.    \n    device = torch.device(\"cuda\")\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n# If not...\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")\n    \nngpu = torch.cuda.device_count()","metadata":{"id":"ZAnJ2iOIQsyq","outputId":"7384417a-2a7b-419b-d81a-3cc3b66cefcd","execution":{"iopub.status.busy":"2024-02-03T22:58:13.621193Z","iopub.execute_input":"2024-02-03T22:58:13.622211Z","iopub.status.idle":"2024-02-03T22:58:13.707850Z","shell.execute_reply.started":"2024-02-03T22:58:13.622180Z","shell.execute_reply":"2024-02-03T22:58:13.706679Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"There are 2 GPU(s) available.\nWe will use the GPU: Tesla T4\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## load the dataset:","metadata":{"id":"A2EynVlITXhz"}},{"cell_type":"code","source":"%%capture\n!pip install gdown \nimport gdown ","metadata":{"execution":{"iopub.status.busy":"2024-02-03T22:58:16.026163Z","iopub.execute_input":"2024-02-03T22:58:16.027105Z","iopub.status.idle":"2024-02-03T22:58:28.583044Z","shell.execute_reply.started":"2024-02-03T22:58:16.027060Z","shell.execute_reply":"2024-02-03T22:58:28.581781Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/drive') \n\n# id = \"11YeloR2eTXcTzdwI04Z-M2QVvIeQAU6-\"\nid = \"1G-XttJCGvkAVkU9N_W_PxR0Cx099qbUa\"\ngdown.download_folder(id=id, quiet=True, use_cookies=False)","metadata":{"id":"IsNFXLLDTpJk","outputId":"3c00b8ab-3143-4460-ed79-a708447a8689","execution":{"iopub.status.busy":"2024-02-03T22:58:28.584981Z","iopub.execute_input":"2024-02-03T22:58:28.585314Z","iopub.status.idle":"2024-02-03T22:58:31.127020Z","shell.execute_reply.started":"2024-02-03T22:58:28.585282Z","shell.execute_reply":"2024-02-03T22:58:31.125971Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"['/kaggle/working/SubtaskB/subtaskB_dev.jsonl',\n '/kaggle/working/SubtaskB/subtaskB_train.jsonl']"},"metadata":{}}]},{"cell_type":"markdown","source":"**Subtask B:**\n\nAn object of the JSON has the following format:","metadata":{"id":"dncLXjy5VWUd"}},{"cell_type":"markdown","source":"\n-  **id** -> identifier of the example,\n- **label** -> label (human: 0, chatGPT: 1, cohere: 2, davinci: 3, bloomz: 4, dolly: 5),\n- **text** -> text generated by machine or written by human,\n- **model** -> model name that generated data,\n- **source** -> source (Wikipedia, Wikihow, Peerread, Reddit, Arxiv) on English\n","metadata":{"id":"WV5LP7CRoOqY"}},{"cell_type":"code","source":"# content/drive/My Drive/Project\nwith open('/kaggle/working/SubtaskB/subtaskB_train.jsonl', 'r') as file:    \n    lines = file.readlines()\n\n# Parse each line as a JSON object\ntrain_objects = [json.loads(line) for line in lines] ","metadata":{"id":"jtqmb0FPTeYc","execution":{"iopub.status.busy":"2024-02-03T22:58:31.128367Z","iopub.execute_input":"2024-02-03T22:58:31.128684Z","iopub.status.idle":"2024-02-03T22:58:32.334185Z","shell.execute_reply.started":"2024-02-03T22:58:31.128658Z","shell.execute_reply":"2024-02-03T22:58:32.333400Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/working/SubtaskB/subtaskB_dev.jsonl', 'r') as file:\n    lines = file.readlines()\n\ndev_objects = [json.loads(line) for line in lines]","metadata":{"id":"ujtd9MmMUUFL","execution":{"iopub.status.busy":"2024-02-03T22:58:32.335863Z","iopub.execute_input":"2024-02-03T22:58:32.336155Z","iopub.status.idle":"2024-02-03T22:58:32.388795Z","shell.execute_reply.started":"2024-02-03T22:58:32.336130Z","shell.execute_reply":"2024-02-03T22:58:32.387894Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"len(train_objects), len(dev_objects) ","metadata":{"id":"rBcEi416T8iQ","outputId":"5994de5e-5f23-4711-e326-cc29da869334","execution":{"iopub.status.busy":"2024-02-03T22:58:32.389977Z","iopub.execute_input":"2024-02-03T22:58:32.390275Z","iopub.status.idle":"2024-02-03T22:58:32.396171Z","shell.execute_reply.started":"2024-02-03T22:58:32.390237Z","shell.execute_reply":"2024-02-03T22:58:32.395190Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"(71027, 3000)"},"metadata":{}}]},{"cell_type":"markdown","source":"an example:","metadata":{"id":"VYTe6PmLV0LW"}},{"cell_type":"code","source":"train_objects[100].keys()","metadata":{"id":"NWJNu5ETVuzX","outputId":"7d10092c-6bbb-468f-e6f1-7f0eb76f74f8","execution":{"iopub.status.busy":"2024-02-03T22:58:37.128183Z","iopub.execute_input":"2024-02-03T22:58:37.129125Z","iopub.status.idle":"2024-02-03T22:58:37.135095Z","shell.execute_reply.started":"2024-02-03T22:58:37.129092Z","shell.execute_reply":"2024-02-03T22:58:37.134212Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"dict_keys(['text', 'model', 'source', 'label', 'id'])"},"metadata":{}}]},{"cell_type":"code","source":"train_objects[100]['model'], train_objects[100]['source'],train_objects[100]['label'],train_objects[100]['id']","metadata":{"id":"LxK-CV8fV-46","outputId":"68c4f9ce-1c9b-4201-939a-34cfbbc0fd2d","execution":{"iopub.status.busy":"2024-02-03T22:58:37.721202Z","iopub.execute_input":"2024-02-03T22:58:37.721582Z","iopub.status.idle":"2024-02-03T22:58:37.728361Z","shell.execute_reply.started":"2024-02-03T22:58:37.721553Z","shell.execute_reply":"2024-02-03T22:58:37.727464Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"('chatGPT', 'wikihow', 1, 100)"},"metadata":{}}]},{"cell_type":"code","source":"print(train_objects[104]['text'])","metadata":{"id":"OgxeDd4pV5fn","outputId":"cb6fed31-5408-482f-f129-2a4e69ddb634","execution":{"iopub.status.busy":"2024-02-03T22:58:38.816314Z","iopub.execute_input":"2024-02-03T22:58:38.816675Z","iopub.status.idle":"2024-02-03T22:58:38.821808Z","shell.execute_reply.started":"2024-02-03T22:58:38.816645Z","shell.execute_reply":"2024-02-03T22:58:38.820856Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"If you're looking to set up a home PC with multiple modems and phone lines, you're in luck! With a little bit of know-how, you can have a faster, more reliable internet connection than ever before. Here's how to get started.\n\nPart 1: Setting Up Your Modems\n\nStep 1: Find out if your local ISP supports Multi-Link accounts. Some ISPs offer plans that allow you to connect multiple modems to your computer to boost your internet speeds. Check with your provider to see if this is an option for you.\n\nStep 2: Get a second modem. Most computers come with one modem installed, but if you need a second one, you can easily purchase one online or at a tech store. Make sure it's compatible with your operating system before you buy.\n\nStep 3: Use the dial-up creation dialog if you're using Windows XP. Navigate to the control panel and click \"network and internet connections.\" From there, select \"create a new connection\" and then choose \"connect to the internet.\"\n\nStep 4: Click next, and it will ask for what network type you want to use. Select \"dial-up.\"\n\nStep 5: Click next, and choose the type of service you would like to use. This will depend on your ISP. Click next and select \"connect using dial-up modem.\"\n\nStep 6: Click next; give the dial-up connection a name (this can be anything you want). Click next, and enter the telephone number needed to access the net.\n\nStep 7: Click next and enter the user name and password, which will be used for logging into the server. Click next and make sure that the check boxes are set up the way you want them to stay.\n\nPart 2: Connecting Your Phone Lines\n\nStep 1: Create the dialog. To set up a multi-link connection, you'll need to connect two phone lines to your computer. Make sure both phones are clear in tone with no noticeable static or wavering noises.\n\nStep 2: Connect the phone lines to each phone line junction box. Make sure to follow the instructions that came with your modem.\n\nStep 3: Double click on the name of the icon created for your connection. This will open up your connection dialog.\n\nStep 4: Hook up two computers using a CAT5 cable through a 5 port Switch. This will allow you to share your internet connection between multiple computers.\n\nStep 5: Click on the properties button on the connection dialog. From there, enable software compression, and negotiating multi-link for single dial-up.\n\nStep 6: Click the ok button, and if needed now you can set up whatever IP address is used for your connection.\n\nStep 7: Setup one computer for the gateway dial out (use the advanced tab for this). This will allow you to share your internet connection between multiple computers.\n\nAnd that's it! With these steps, you'll be up and running with a high-speed, reliable internet connection that's perfect for all your home computing needs. Bear in mind that setting up multiple modems and phone lines can be a complex process, especially if you're not particularly tech-savvy. If you're having difficulty, don't hesitate to reach out for help from a qualified tech support professional.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"More details about the dataset and the Exploratory Data Analysis have been reported in `EDA.ipynb`.","metadata":{}},{"cell_type":"markdown","source":"## load the pretrained `RoBERTa`/`DidstilBert` from $huggingface$:","metadata":{"id":"ubyocB_epmnh"}},{"cell_type":"code","source":"!pip install transformers\n!pip install sentencepiece","metadata":{"id":"dKyrJEtPQSzH","outputId":"02800931-b2b5-448a-dd7f-f13ff732d00c","execution":{"iopub.status.busy":"2024-02-03T22:58:39.768960Z","iopub.execute_input":"2024-02-03T22:58:39.769338Z","iopub.status.idle":"2024-02-03T22:59:04.257871Z","shell.execute_reply.started":"2024-02-03T22:58:39.769308Z","shell.execute_reply":"2024-02-03T22:59:04.256811Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.37.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.1)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.1.99)\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import RobertaTokenizer, RobertaModel\nfrom transformers import DistilBertTokenizer, DistilBertModel\nfrom transformers import AlbertModel, AlbertTokenizer\n\nimport sentencepiece\nfrom transformers import get_constant_schedule_with_warmup","metadata":{"id":"sOOunD55QVXY","execution":{"iopub.status.busy":"2024-02-03T22:59:04.260052Z","iopub.execute_input":"2024-02-03T22:59:04.260394Z","iopub.status.idle":"2024-02-03T22:59:07.900709Z","shell.execute_reply.started":"2024-02-03T22:59:04.260363Z","shell.execute_reply":"2024-02-03T22:59:07.899921Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Load pre-trained BERT model and tokenizer\nmodel_name_roberta = 'roberta-large'\n# tokenizer = RobertaTokenizer.from_pretrained(model_name)\n# bert_model = RobertaModel.from_pretrained(model_name)\n\nmodel_name_distil = 'distilbert-base-uncased'\nmodel_name_albert = 'albert-base-v2'\n\ntokenizer = RobertaTokenizer.from_pretrained(model_name_roberta)\n# bert_model = DistilBertModel.from_pretrained(model_name_distil)\n\n# bert_generator = DistilBertModel.from_pretrained(model_name_distil)","metadata":{"id":"zZp3vmPPQiqs","outputId":"47183acf-b17b-4120-d696-481f1657c79b","execution":{"iopub.status.busy":"2024-02-03T22:59:07.901765Z","iopub.execute_input":"2024-02-03T22:59:07.902189Z","iopub.status.idle":"2024-02-03T22:59:09.482983Z","shell.execute_reply.started":"2024-02-03T22:59:07.902162Z","shell.execute_reply":"2024-02-03T22:59:09.482075Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d665bc4e80542e099123fac60698d52"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29557801e979467aae3ee21e930427e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6674ea780b74ae5bebc0ab3dbd8e206"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2e6ea6b1fb64e5f92d74c11ee314eeb"}},"metadata":{}}]},{"cell_type":"code","source":"class BERT_Embedder(nn.Module):\n    def __init__(self, bert_modele):\n        super(BERT_Embedder, self).__init__()\n        self.bert = bert_modele\n\n    def forward(self, encoded_ids,attention_mask):\n        outputs = self.bert(encoded_ids,attention_mask)\n        last_hidden_states = outputs.last_hidden_state[:,0]  # return embedding of 'CLS' token for classification.\n\n        return last_hidden_states","metadata":{"id":"6DyPAryTRPeF","execution":{"iopub.status.busy":"2024-02-03T22:59:09.485128Z","iopub.execute_input":"2024-02-03T22:59:09.485455Z","iopub.status.idle":"2024-02-03T22:59:09.496125Z","shell.execute_reply.started":"2024-02-03T22:59:09.485428Z","shell.execute_reply":"2024-02-03T22:59:09.495156Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"# **Bag of Words**","metadata":{}},{"cell_type":"markdown","source":"## Data cleaning","metadata":{}},{"cell_type":"markdown","source":"- Expand Contractions","metadata":{}},{"cell_type":"code","source":"# For regular expressions\nimport re\n\n# Dictionary of English Contractions\ncontractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\n                     \"can't\": \"cannot\",\"can't've\": \"cannot have\",\n                     \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n                     \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n                     \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n                     \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n                     \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n                     \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n                     \"I'd\": \"I would\", \"I'd've\": \"I would have\",\"I'll\": \"I will\",\n                     \"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\", \"isn't\": \"is not\",\n                     \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n                     \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n                     \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n                     \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n                     \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n                     \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n                     \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n                     \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n                     \"she'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n                     \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n                     \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n                     \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n                     \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n                     \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\n                     \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n                     \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n                     \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n                     \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n                     \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n                     \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n                     \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n                     \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n                     \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n                     \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n                     \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n                     \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n                     \"you've\": \"you have\"}\n\n# Regular expression for finding contractions\ncontractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n\n# Function for expanding contractions\ndef expand_contractions(text,contractions_dict=contractions_dict):\n    def replace(match):\n        return contractions_dict[match.group(0)]\n    return contractions_re.sub(replace, text)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-03T22:59:09.497559Z","iopub.execute_input":"2024-02-03T22:59:09.497915Z","iopub.status.idle":"2024-02-03T22:59:09.517315Z","shell.execute_reply.started":"2024-02-03T22:59:09.497882Z","shell.execute_reply":"2024-02-03T22:59:09.516603Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"- Remove digits and words containing digits","metadata":{}},{"cell_type":"code","source":"def demove_digts(x):\n    return re.sub('\\w*\\d\\w*','', x)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T22:57:47.389729Z","iopub.status.idle":"2024-02-03T22:57:47.390058Z","shell.execute_reply.started":"2024-02-03T22:57:47.389890Z","shell.execute_reply":"2024-02-03T22:57:47.389904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Remove Punctuations","metadata":{}},{"cell_type":"code","source":"import string\n\ndef demove_punctuations(x):\n    out = re.sub('[%s]' % re.escape(string.punctuation), '', x)\n    return re.sub(' +',' ',out) # Removing extra spaces","metadata":{"execution":{"iopub.status.busy":"2024-02-03T22:55:21.838469Z","iopub.execute_input":"2024-02-03T22:55:21.838790Z","iopub.status.idle":"2024-02-03T22:55:21.849299Z","shell.execute_reply.started":"2024-02-03T22:55:21.838766Z","shell.execute_reply":"2024-02-03T22:55:21.848321Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"\nclass Fake_Dataset(Dataset):\n    \"\"\"\n    Generate the fake sentences as inputs of G2 (BERT Generator) \n    \"\"\"\n    def __init__(self, json_file,seq_length, tokenizer):\n\n        self.json_file = json_file\n        self.seq_length = seq_length\n        self.tokenizer = tokenizer\n        self.weighted_keys = self.create_distribiution()\n\n    def __len__(self):\n        return len(self.json_file)\n    \n    def text_cleaner(self,text):\n         return demove_punctuations(demove_digts(expand_contractions(text)))\n    \n    \n    def create_distribiution(self):\n        token_counts = Counter([]) \n#         i = 0\n        for sample in tqdm(self.json_file):\n            text = self.text_cleaner(sample['text'])\n            tokens_a = self.tokenizer.tokenize(text)\n            input_ids = self.tokenizer.convert_tokens_to_ids(tokens_a)\n            token_counts += Counter(input_ids) \n#             i += 1\n#             if i == 1000:\n#                 break\n        token_counts_dict = dict(token_counts)\n        weighted_keys = [key for key, count in token_counts_dict.items() for _ in range(count)]\n        return weighted_keys\n        \n\n    def sampler(self,seq_length):\n        input_ids = random.choices(self.weighted_keys, k=seq_length)\n        input_ids = torch.tensor(input_ids, dtype=torch.long)\n        input_mask = torch.ones(input_ids.shape[0],dtype=torch.int32)\n        return input_ids, input_mask \n\n    def __getitem__(self,idx):\n        input_ids, input_mask = self.sampler(self.seq_length)\n        return input_ids, input_mask #input_ids.squeeze(0)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T22:57:47.391135Z","iopub.status.idle":"2024-02-03T22:57:47.391522Z","shell.execute_reply.started":"2024-02-03T22:57:47.391330Z","shell.execute_reply":"2024-02-03T22:57:47.391346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can define **Discriminator** and **Generator** completely:","metadata":{"id":"ah-BDYpjTDNR"}},{"cell_type":"code","source":"# custom weights initialization\nimport torch.nn.init as init\n\ndef custom_weights_init(m):\n    if isinstance(m, nn.Linear):\n        init.xavier_normal_(m.weight.data)\n        if m.bias is not None:\n            init.constant_(m.bias.data, 0)\n","metadata":{"id":"b00Mx2wuRUmF","execution":{"iopub.status.busy":"2024-02-03T22:59:09.518481Z","iopub.execute_input":"2024-02-03T22:59:09.518794Z","iopub.status.idle":"2024-02-03T22:59:09.531408Z","shell.execute_reply.started":"2024-02-03T22:59:09.518769Z","shell.execute_reply":"2024-02-03T22:59:09.530624Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self, input_size, num_classes,dropout_rate=0.2,relu_slop=0.2):\n        super(Discriminator, self).__init__()\n        self.input_size = input_size\n        self.num_classes = num_classes\n\n        self.main = torch.nn.Sequential(\n            Dropout(p=dropout_rate),\n            \n            Linear(in_features=self.input_size, out_features=512, bias=True),\n            nn.LeakyReLU(relu_slop, inplace=True),\n            Dropout(p=dropout_rate),\n            \n            Linear(in_features=512, out_features=256, bias=True),\n            nn.LeakyReLU(relu_slop, inplace=True),\n            Dropout(p=dropout_rate),\n            \n            Linear(in_features=256, out_features=256, bias=True),\n            nn.LeakyReLU(relu_slop, inplace=True),\n            Dropout(p=dropout_rate),\n        )\n\n        self.logit = nn.Linear(256,self.num_classes+1)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, input):\n        last_rep = self.main(input)  # for do 'feature matching'\n        logits = self.logit(last_rep)\n        probs = self.softmax(logits)\n        return last_rep, logits, probs\n","metadata":{"id":"Ml-JM9SsTMM5","execution":{"iopub.status.busy":"2024-02-03T22:59:09.532360Z","iopub.execute_input":"2024-02-03T22:59:09.532627Z","iopub.status.idle":"2024-02-03T22:59:09.542956Z","shell.execute_reply.started":"2024-02-03T22:59:09.532603Z","shell.execute_reply":"2024-02-03T22:59:09.542110Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"class Generator1(nn.Module):\n    def __init__(self, input_size, output_size,dropout_rate=0.2,relu_slop=0.2):\n        super(Generator1, self).__init__()\n\n        self.input_size = input_size\n        self.output_size = output_size\n\n        self.main = torch.nn.Sequential(\n            Linear(in_features=self.input_size, out_features=256, bias=True),\n            nn.LeakyReLU(relu_slop, inplace=True),\n            Dropout(p=dropout_rate, inplace=False),\n            Linear(in_features=256, out_features=self.output_size, bias=True),\n        )\n\n    def forward(self, input):\n        return self.main(input)\n","metadata":{"id":"BYUu-z4NTCp5","execution":{"iopub.status.busy":"2024-02-03T22:59:09.543944Z","iopub.execute_input":"2024-02-03T22:59:09.544224Z","iopub.status.idle":"2024-02-03T22:59:09.555066Z","shell.execute_reply.started":"2024-02-03T22:59:09.544192Z","shell.execute_reply":"2024-02-03T22:59:09.554295Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"class Generator2(nn.Module):\n    def __init__(self, bert_modele):\n        super(Generator2, self).__init__()\n        self.bert = bert_modele\n\n    def forward(self, encoded_ids,attention_mask):\n        outputs = self.bert(encoded_ids,attention_mask)\n        last_hidden_states = outputs.last_hidden_state[:,0]  # return embedding of 'CLS' token for classification.\n\n        return last_hidden_states","metadata":{"execution":{"iopub.status.busy":"2024-02-03T22:59:09.555957Z","iopub.execute_input":"2024-02-03T22:59:09.556209Z","iopub.status.idle":"2024-02-03T22:59:09.563629Z","shell.execute_reply.started":"2024-02-03T22:59:09.556186Z","shell.execute_reply":"2024-02-03T22:59:09.562894Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"---\n---","metadata":{}},{"cell_type":"code","source":"%%capture\n# !pip install datasets\n!pip install -qq adapters datasets\n!pip install accelerate -U","metadata":{"execution":{"iopub.status.busy":"2024-02-03T22:59:09.566736Z","iopub.execute_input":"2024-02-03T22:59:09.566996Z","iopub.status.idle":"2024-02-03T22:59:45.253372Z","shell.execute_reply.started":"2024-02-03T22:59:09.566973Z","shell.execute_reply":"2024-02-03T22:59:45.252246Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"from transformers import RobertaConfig,DistilBertConfig, AlbertConfig\nfrom adapters import AutoAdapterModel  ","metadata":{"execution":{"iopub.status.busy":"2024-02-03T22:59:45.254789Z","iopub.execute_input":"2024-02-03T22:59:45.255091Z","iopub.status.idle":"2024-02-03T22:59:45.307313Z","shell.execute_reply.started":"2024-02-03T22:59:45.255062Z","shell.execute_reply":"2024-02-03T22:59:45.306317Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"num_classes = 6\nfrom adapters import AutoAdapterModel\nconfig = RobertaConfig.from_pretrained(model_name_roberta, num_labels=num_classes)\n\ndef BertWithAdapter(model_name, adapter_name):\n\n    model = AutoAdapterModel.from_pretrained(model_name, config=config).to(device)\n    # Add a new adapter\n    model.add_adapter(adapter_name)\n    model.set_active_adapters = adapter_name\n\n    # Add a matching classification head\n    model.add_classification_head(adapter_name, num_labels=768, overwrite_ok=True)  \n    # Activate the adapter\n    model.train_adapter(adapter_name)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-02-03T22:59:45.308739Z","iopub.execute_input":"2024-02-03T22:59:45.309555Z","iopub.status.idle":"2024-02-03T22:59:45.407563Z","shell.execute_reply.started":"2024-02-03T22:59:45.309506Z","shell.execute_reply":"2024-02-03T22:59:45.406793Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"adapter_name = \"SubtaskB\"\n# generator2_ = BertWithAdapter(model_name, adapter_name).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T22:59:45.408658Z","iopub.execute_input":"2024-02-03T22:59:45.408989Z","iopub.status.idle":"2024-02-03T22:59:45.413150Z","shell.execute_reply.started":"2024-02-03T22:59:45.408957Z","shell.execute_reply":"2024-02-03T22:59:45.412204Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"Set Hyperparameter :","metadata":{"id":"TPbVNzM3vnWH"}},{"cell_type":"code","source":"num_classes = 6\ninput_size = 768\nnoise_size = 100\nlabel_list = list(range(6))","metadata":{"id":"24sxiObdh5z7","execution":{"iopub.status.busy":"2024-02-03T22:59:45.414596Z","iopub.execute_input":"2024-02-03T22:59:45.414937Z","iopub.status.idle":"2024-02-03T22:59:45.423061Z","shell.execute_reply.started":"2024-02-03T22:59:45.414904Z","shell.execute_reply":"2024-02-03T22:59:45.422096Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"### Define the Dataset class:","metadata":{"id":"YyyGFvUSW1ml"}},{"cell_type":"code","source":"\nclass SemEval_Dataset(Dataset):\n    def __init__(self, json_file,label_list,label_masks,\n                 max_seq_length, tokenizer,dtype=torch.long):\n\n        self.json_file = json_file\n        self.dtype = dtype\n        self.label_list = label_list # [0, 1, 2, 3, 4, 5]\n        self.max_seq_length = max_seq_length\n        self.tokenizer = tokenizer\n        self.label_masks = label_masks\n\n    def __len__(self):\n        return len(self.json_file)\n\n    def feature_extractor(self, text, label=None):\n        features = []\n        tokenized_text = tokenizer(text,padding='max_length', truncation=True,\n                                   max_length=self.max_seq_length,\n                                   return_tensors=\"pt\")\n\n        input_ids = tokenized_text['input_ids']\n        input_mask = tokenized_text['attention_mask']\n\n        if len(input_ids) > self.max_seq_length:\n            input_ids = input_ids[0:(self.max_seq_length)]   # crop long sentences\n            input_mask = input_mask[0:(self.max_seq_length)]\n\n        assert len(input_ids[0]) == self.max_seq_length\n        assert len(input_mask[0]) == self.max_seq_length\n\n        if label != None:\n            return input_ids, input_mask, label\n        else:\n            return input_ids, input_mask\n\n    def __getitem__(self, idx):\n        data = self.json_file[idx]\n        input_ids, input_mask, label_id = self.feature_extractor(data['text'], label=data['label'])\n\n        return input_ids.squeeze(0), input_mask.squeeze(0), data['label'], self.label_masks[idx]","metadata":{"id":"hTsPHA58npxK","execution":{"iopub.status.busy":"2024-02-03T22:59:45.424582Z","iopub.execute_input":"2024-02-03T22:59:45.424969Z","iopub.status.idle":"2024-02-03T22:59:45.437083Z","shell.execute_reply.started":"2024-02-03T22:59:45.424935Z","shell.execute_reply":"2024-02-03T22:59:45.436259Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"Create Dataset and Dataloader :","metadata":{"id":"5-6cooKmuy6d"}},{"cell_type":"code","source":"max_seq_length = 256\nbatch_size = 24","metadata":{"id":"XbCXkjW2Ib9p","execution":{"iopub.status.busy":"2024-02-03T22:59:45.438171Z","iopub.execute_input":"2024-02-03T22:59:45.438517Z","iopub.status.idle":"2024-02-03T22:59:45.450911Z","shell.execute_reply.started":"2024-02-03T22:59:45.438486Z","shell.execute_reply":"2024-02-03T22:59:45.450154Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"unlabeled_examples = True\nlabeled_ratio = 1               # 0.01, 0.1 ,0.05 ,0.5 \ntrain_dataset_size_labeled = int(labeled_ratio* len(train_objects))\n\n#The labeled (train) dataset is assigned with a mask set to True\ntrain_label_masks = torch.ones(train_dataset_size_labeled, dtype=bool)\n#If unlabel examples are available \nif unlabeled_examples:\n  #The unlabeled (train) dataset is assigned with a mask set to False\n    tmp_masks = torch.zeros(len(train_objects)- train_dataset_size_labeled , dtype=bool)\n    train_label_masks = torch.concatenate([train_label_masks,tmp_masks])\n    idx = torch.randperm(train_label_masks.shape[0])\n    train_label_masks = train_label_masks[idx].view(train_label_masks.size())\n\nassert train_label_masks.shape[0] == len(train_objects)\ntrain_dataset = SemEval_Dataset(train_objects, label_list, train_label_masks,max_seq_length, tokenizer)\n# train_dataset = torch.utils.data.Subset(train_dataset, [i for i in range(train_dataset_size)])","metadata":{"id":"vBsI2GALOlIS","execution":{"iopub.status.busy":"2024-02-03T22:59:45.451918Z","iopub.execute_input":"2024-02-03T22:59:45.452232Z","iopub.status.idle":"2024-02-03T22:59:45.534745Z","shell.execute_reply.started":"2024-02-03T22:59:45.452199Z","shell.execute_reply":"2024-02-03T22:59:45.533753Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"train_size = int(0.8 * len(train_objects))  # 80% for training\nval_size = len(train_objects) - train_size  # Remaining 20% for validation\n\ntrain_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\ntest_label_masks = torch.ones(len(dev_objects), dtype=bool)\ntest_dataset = SemEval_Dataset(dev_objects, label_list, test_label_masks,max_seq_length, tokenizer)\n\ntrain_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, num_workers=os.cpu_count(),shuffle=True, drop_last=False)\nval_dataloader = DataLoader(dataset=val_dataset, batch_size=batch_size, num_workers=os.cpu_count(),shuffle=True, drop_last=False)\ntest_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, num_workers=os.cpu_count(),shuffle=True, drop_last=False)","metadata":{"id":"qQdi1pyGtCE6","execution":{"iopub.status.busy":"2024-02-03T22:59:45.535917Z","iopub.execute_input":"2024-02-03T22:59:45.536203Z","iopub.status.idle":"2024-02-03T22:59:45.551942Z","shell.execute_reply.started":"2024-02-03T22:59:45.536178Z","shell.execute_reply":"2024-02-03T22:59:45.551175Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"print('Number of train samples: ', len(train_dataset))\nprint('Number of validation samples: ', len(val_dataset))\nprint('Number of test samples: ', len(test_dataset))","metadata":{"id":"DGwYFnbRPPsP","outputId":"3ef791d9-52a9-43ed-b3cd-13fa96d3bf6b","execution":{"iopub.status.busy":"2024-02-03T22:59:45.553059Z","iopub.execute_input":"2024-02-03T22:59:45.553351Z","iopub.status.idle":"2024-02-03T22:59:45.558760Z","shell.execute_reply.started":"2024-02-03T22:59:45.553326Z","shell.execute_reply":"2024-02-03T22:59:45.557727Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Number of train samples:  56821\nNumber of validation samples:  14206\nNumber of test samples:  3000\n","output_type":"stream"}]},{"cell_type":"code","source":"# test train_dataset\nbetch = next(iter(train_dataloader))\nprint(f'input_ids shape: {betch[0].shape}, \\ninput_mask shape: {betch[1].shape}, \\\n        \\nlabel_ids shape: {betch[2].shape},\\nlabel_mask shape: {betch[3].shape}')","metadata":{"id":"UUXhfRCQxR2G","outputId":"90b8fef2-6212-461a-dd23-f21adbf11a0d","execution":{"iopub.status.busy":"2024-02-03T22:59:45.562636Z","iopub.execute_input":"2024-02-03T22:59:45.562978Z","iopub.status.idle":"2024-02-03T22:59:46.277193Z","shell.execute_reply.started":"2024-02-03T22:59:45.562946Z","shell.execute_reply":"2024-02-03T22:59:46.275955Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"input_ids shape: torch.Size([24, 256]), \ninput_mask shape: torch.Size([24, 256]),         \nlabel_ids shape: torch.Size([24]),\nlabel_mask shape: torch.Size([24])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Create a **Fake_Dataset** :","metadata":{}},{"cell_type":"code","source":"fake_dataset =  Fake_Dataset(train_objects,256, tokenizer) ","metadata":{"execution":{"iopub.status.busy":"2024-02-03T22:02:16.692197Z","iopub.execute_input":"2024-02-03T22:02:16.692553Z","iopub.status.idle":"2024-02-03T22:11:33.735169Z","shell.execute_reply.started":"2024-02-03T22:02:16.692525Z","shell.execute_reply":"2024-02-03T22:11:33.734328Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"100%|██████████| 71027/71027 [09:15<00:00, 127.77it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"noisy_dataloader = DataLoader(dataset=fake_dataset, batch_size=batch_size, num_workers=os.cpu_count(),shuffle=True) ","metadata":{"execution":{"iopub.status.busy":"2024-02-03T22:11:33.737138Z","iopub.execute_input":"2024-02-03T22:11:33.737825Z","iopub.status.idle":"2024-02-03T22:11:33.742808Z","shell.execute_reply.started":"2024-02-03T22:11:33.737788Z","shell.execute_reply":"2024-02-03T22:11:33.741823Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"next(iter(noisy_dataloader))[0] ","metadata":{"execution":{"iopub.status.busy":"2024-02-03T22:11:33.744043Z","iopub.execute_input":"2024-02-03T22:11:33.744317Z","iopub.status.idle":"2024-02-03T22:11:34.133806Z","shell.execute_reply.started":"2024-02-03T22:11:33.744294Z","shell.execute_reply":"2024-02-03T22:11:34.132560Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"tensor([[   13,   144,    16,  ...,    25, 14921, 17836],\n        [  809,  9270,   763,  ...,  2261,  5411,   308],\n        [ 2459,  4681,   978,  ...,    42,    88,   251],\n        ...,\n        [   19, 10466,    25,  ...,  3547,    21,  4325],\n        [  225,  2313, 14748,  ...,    17,    17, 15789],\n        [ 2274,    25,    20,  ..., 17301,  5340,    20]])"},"metadata":{}}]},{"cell_type":"markdown","source":"## GAN-BERT","metadata":{"id":"AwpJPzUqbgzI"}},{"cell_type":"markdown","source":"Hyperparameters:","metadata":{"id":"ywMFAgkjjQeG"}},{"cell_type":"code","source":"epoch_num = 5\n\nlearning_rate = 5e-4\nnoise_size = 100\nepsilon = 1e-8\nwarmup_proportion = 0.1  #TODO","metadata":{"id":"GaylbkGFOBX5","execution":{"iopub.status.busy":"2024-02-03T22:59:52.807604Z","iopub.execute_input":"2024-02-03T22:59:52.808016Z","iopub.status.idle":"2024-02-03T22:59:52.813285Z","shell.execute_reply.started":"2024-02-03T22:59:52.807975Z","shell.execute_reply":"2024-02-03T22:59:52.812296Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# Create the Discriminator and Generator\ndiscriminator = Discriminator(input_size,num_classes).to(device)\ngenerator1 = Generator1(noise_size,input_size).to(device)\n# bert = BERT_Embedder(bert_model).to(device)\n# generator2_ = BertWithAdapter(model_name_albert, adapter_name).to(device)\nbert = BertWithAdapter(model_name_roberta, adapter_name).to(device)\n\n# bert = BERT_Embedder(bert_model).to(device)\n\n\n# Handle multi-GPU if desired\n# if (device.type == 'cuda') and (ngpu > 1):\n#     discriminator = nn.DataParallel(discriminator, list(range(ngpu)))\n#     generator2_ = nn.DataParallel(generator2_, list(range(ngpu)))    \n#     bert = nn.DataParallel(bert, list(range(ngpu)))\n\n# weights initialization   # TODO : Xavier weight initialization\n# discriminator.apply(custom_weights_init)\n# generator1.apply(custom_weights_init)\n\n# print(discriminator)\n# print()\n# print(generator1)\n# print()\n# print(bert)","metadata":{"id":"-COnKU7NhU2x","outputId":"28480ff1-cc2c-405c-9903-2182abe3723f","execution":{"iopub.status.busy":"2024-02-03T22:59:53.432242Z","iopub.execute_input":"2024-02-03T22:59:53.432623Z","iopub.status.idle":"2024-02-03T23:00:01.637524Z","shell.execute_reply.started":"2024-02-03T22:59:53.432597Z","shell.execute_reply":"2024-02-03T23:00:01.636588Z"},"trusted":true},"execution_count":39,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a4ac00349c340fea1f2930e18e499a0"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['heads.default.3.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import TransformerEncoder, TransformerEncoderLayer\n\n# Define the TransformerEncoderModel\nclass TransformerEncoderModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_layers, num_heads,num_classes):\n        super(TransformerEncoderModel, self).__init__()\n        self.num_classes = num_classes\n        self.hidden_dim = hidden_dim\n        self.embedding = nn.Linear(input_dim, hidden_dim)\n        encoder_layers = TransformerEncoderLayer(hidden_dim, num_heads)\n        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers)\n        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n        \n        self.fc_layers = torch.nn.Sequential(\n            Dropout(p=0.3),\n            \n            Linear(in_features=768, out_features=256, bias=True),\n            nn.LeakyReLU(0.2, inplace=True),\n            Dropout(p=0.3),\n            \n            Linear(in_features=256, out_features=256, bias=True),\n            nn.LeakyReLU(0.2, inplace=True),\n            Dropout(p=0.3),\n        )\n\n        self.logit = nn.Linear(256,self.num_classes+1)\n        self.softmax = nn.Softmax(dim=-1)\n        \n        \n    def forward(self, x):\n        x = self.embedding(x)\n        x = x.permute(1, 0, 2)  # Reshape to (seq_len, batch_size, hidden_dim)\n        output = self.transformer_encoder(x)\n        output = output.permute(1, 0, 2)  # Reshape back to (batch_size, seq_len, hidden_dim)\n        output = self.global_avg_pool(output) \n#         print(output.shape)\n\n        output = output.view(output.shape[0], -1)\n        \n\n        last_rep = self.fc_layers(output)\n        logits = self.logit(last_rep)\n        probs = self.softmax(logits)\n\n        return last_rep, logits, probs  \n","metadata":{"execution":{"iopub.status.busy":"2024-02-03T22:51:49.502274Z","iopub.execute_input":"2024-02-03T22:51:49.502559Z","iopub.status.idle":"2024-02-03T22:51:49.508513Z","shell.execute_reply.started":"2024-02-03T22:51:49.502534Z","shell.execute_reply":"2024-02-03T22:51:49.507428Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# Example usage\ninput_dim = 1  # Dimension of each time step in the time series\nhidden_dim = 256  # Hidden dimension of the transformer\nnum_layers = 2  # Number of transformer encoder layers\nnum_heads = 8  # Number of attention heads\n\n# Create an instance of the TransformerEncoderModel\ndiscriminator = TransformerEncoderModel(input_dim, hidden_dim, num_layers, num_heads, 6).to(device)\ndiscriminator.apply(custom_weights_init)\n\n# Generate some dummy time series data\nbatch_size = 16\nseq_len = 768\nsample = torch.randn(batch_size, seq_len, input_dim).to(device)\n\n# Pass the time series through the transformer encoder\nresult = discriminator(sample)    # shape (batch_size, seq_len, hidden_dim) \n\n# D_real_features, D_real_logits, D_real_probs","metadata":{"execution":{"iopub.status.busy":"2024-02-03T22:51:49.509672Z","iopub.execute_input":"2024-02-03T22:51:49.510298Z","iopub.status.idle":"2024-02-03T22:51:49.527678Z","shell.execute_reply.started":"2024-02-03T22:51:49.510267Z","shell.execute_reply":"2024-02-03T22:51:49.526877Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"### Define the Optimizers and Scheduler:","metadata":{"id":"ja2TR4WPi3fY"}},{"cell_type":"code","source":"gen_optimizer = torch.optim.AdamW(generator1.parameters(), lr=learning_rate)\ndis_optimizer = torch.optim.AdamW(list(bert.parameters()) + list(discriminator.parameters()), lr=learning_rate)\n\n#scheduler\nnum_train_examples = len(train_dataset)\nnum_train_steps = int(num_train_examples / batch_size * epoch_num) \nnum_warmup_steps = int(num_train_steps * warmup_proportion)\n\nscheduler_d = get_constant_schedule_with_warmup(dis_optimizer, \n                                       num_warmup_steps = num_warmup_steps)\nscheduler_g = get_constant_schedule_with_warmup(gen_optimizer, \n                                       num_warmup_steps = num_warmup_steps) ","metadata":{"id":"2zuKJdvdgjoO","execution":{"iopub.status.busy":"2024-02-03T23:00:01.639542Z","iopub.execute_input":"2024-02-03T23:00:01.640223Z","iopub.status.idle":"2024-02-03T23:00:01.651184Z","shell.execute_reply.started":"2024-02-03T23:00:01.640182Z","shell.execute_reply":"2024-02-03T23:00:01.650313Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":" The loss function of $Discriminator$ is defined as:  $$\\quad L_{\\mathcal{D}}=L_{\\mathcal{D}_{\\text {sup. }}}+L_{\\mathcal{D}_{\\text {unsup. }}}$$\n where:\n$$\n\\begin{aligned}\nL_{\\mathcal{D}_{\\text {sup. }}} & =-\\mathbb{E}_{x, y \\sim p_d} \\log \\left[p_{\\mathrm{m}}(\\hat{y}=y \\mid x, y \\in(1, \\ldots, k))\\right] \\\\\nL_{\\mathcal{D}_{\\text {unsup. }}} & =-\\mathbb{E}_{x \\sim p_d} \\log \\left[1-p_{\\mathrm{m}}(\\hat{y}=y \\mid x, y=k+1)\\right] -\\mathbb{E}_{x \\sim \\mathcal{G}} \\log \\left[p_{\\mathrm{m}}(\\hat{y}=y \\mid x, y=k+1)\\right] \\\\\n\\rightarrow  L_{\\mathcal{D}_{\\text {unsup. }}} & =-\\mathbb{E}_{x \\sim p_d} [\\log (\\mathcal{D}(x))] -\\mathbb{E}_{x \\sim \\mathcal{G}} \n[\\log (1-\\mathcal{D}(x))]\n\\end{aligned}\n$$","metadata":{}},{"cell_type":"markdown","source":"And loss function of $Generator$ is defined as: $$\\quad L_{\\mathcal{G}}=L_{\\mathcal{G}_{\\text {feature matching }}}+L_{\\mathcal{G}_{\\text {unsup. }}}$$ \nwhere:\n\n$$L_{\\mathcal{G}_{\\text {unsup. }}}=-\\mathbb{E}_{x \\sim \\mathcal{G}} \n\\log \\left[1-p_m(\\hat{y}=y \\mid x, y=k+1)\\right]$$\n\n$$ L_{\\mathcal{G}_{\\text {feature matching }}} = ||\\mathbb{E}_{x \\sim p_d} f(x) \n- \\mathbb{E}_{x \\sim \\mathcal{G}} f(x) ||_2^2$$\n","metadata":{}},{"cell_type":"code","source":"class GANBERT():\n    def __init__(self, discriminator, generator, bert,gen_optimizer, dis_optimizer,\n                scheduler_d,scheduler_g, path,G2=False): \n\n        self.discriminator = discriminator\n        self.generator = generator\n        self.bert = bert\n        self.gen_optimizer = gen_optimizer\n        self.dis_optimizer = dis_optimizer\n        self.scheduler_g = scheduler_g\n        self.scheduler_d = scheduler_d\n        self.nll_loss = torch.nn.CrossEntropyLoss(ignore_index=-1 , label_smoothing=0.005) # which one\n        self.path = path\n        self.G2 = G2\n \n    def trainer(self, epoch_num, label_list,labeled_ratio,\n               train_dataloader, val_dataloader=None, noisy_dataloader=None\n                ,report=True):\n        \n        best_score = 1e-5\n    \n        def format_time(elapsed):\n            '''\n            Takes a time in seconds and returns a string hh:mm:ss\n            '''\n            # Round to the nearest second.\n            elapsed_rounded = int(round((elapsed)))\n            # Format as hh:mm:ss\n            return str(datetime.timedelta(seconds=elapsed_rounded))\n        \n        results = []\n        print(f'With labeled_ratio : {labeled_ratio}\\n')\n        for epoch in range(epoch_num):\n            # Measure how long the each epoch takes.\n            t0 = time.time()\n            \n            self.bert.train()\n            self.generator.train()\n            self.discriminator.train()\n\n            tr_g_loss = 0\n            tr_d_loss = 0\n            nb_tr_examples, nb_tr_steps = 0, 0\n\n            print(f'Epoch {epoch+1}/{epoch_num} :')\n            for step, batch in enumerate(train_dataloader):\n\n                src_input_ids, src_input_mask, label_ids, b_label_mask = batch # unpacking\n                src_input_ids = src_input_ids.to(device)\n                src_input_mask = src_input_mask.to(device)\n                label_ids = label_ids.to(device)\n                b_label_mask = b_label_mask.to(device)\n\n                self.bert.zero_grad()\n                self.discriminator.zero_grad()\n\n                # Real representations\n                embedding = self.bert(src_input_ids, attention_mask=src_input_mask)['logits']#.unsqueeze(-1)\n#                 print(embedding.shape,'ppp')\n#                 print(embedding.shape)\n\n                D_real_features, D_real_logits, D_real_probs = self.discriminator(embedding)\n#                 print(D_real_probs.shape, 'ppppp')\n                # Random noise\n                if self.G2:\n                    noisy_input_ids, noisy_input_mask = next(iter(noisy_dataloader))\n                    noisy_input_ids = noisy_input_ids.to(device)\n                    noisy_input_mask = noisy_input_mask.to(device) #.to(torch.long)\n                    gen_rep = self.generator(noisy_input_ids, attention_mask=noisy_input_mask)['logits']#.unsqueeze(-1)\n#                     gen_rep = torch.ones(728).to(device)\n                else:\n                    noise = torch.zeros(src_input_ids.shape[0],noise_size, device=device).uniform_(0, 1)\n                    gen_rep = self.generator(noise)\n                \n                \n                ############################\n                # Update Generator network: minimize -E[log(D(G(z)))] + feature_matching LOSS\n                ###########################\n                D_fake_features, D_fake_logits, D_fake_probs = self.discriminator(gen_rep) # .detach()\n\n                g_loss_d = -1 * torch.mean(torch.log(1 - D_fake_probs[:,-1] + epsilon))\n                g_feat_reg = torch.mean(torch.pow(torch.mean(D_real_features, dim=0) - torch.mean(D_fake_features, dim=0), 2))\n                g_loss = g_loss_d + g_feat_reg\n\n                ############################\n                #  Update Discriminator network: minimize -E[log(D(x)) + log(1 - D(G(z)))]\n                ###########################\n                logits = D_real_logits[:,0:-1]\n                log_probs = F.log_softmax(logits, dim=-1)\n                # The discriminator provides an output for labeled and unlabeled real data\n                # so the loss evaluated for unlabeled data is ignored (masked)\n                label2one_hot = torch.nn.functional.one_hot(label_ids, len(label_list))\n                per_example_loss = -torch.sum(label2one_hot * log_probs, dim=-1)\n                per_example_loss = torch.masked_select(per_example_loss, b_label_mask.to(device))\n                labeled_example_count = per_example_loss.type(torch.float32).numel()\n\n                # It may be the case that a batch does not contain labeled examples,\n                # so the \"supervised loss\" in this case is not evaluated\n                if labeled_example_count == 0:\n                    D_L_Supervised = 0\n                else:\n                    D_L_Supervised = torch.div(torch.sum(per_example_loss.to(device)), labeled_example_count)\n                \n                # - torch.mean(torch.log(1 - D_real_probs[:, -1] + epsilon)) \n                D_L_unsupervised1U = torch.mean(torch.log(D_real_probs[:, -1] + epsilon))  # changed\n                D_L_unsupervised2U = -1 * torch.mean(torch.log(D_fake_probs[:, -1] + epsilon))\n                d_loss = D_L_Supervised + D_L_unsupervised1U + D_L_unsupervised2U\n\n                #---------------------------------\n                #  OPTIMIZATION\n                #---------------------------------\n                self.gen_optimizer.zero_grad()\n                self.dis_optimizer.zero_grad()\n\n                # Calculate weigth updates\n                # retain_graph=True is required since the underlying graph will be deleted after backward\n                g_loss.backward(retain_graph=True)\n                d_loss.backward() \n\n                # Apply modifications\n                self.gen_optimizer.step()\n                self.dis_optimizer.step()\n\n                # Save the losses to print them later\n                tr_g_loss += g_loss.item()\n                tr_d_loss += d_loss.item()\n\n            # Output training stats\n                if report:\n                    if step % 100 == 0:\n                        print('''\\n[Epoch %d/%d][iter %d/%d]\\ttotal Loss_D: %.4f\\ttotal Loss_G: %.4f,\\n\n                        details of Loss_D:  Loss_D_sup: %.4f,\\t-E[log(D(x))]: %.4f,\\t-E[log(1-D(G(z)))]: %.4f,\\n\n                        details of Loss_G:  -E[log(D(G(z)))]: %.4f,\\tLoss_G_feat: %.4f\\n\n                        D(x): %.4f\\tD(G(z)): %.4f'''\n                          %(epoch+1, epoch_num, step, len(train_dataloader),\n                            d_loss.mean().item(), g_loss.mean().item(), \n                            D_L_Supervised, D_L_unsupervised1U, D_L_unsupervised2U,\n                            g_loss_d, g_feat_reg,\n                            torch.mean(D_real_probs[:, -1]).item(), \n                              torch.mean(D_fake_probs[:, -1]).item() ))\n                        \n                        # save checkpoints\n                        self.save_checkpoint(epoch)\n\n            # Update the learning rate with the scheduler\n            self.scheduler_d.step()\n            self.scheduler_g.step()\n\n            # Calculate the average loss over all of the batches.\n            avg_train_loss_g = tr_g_loss / len(train_dataloader)\n            avg_train_loss_d = tr_d_loss / len(train_dataloader)\n\n            # Measure how long this epoch took.\n            epoch_time = format_time(time.time() - t0)\n\n            print(\"\")\n            print(f' Training stats at epoch {epoch+1}: ')\n            print(f' G_loss = {tr_g_loss}, D_loss = {tr_d_loss} \\n')\n            print(\" Training epcoh took: {:}\".format(epoch_time))\n            \n            if val_dataloader != None:\n                self.bert.eval()\n                self.discriminator.eval() \n\n                all_preds = np.array([])\n                all_label_ids = np.array([])\n                eval_loss = 0\n                nb_eval_steps = 0 \n                for val_step, batch in enumerate(val_dataloader):\n                    src_input_ids, src_input_mask, label_ids, _ = batch # unpacking\n                    src_input_ids = src_input_ids.to(device)\n                    src_input_mask = src_input_mask.to(device)\n                    label_ids = label_ids.to(device)\n\n\n                    with torch.no_grad():\n                        doc_rep = self.bert(src_input_ids, attention_mask=src_input_mask)['logits']#.unsqueeze(-1)\n                        _, logits, _ = self.discriminator(doc_rep)\n#                         probs = torch.nn.functional.softmax(logits[:,0:-1], dim=-1)\n                        probs = logits[:,0:-1]\n                        tmp_eval_loss = self.nll_loss(probs, label_ids.view(-1))\n\n                    eval_loss += tmp_eval_loss.mean().item()\n\n                    probs = probs.detach().cpu().numpy()\n                    label_ids = label_ids.to('cpu').numpy()\n                    all_preds = np.append(all_preds, np.argmax(probs, axis=1))\n                    all_label_ids = np.append(all_label_ids, label_ids)\n\n                    nb_eval_steps += 1\n\n                eval_loss = eval_loss / nb_eval_steps\n#                 precision, recall, f1, _ = precision_recall_fscore_support(all_label_ids, all_preds, average=\"micro\",\n#                                                                          labels=list(range(0,len(label_list))))\n                mcc = matthews_corrcoef(all_preds, all_label_ids)\n                acc = (all_preds == all_label_ids).sum().item() / all_label_ids.shape[0]\n\n\n                # Output validation stats\n                print(f'Validation stats: ')\n                print('Loss: %.4f,\\tAccuracy: %.4f,\\tmcc: %.4f,'\n                  %(eval_loss,acc,mcc))\n                \n            result = {\n                'epoch': epoch_time,\n                \"gen_loss\": tr_g_loss,\n                \"dis_loss\": tr_d_loss,\n                \"eval_loss\": eval_loss,\n                \"mcc\": mcc,\n                \"acc\": acc,\n                'epoch_time': epoch_time}\n#                 \"precision_micro\": precision,\n#                 \"recall_micro\": recall,\n#                 \"f1_micro\": f1,\n                \n\n            results.append(result)\n            # save checkpoints\n            self.save_checkpoint(epoch,results)\n            \n            # seva best model\n            if acc > best_score:\n                best_score = acc \n                self.save_checkpoint(epoch ,result,best=True)\n            \n    def save_checkpoint(self,epoch,results=None,best=False):\n        checkpoint = {\n            'epoch': epoch + 1,\n            'bert_state_dict': self.bert.state_dict(),\n            'disc_state_dict': self.discriminator.state_dict(),\n            'gen_state_dict': self.generator.state_dict(),\n            'disc_optimizer_state_dict': self.dis_optimizer.state_dict(),\n            'gen_optimizer_state_dict': self.gen_optimizer.state_dict(),\n            }\n        # for colab : /content/drive/My Drive/Project/checkpoints\n        if best:\n            torch.save(checkpoint, f'{self.path}/GAN_BERT_checkpoint_BEST.pth')\n            if results!= None:\n                with open(f'{self.path}/results_BEST.pickle', 'wb') as file:\n                    pickle.dump(results, file)\n            \n        else:\n            torch.save(checkpoint, f'{self.path}/GAN_BERT_checkpoint{epoch+1}.pth')\n            if results!= None:\n                with open(f'{self.path}/results.pickle', 'wb') as file:\n                    pickle.dump(results, file)\n    \n    def test(self, test_dataloader):\n        self.bert.eval()\n        self.discriminator.eval()\n\n        all_preds = np.array([])\n        all_label_ids = np.array([])\n        eval_loss = 0\n        nb_eval_steps = 0\n        nll_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\n        \n        for val_step, batch in enumerate(test_dataloader):\n            src_input_ids, src_input_mask, label_ids, _ = batch # unpacking\n            src_input_ids = src_input_ids.to(device)\n            src_input_mask = src_input_mask.to(device)\n            label_ids = label_ids.to(device)\n\n\n            with torch.no_grad():\n                doc_rep = self.bert(src_input_ids, attention_mask=src_input_mask)['logits']#.unsqueeze(-1)\n                _, logits, _ = self.discriminator(doc_rep)\n            # probs = torch.nn.functional.softmax(logits[:,0:-1], dim=-1)\n            probs = logits[:,0:-1]    \n            tmp_eval_loss = nll_loss(probs, label_ids.view(-1))\n\n            eval_loss += tmp_eval_loss.mean().item()\n\n            probs = probs.detach().cpu().numpy()\n            label_ids = label_ids.to('cpu').numpy()\n            \n            all_preds = np.append(all_preds, np.argmax(probs, axis=1))\n            all_label_ids = np.append(all_label_ids, label_ids)\n\n            nb_eval_steps += 1\n\n        eval_loss = eval_loss / nb_eval_steps\n\n\n        mcc = matthews_corrcoef(all_preds, all_label_ids)\n        acc = (all_preds == all_label_ids).sum().item() / all_label_ids.shape[0]\n        \n        # Output validation stats\n        print(f'Test stats: ')\n        print('Total loss: %.4f,\\tAccuracy: %.4f,\\tmcc: %.4f,'\n          %(eval_loss,acc,mcc) ) \n        return all_preds\n\n    @staticmethod\n    def rename_keys(original_ordered_dict):\n        new_keys_mapping = dict()\n        for a in list(original_ordered_dict.keys()):\n            new_keys_mapping[a] = a.split('module.')[-1] \n\n        return OrderedDict((new_keys_mapping.get(k, k), v) for k, v in original_ordered_dict.items())\n\n    \n    def load_checkpoint(self,checkpoint_path):\n        state_dict = torch.load(checkpoint_path)\n        \n        if (device.type == 'cuda') and (ngpu > 1):\n            # Load the state dictionary into the model\n            self.bert.load_state_dict(state_dict['bert_state_dict'])\n            self.discriminator.load_state_dict(state_dict['disc_state_dict'])\n            self.generator.load_state_dict(state_dict['gen_state_dict'])\n            self.dis_optimizer.load_state_dict(state_dict['disc_optimizer_state_dict'])\n            self.gen_optimizer.load_state_dict(state_dict['gen_optimizer_state_dict'])\n            \n        else: \n            self.bert.load_state_dict(self.rename_keys(state_dict['bert_state_dict']))\n            self.discriminator.load_state_dict(self.rename_keys(state_dict['disc_state_dict']))\n            self.generator.load_state_dict(self.rename_keys(state_dict['gen_state_dict']))\n            self.dis_optimizer.load_state_dict(state_dict['disc_optimizer_state_dict'])\n            self.gen_optimizer.load_state_dict(state_dict['gen_optimizer_state_dict'])\n\n        print('Loaded !')\n            \n    def plot_results():\n        pass\n            \n    def show_tensorboard():\n        pass","metadata":{"execution":{"iopub.status.busy":"2024-02-03T23:00:01.652850Z","iopub.execute_input":"2024-02-03T23:00:01.653178Z","iopub.status.idle":"2024-02-03T23:00:01.819242Z","shell.execute_reply.started":"2024-02-03T23:00:01.653146Z","shell.execute_reply":"2024-02-03T23:00:01.818426Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# !pip install numba\n\n# from numba import cuda\n# device = cuda.get_current_device()\n# device.reset()\n# torch.cuda.empty_cache()  ","metadata":{"execution":{"iopub.status.busy":"2024-02-03T23:00:02.078233Z","iopub.execute_input":"2024-02-03T23:00:02.078982Z","iopub.status.idle":"2024-02-03T23:00:02.084856Z","shell.execute_reply.started":"2024-02-03T23:00:02.078952Z","shell.execute_reply":"2024-02-03T23:00:02.082967Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# # !ls\n!mkdir part3\n!ls","metadata":{"execution":{"iopub.status.busy":"2024-02-03T23:00:03.129088Z","iopub.execute_input":"2024-02-03T23:00:03.130269Z","iopub.status.idle":"2024-02-03T23:00:05.140367Z","shell.execute_reply.started":"2024-02-03T23:00:03.130214Z","shell.execute_reply":"2024-02-03T23:00:05.139079Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"SubtaskB  part3\n","output_type":"stream"}]},{"cell_type":"markdown","source":"----\n## With **G1** :","metadata":{}},{"cell_type":"code","source":"# ganbert = GANBERT(discriminator, generator2, bert,gen_optimizer, dis_optimizer,\n#                 scheduler_d,scheduler_g, path='/kaggle/working/part3',G2=True) ","metadata":{"execution":{"iopub.status.busy":"2024-02-03T23:00:05.142358Z","iopub.execute_input":"2024-02-03T23:00:05.142683Z","iopub.status.idle":"2024-02-03T23:00:05.147198Z","shell.execute_reply.started":"2024-02-03T23:00:05.142652Z","shell.execute_reply":"2024-02-03T23:00:05.146150Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"# ganbert.trainer(epoch_num,label_list,labeled_ratio,train_dataloader, val_dataloader,report=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T23:00:05.296074Z","iopub.execute_input":"2024-02-03T23:00:05.296427Z","iopub.status.idle":"2024-02-03T23:00:05.300829Z","shell.execute_reply.started":"2024-02-03T23:00:05.296398Z","shell.execute_reply":"2024-02-03T23:00:05.299697Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"# test_res = ganbert.test(test_dataloader) ","metadata":{"execution":{"iopub.status.busy":"2024-02-03T23:00:06.501395Z","iopub.execute_input":"2024-02-03T23:00:06.501783Z","iopub.status.idle":"2024-02-03T23:00:06.506298Z","shell.execute_reply.started":"2024-02-03T23:00:06.501755Z","shell.execute_reply":"2024-02-03T23:00:06.505127Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"The `Matthews correlation coefficient` , is a measure of the quality of classifications in machine learning. It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. It's defined in the range from -1 to 1, with 1 being a perfect prediction, 0 being the result of a random prediction, and -1 indicating total disagreement between prediction and observation.","metadata":{"id":"x1zcuLgibO3X"}},{"cell_type":"markdown","source":"---\n### Load the best model","metadata":{"id":"Hm86KFjT1d9I"}},{"cell_type":"code","source":"discriminator = Discriminator(input_size,num_classes).to(device)\ngenerator1 = Generator1(noise_size,input_size).to(device)\nbert = BERT_Embedder(bert_model).to(device)\n\nif (device.type == 'cuda') and (ngpu > 1):\n    discriminator = nn.DataParallel(discriminator, list(range(ngpu)))\n    generator = nn.DataParallel(generator1, list(range(ngpu)))    \n    bert = nn.DataParallel(bert, list(range(ngpu)))\n    \ngen_optimizer = torch.optim.AdamW(generator1.parameters(), lr=learning_rate)\ndis_optimizer = torch.optim.AdamW(list(bert.parameters()) + list(discriminator.parameters()), lr=learning_rate)\n\n#scheduler\nnum_train_examples = len(train_dataset)\nnum_train_steps = int(num_train_examples / batch_size * epoch_num) \nnum_warmup_steps = int(num_train_steps * warmup_proportion)\n\nscheduler_d = get_constant_schedule_with_warmup(dis_optimizer, \n                                       num_warmup_steps = num_warmup_steps)\nscheduler_g = get_constant_schedule_with_warmup(gen_optimizer, \n                                       num_warmup_steps = num_warmup_steps) \n\nganbert_best = GANBERT(discriminator, generator1, bert,gen_optimizer, dis_optimizer,\n                scheduler_d,scheduler_g, path='/kaggle/working/part3') ","metadata":{"execution":{"iopub.status.busy":"2024-02-03T18:54:44.221256Z","iopub.execute_input":"2024-02-03T18:54:44.221611Z","iopub.status.idle":"2024-02-03T18:54:44.226748Z","shell.execute_reply.started":"2024-02-03T18:54:44.221581Z","shell.execute_reply":"2024-02-03T18:54:44.225728Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"path = '/kaggle/working/part3/GAN_BERT_checkpoint_BEST.pth'\nganbert_best.load_checkpoint(path)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T18:54:45.224328Z","iopub.execute_input":"2024-02-03T18:54:45.224694Z","iopub.status.idle":"2024-02-03T18:54:45.229774Z","shell.execute_reply.started":"2024-02-03T18:54:45.224664Z","shell.execute_reply":"2024-02-03T18:54:45.228803Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"test_res = ganbert_best.test(test_dataloader) ","metadata":{"execution":{"iopub.status.busy":"2024-02-03T18:54:45.561144Z","iopub.execute_input":"2024-02-03T18:54:45.561593Z","iopub.status.idle":"2024-02-03T18:54:45.565908Z","shell.execute_reply.started":"2024-02-03T18:54:45.561555Z","shell.execute_reply":"2024-02-03T18:54:45.564942Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"markdown","source":"----\n## With **G2** :","metadata":{}},{"cell_type":"code","source":"import warnings\n\n# Ignore some torchtext warnings due to originally writing this code with an\n# older version of torchtext\nwarnings.filterwarnings(\"ignore\", category=UserWarning)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T22:45:25.440459Z","iopub.execute_input":"2024-02-03T22:45:25.440935Z","iopub.status.idle":"2024-02-03T22:45:25.446671Z","shell.execute_reply.started":"2024-02-03T22:45:25.440891Z","shell.execute_reply":"2024-02-03T22:45:25.445753Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"ganbert = GANBERT(discriminator, generator1, bert,gen_optimizer, dis_optimizer,\n                scheduler_d,scheduler_g, path='/kaggle/working/part3') \n\nganbert.trainer(epoch_num,label_list,labeled_ratio,train_dataloader, val_dataloader,report=True)\n# test_res = ganbert.test(test_dataloader) ","metadata":{"execution":{"iopub.status.busy":"2024-02-03T23:00:10.991228Z","iopub.execute_input":"2024-02-03T23:00:10.991934Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"With labeled_ratio : 1\n\nEpoch 1/5 :\n\n[Epoch 1/5][iter 0/2368]\ttotal Loss_D: 1.7939\ttotal Loss_G: 0.1612,\n\n                        details of Loss_D:  Loss_D_sup: 1.8032,\t-E[log(D(x))]: -1.9167,\t-E[log(1-D(G(z)))]: 1.9073,\n\n                        details of Loss_G:  -E[log(D(G(z)))]: 0.1607,\tLoss_G_feat: 0.0005\n\n                        D(x): 0.1471\tD(G(z)): 0.1485\n\n[Epoch 1/5][iter 100/2368]\ttotal Loss_D: 1.7911\ttotal Loss_G: 0.1600,\n\n                        details of Loss_D:  Loss_D_sup: 1.7903,\t-E[log(D(x))]: -1.9134,\t-E[log(1-D(G(z)))]: 1.9142,\n\n                        details of Loss_G:  -E[log(D(G(z)))]: 0.1596,\tLoss_G_feat: 0.0005\n\n                        D(x): 0.1476\tD(G(z)): 0.1475\n\n[Epoch 1/5][iter 200/2368]\ttotal Loss_D: 1.7795\ttotal Loss_G: 0.1611,\n\n                        details of Loss_D:  Loss_D_sup: 1.7893,\t-E[log(D(x))]: -1.9178,\t-E[log(1-D(G(z)))]: 1.9080,\n\n                        details of Loss_G:  -E[log(D(G(z)))]: 0.1606,\tLoss_G_feat: 0.0004\n\n                        D(x): 0.1469\tD(G(z)): 0.1484\n\n[Epoch 1/5][iter 300/2368]\ttotal Loss_D: 1.7844\ttotal Loss_G: 0.1607,\n\n                        details of Loss_D:  Loss_D_sup: 1.7887,\t-E[log(D(x))]: -1.9144,\t-E[log(1-D(G(z)))]: 1.9101,\n\n                        details of Loss_G:  -E[log(D(G(z)))]: 0.1603,\tLoss_G_feat: 0.0004\n\n                        D(x): 0.1474\tD(G(z)): 0.1481\n\n[Epoch 1/5][iter 400/2368]\ttotal Loss_D: 1.7955\ttotal Loss_G: 0.1602,\n\n                        details of Loss_D:  Loss_D_sup: 1.7964,\t-E[log(D(x))]: -1.9136,\t-E[log(1-D(G(z)))]: 1.9127,\n\n                        details of Loss_G:  -E[log(D(G(z)))]: 0.1598,\tLoss_G_feat: 0.0004\n\n                        D(x): 0.1476\tD(G(z)): 0.1477\n\n[Epoch 1/5][iter 500/2368]\ttotal Loss_D: 1.7929\ttotal Loss_G: 0.1610,\n\n                        details of Loss_D:  Loss_D_sup: 1.8053,\t-E[log(D(x))]: -1.9209,\t-E[log(1-D(G(z)))]: 1.9085,\n\n                        details of Loss_G:  -E[log(D(G(z)))]: 0.1605,\tLoss_G_feat: 0.0005\n\n                        D(x): 0.1465\tD(G(z)): 0.1483\n\n[Epoch 1/5][iter 600/2368]\ttotal Loss_D: 1.7681\ttotal Loss_G: 0.1617,\n\n                        details of Loss_D:  Loss_D_sup: 1.7803,\t-E[log(D(x))]: -1.9164,\t-E[log(1-D(G(z)))]: 1.9042,\n\n                        details of Loss_G:  -E[log(D(G(z)))]: 0.1613,\tLoss_G_feat: 0.0004\n\n                        D(x): 0.1472\tD(G(z)): 0.1490\n\n[Epoch 1/5][iter 700/2368]\ttotal Loss_D: 1.7760\ttotal Loss_G: 0.1603,\n\n                        details of Loss_D:  Loss_D_sup: 1.7801,\t-E[log(D(x))]: -1.9160,\t-E[log(1-D(G(z)))]: 1.9119,\n\n                        details of Loss_G:  -E[log(D(G(z)))]: 0.1599,\tLoss_G_feat: 0.0003\n\n                        D(x): 0.1472\tD(G(z)): 0.1478\n\n[Epoch 1/5][iter 800/2368]\ttotal Loss_D: 1.7859\ttotal Loss_G: 0.1600,\n\n                        details of Loss_D:  Loss_D_sup: 1.7855,\t-E[log(D(x))]: -1.9133,\t-E[log(1-D(G(z)))]: 1.9137,\n\n                        details of Loss_G:  -E[log(D(G(z)))]: 0.1596,\tLoss_G_feat: 0.0004\n\n                        D(x): 0.1476\tD(G(z)): 0.1476\n\n[Epoch 1/5][iter 900/2368]\ttotal Loss_D: 1.7837\ttotal Loss_G: 0.1605,\n\n                        details of Loss_D:  Loss_D_sup: 1.7895,\t-E[log(D(x))]: -1.9167,\t-E[log(1-D(G(z)))]: 1.9109,\n\n                        details of Loss_G:  -E[log(D(G(z)))]: 0.1601,\tLoss_G_feat: 0.0004\n\n                        D(x): 0.1471\tD(G(z)): 0.1480\n\n[Epoch 1/5][iter 1000/2368]\ttotal Loss_D: 1.7887\ttotal Loss_G: 0.1609,\n\n                        details of Loss_D:  Loss_D_sup: 1.7965,\t-E[log(D(x))]: -1.9168,\t-E[log(1-D(G(z)))]: 1.9090,\n\n                        details of Loss_G:  -E[log(D(G(z)))]: 0.1604,\tLoss_G_feat: 0.0004\n\n                        D(x): 0.1471\tD(G(z)): 0.1482\n\n[Epoch 1/5][iter 1100/2368]\ttotal Loss_D: 1.7863\ttotal Loss_G: 0.1610,\n\n                        details of Loss_D:  Loss_D_sup: 1.7930,\t-E[log(D(x))]: -1.9149,\t-E[log(1-D(G(z)))]: 1.9082,\n\n                        details of Loss_G:  -E[log(D(G(z)))]: 0.1606,\tLoss_G_feat: 0.0004\n\n                        D(x): 0.1474\tD(G(z)): 0.1484\n\n[Epoch 1/5][iter 1200/2368]\ttotal Loss_D: 1.7877\ttotal Loss_G: 0.1612,\n\n                        details of Loss_D:  Loss_D_sup: 1.7943,\t-E[log(D(x))]: -1.9137,\t-E[log(1-D(G(z)))]: 1.9071,\n\n                        details of Loss_G:  -E[log(D(G(z)))]: 0.1608,\tLoss_G_feat: 0.0004\n\n                        D(x): 0.1476\tD(G(z)): 0.1485\n\n[Epoch 1/5][iter 1300/2368]\ttotal Loss_D: 1.7926\ttotal Loss_G: 0.1609,\n\n                        details of Loss_D:  Loss_D_sup: 1.7975,\t-E[log(D(x))]: -1.9136,\t-E[log(1-D(G(z)))]: 1.9087,\n\n                        details of Loss_G:  -E[log(D(G(z)))]: 0.1605,\tLoss_G_feat: 0.0004\n\n                        D(x): 0.1476\tD(G(z)): 0.1483\n\n[Epoch 1/5][iter 1400/2368]\ttotal Loss_D: 1.7774\ttotal Loss_G: 0.1613,\n\n                        details of Loss_D:  Loss_D_sup: 1.7851,\t-E[log(D(x))]: -1.9140,\t-E[log(1-D(G(z)))]: 1.9063,\n\n                        details of Loss_G:  -E[log(D(G(z)))]: 0.1609,\tLoss_G_feat: 0.0004\n\n                        D(x): 0.1475\tD(G(z)): 0.1486\n\n[Epoch 1/5][iter 1500/2368]\ttotal Loss_D: 1.7878\ttotal Loss_G: 0.1612,\n\n                        details of Loss_D:  Loss_D_sup: 1.7917,\t-E[log(D(x))]: -1.9105,\t-E[log(1-D(G(z)))]: 1.9066,\n\n                        details of Loss_G:  -E[log(D(G(z)))]: 0.1609,\tLoss_G_feat: 0.0004\n\n                        D(x): 0.1480\tD(G(z)): 0.1486\n\n[Epoch 1/5][iter 1600/2368]\ttotal Loss_D: 1.7817\ttotal Loss_G: 0.1618,\n\n                        details of Loss_D:  Loss_D_sup: 1.7902,\t-E[log(D(x))]: -1.9121,\t-E[log(1-D(G(z)))]: 1.9035,\n\n                        details of Loss_G:  -E[log(D(G(z)))]: 0.1614,\tLoss_G_feat: 0.0004\n\n                        D(x): 0.1478\tD(G(z)): 0.1490\n\n[Epoch 1/5][iter 1700/2368]\ttotal Loss_D: 1.7898\ttotal Loss_G: 0.1606,\n\n                        details of Loss_D:  Loss_D_sup: 1.7938,\t-E[log(D(x))]: -1.9150,\t-E[log(1-D(G(z)))]: 1.9109,\n\n                        details of Loss_G:  -E[log(D(G(z)))]: 0.1601,\tLoss_G_feat: 0.0005\n\n                        D(x): 0.1474\tD(G(z)): 0.1480\n\n[Epoch 1/5][iter 1800/2368]\ttotal Loss_D: 1.7751\ttotal Loss_G: 0.1611,\n\n                        details of Loss_D:  Loss_D_sup: 1.7831,\t-E[log(D(x))]: -1.9155,\t-E[log(1-D(G(z)))]: 1.9075,\n\n                        details of Loss_G:  -E[log(D(G(z)))]: 0.1607,\tLoss_G_feat: 0.0004\n\n                        D(x): 0.1473\tD(G(z)): 0.1485\n\n[Epoch 1/5][iter 1900/2368]\ttotal Loss_D: 1.7807\ttotal Loss_G: 0.1609,\n\n                        details of Loss_D:  Loss_D_sup: 1.7885,\t-E[log(D(x))]: -1.9169,\t-E[log(1-D(G(z)))]: 1.9091,\n\n                        details of Loss_G:  -E[log(D(G(z)))]: 0.1604,\tLoss_G_feat: 0.0004\n\n                        D(x): 0.1471\tD(G(z)): 0.1482\n\n[Epoch 1/5][iter 2000/2368]\ttotal Loss_D: 1.7801\ttotal Loss_G: 0.1614,\n\n                        details of Loss_D:  Loss_D_sup: 1.7857,\t-E[log(D(x))]: -1.9110,\t-E[log(1-D(G(z)))]: 1.9054,\n\n                        details of Loss_G:  -E[log(D(G(z)))]: 0.1611,\tLoss_G_feat: 0.0003\n\n                        D(x): 0.1480\tD(G(z)): 0.1488\n\n[Epoch 1/5][iter 2100/2368]\ttotal Loss_D: 1.7727\ttotal Loss_G: 0.1611,\n\n                        details of Loss_D:  Loss_D_sup: 1.7785,\t-E[log(D(x))]: -1.9136,\t-E[log(1-D(G(z)))]: 1.9078,\n\n                        details of Loss_G:  -E[log(D(G(z)))]: 0.1607,\tLoss_G_feat: 0.0005\n\n                        D(x): 0.1476\tD(G(z)): 0.1484\n\n[Epoch 1/5][iter 2200/2368]\ttotal Loss_D: 1.7790\ttotal Loss_G: 0.1612,\n\n                        details of Loss_D:  Loss_D_sup: 1.7932,\t-E[log(D(x))]: -1.9212,\t-E[log(1-D(G(z)))]: 1.9070,\n\n                        details of Loss_G:  -E[log(D(G(z)))]: 0.1608,\tLoss_G_feat: 0.0004\n\n                        D(x): 0.1464\tD(G(z)): 0.1485\n\n[Epoch 1/5][iter 2300/2368]\ttotal Loss_D: 1.7861\ttotal Loss_G: 0.1619,\n\n                        details of Loss_D:  Loss_D_sup: 1.7972,\t-E[log(D(x))]: -1.9143,\t-E[log(1-D(G(z)))]: 1.9031,\n\n                        details of Loss_G:  -E[log(D(G(z)))]: 0.1615,\tLoss_G_feat: 0.0005\n\n                        D(x): 0.1475\tD(G(z)): 0.1491\n\n Training stats at epoch 1: \n G_loss = 381.1063245087862, D_loss = 4228.586377978325 \n\n Training epcoh took: 2:20:51\n","output_type":"stream"}]},{"cell_type":"code","source":"test_res = ganbert.test(test_dataloader)  ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}}]}